# nnsight Source-Based Serialization Design Doc

## TL;DR

Replace cloudpickle with source-based serialization to eliminate Python version lock-in and enable third-party libraries to work with NDIF without server-side installation.

**Key changes:**
- New `@nnsight.remote` decorator for functions and classes used in traces
- Serialization sends source code + JSON instead of pickled bytecode
- Module aliases (`np`, `F`, `torch`) and constants auto-captured
- Backward compatible: falls back to cloudpickle with deprecation warning

**User-facing change:**
```python
@nnsight.remote  # Add this decorator
class MyAnalyzer:
    def __init__(self, model, top_k=10):
        self.model = model
        self.top_k = top_k

    def analyze(self, layer):
        h = self.model.transformer.h[layer].output[0]
        return h.topk(self.top_k)
```

**Benefits:**
- Works across Python versions (3.10 client â†” 3.12 server)
- Libraries like nnterp work without NDIF server installation
- Validation at import time, not mysterious runtime failures

## Implementation Status & Roadmap

**âœ… Implemented (v2.1)**
*   `@nnsight.remote` decorator with import-time validation
*   AST validation for allowed modules and attribute chains
*   Source serialization for functions, classes, and instances
*   Capture of module-level constants and callable references (`__callable_ref__`)
*   Heuristics for identifying the Model and internal variables
*   **Allowed attribute chains** - `os.path.join`, `pathlib.Path()` allowed; I/O blocked
*   **Closure variable support** - `__closure__` and `co_freevars` extraction
*   **File/line metadata** - Source payload includes origin for error mapping
*   **Cycle detection** - Clear error for self-referential objects
*   **Backward compatibility** - Falls back to cloudpickle with deprecation warning

**ðŸš§ Deferred (See "Deferred Features" section below)**
1.  **Binary Sidecars**: Tensor/Array extraction to binary buffers (requires transport changes)
2.  **Flat Graph**: `objects` map for circular references (rare use case)
3.  **Pickle Hooks**: `__getstate__`/`__setstate__` support (default `__dict__` works)
4.  **Source Hashing**: Cache verification (version string sufficient for now)
5.  **Notebook Support**: IPython history fallback (fragile, `.py` files primary use case)

---

## Motivation: The Cloudpickle Problem

When a researcher writes a trace like this:

```python
with model.trace("Hello world", remote=True):
    h = model.layers[10].output[0]
    result = h.topk(10).save()
```

nnsight needs to send this code to the NDIF server for execution. Currently, it uses **cloudpickle** to serialize the code block and its context. This approach has a fundamental limitation: cloudpickle serializes Python *bytecode*, not source code.

### Why Bytecode Serialization is Problematic

Python bytecode is version-specific. The bytecode generated by Python 3.10 is not guaranteed to run on Python 3.12, and vice versa. This creates a hard constraint: **clients must run the exact same Python version as the NDIF servers**.

Even worse, cloudpickle creates an *illusion* of flexibility. It appears to support "arbitrary Python," but in practice:
- Any library used in a trace must be installed on the server
- Any class or function used must be available on both sides
- Complex objects may serialize but fail mysteriously at runtime

### The Organic Library Problem

Consider a library like **nnterp** that provides helper classes for working with transformer internals:

```python
from nnterp import StandardizedTransformer

st = StandardizedTransformer(model)
with model.trace("Hello world", remote=True):
    h = st.get_hidden(10)  # Convenience method
    result = h.topk(10).save()
```

With cloudpickle, this code *only works if nnterp is installed on the NDIF server*. This creates a bottleneck: every third-party library needs NDIF team coordination to deploy. The library ecosystem cannot grow organically.

---

## The Key Insight: Libraries as Query Generators

The solution comes from recognizing what these helper libraries actually do: **they generate queries, not server-side computation**.

When `st.get_hidden(10)` executes inside a trace, it doesn't actually retrieve hidden statesâ€”it builds up a symbolic computation graph that will be executed on the server. The `StandardizedTransformer` class is essentially a query generator, not runtime code.

This means we can:
1. Send the *source code* of `StandardizedTransformer` to the server
2. Reconstruct the class on the server
3. Execute the trace using the reconstructed class

The server doesn't need nnterp installedâ€”it just needs the source code of the specific classes being used.

---

## Problem Statement

nnsight currently uses cloudpickle to serialize trace blocks for remote execution on NDIF. This creates several problems:

1. **Python version lock-in**: cloudpickle serializes bytecode, which is Python-version-specific. Clients must run the exact same Python version as NDIF servers.

2. **Illusory flexibility**: While cloudpickle appears to allow "arbitrary Python," in practice code can only use libraries installed on the server. The flexibility is more limited than it appears.

3. **No organic library growth**: Third-party libraries (like nnterp) cannot be used in traces without installing them on NDIF servers, requiring coordination with the NDIF team.

4. **Fragile failures**: Version mismatches and missing dependencies cause mysterious runtime errors rather than clear, early failures.

## Goals

1. **Version-agnostic serialization**: Clients on Python 3.10 should work with servers on Python 3.12+
2. **Enable organic library ecosystem**: Third-party libraries can work with NDIF without server-side installation
3. **Fail fast and loud**: Incompatible code should error at import time with clear messages
4. **Preserve expressiveness**: Loops, conditionals, and server-side computation should still work
5. **Minimize breaking changes**: Existing simple traces should continue to work

## Non-Goals

1. Support for arbitrary Python objects (we accept explicit constraints)
2. Support for libraries with C extensions in traces
3. Indefinite cloudpickle support (deprecated, removed in 2.0)

---

## Current Architecture

### What nnsight already does well

nnsight already captures source code as text:

```python
# tracing/base.py - capture()
source_lines, offset = inspect.getsourcelines(frame)

# tracing/base.py - parse()
tree = ast.parse("".join(source_lines))
# ... finds the with block via AST visitor

# tracing/base.py - compile()
self.info.source = [
    f"def {function_name}(__nnsight_tracer__, __nnsight_tracing_info__):\n",
    "    __nnsight_tracer__.pull()\n",
    *self.info.source,  # <-- Source as text!
    "    __nnsight_tracer__.push()\n",
]
```

### Where cloudpickle enters

Cloudpickle is used in `serialization.py` to serialize:

1. `tracer.info.frame` - the FrameType containing closure variables
2. Any objects referenced in the trace (via `pull()`)

```python
# serialization.py
import cloudpickle

def save(obj):
    CustomCloudPickler(file, protocol=4).dump(obj)  # <-- Here
```

### The pull/push mechanism

```python
# pull() - imports variables from original scope into trace execution
original_state = self.info.frame.f_locals
push_variables(current_frame, filtered_state)

# push() - exports variables back after execution
push_variables(target_frame, filtered_state)
```

This is where closure variables get transferred. Currently they're cloudpickled; we need to JSON-serialize them instead.

---

## Proposed Design

### Core Principle

**Libraries are query generators, not server-side code.**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client-side (pip installable, version-agnostic)   â”‚
â”‚                                                     â”‚
â”‚  User code                                          â”‚
â”‚    â†“                                                â”‚
â”‚  Third-party libs (nnterp, logitlenskit, etc.)     â”‚
â”‚    â†“                                                â”‚
â”‚  nnsight (generates traces)                         â”‚
â”‚    â†“                                                â”‚
â”‚  Serialized: source text + JSON variables           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  NDIF Server                                        â”‚
â”‚                                                     â”‚
â”‚  Receives: source + JSON                            â”‚
â”‚  Reconstructs: @nnsight.remote classes/functions    â”‚
â”‚  Executes: with torch, numpy, model access          â”‚
â”‚  Returns: requested tensors                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### New Serialization Format: Flat Graph + Buffers

To support **binary attachments** (like tensors) and **circular references**, the format uses a "flat graph" approach alongside sidecar binary buffers.

```python
# Returns: Tuple[json_payload, List[bytes]]
# json_payload:
{
    "version": "2.1",
    "source": [
        {"code": "def ...", "file": "user_script.py", "line": 10},
        ...
    ],
    # Flat object graph
    "objects": {
        "root": {
            "top_k": 10,
            "bias_vector": {"__tensor_ref__": 0},  # Index into buffers
            "analyzer": {"__ref__": "obj_123"}
        },
        "obj_123": {
            "__type__": "LogitLensKit",
            "top_k": 10,
            "parent": {"__ref__": "root"}  # Circular reference handled
        }
    },
    "remote_objects": { ... },
    "model_refs": [...]
}

# buffers: [
#    b'...binary tensor data...'
# ]
```

---

## The @nnsight.remote Decorator

### When is @remote Required?

**Simple rule: If you wrote it, decorate it.**

`@nnsight.remote` is **required** for any user-defined code used in traces:

| What you use in a trace | @remote needed? |
|------------------------|----------------|
| Built-in torch modules (`nn.Linear`, `nn.Conv2d`) | No - works automatically |
| Built-in torch/numpy functions (`torch.relu`, `np.mean`) | No - works automatically |
| JSON-serializable values (`int`, `str`, `list`, `dict`) | No - passes through |
| **Your custom `nn.Module` subclass** | **Yes - required** |
| **Your custom class** | **Yes - required** |
| **Your custom function** | **Yes - required** |

Without `@remote`, user-defined code cannot be validated or serialized for remote execution. The decorator:
1. **Validates at import time** - catches disallowed imports, unsafe operations, non-serializable references
2. **Captures source code** - enables serialization without cloudpickle
3. **Signals intent** - makes it explicit which code runs remotely

### Why a Decorator?

Not all Python code can be safely serialized and reconstructed on a server. Consider these constraints:

1. **Source must be available**: We need to send the actual source text, which means the code must be in `.py` files (not dynamically generated or from C extensions).

2. **Only certain imports are allowed**: The server has `torch`, `numpy`, `math`, `collections`, `itertools`, `functools`, and `operator`, but not `pandas`, `sklearn`, or arbitrary libraries.

3. **No side effects**: Code that opens files, makes network requests, or spawns processes would behave unpredictably on the server.

4. **References must be resolvable**: If your code uses a module-level constant like `TOP_K = 10`, we need to capture that value. But if it references a complex object, we can't serialize it.

The `@nnsight.remote` decorator performs **validation at import time**. When you decorate a class or function, the decorator immediately checks all these constraints and raises a clear error if any are violated. This is much better than discovering problems at runtime when the trace executes on the server.

### Validating Attribute Chains

To strike a balance between safety and expressiveness, the validator checks **attribute chains** instead of just banning modules.

- **Blocked**: `os.system()`, `subprocess.run()`, `pathlib.Path.read_text()`
- **Allowed**: `os.path.join()`, `pathlib.Path('foo')`

This nuance allows users to use benign parts of standard libraries (like path manipulation) while strictly preventing dangerous operations (like command execution or file I/O).

### Design Choice: One Decorator for Everything

We initially considered separate decorators (`@inlineable` for functions, `@remote` for classes), but this added unnecessary complexity. A single `@nnsight.remote` decorator works for both functions and classes, making the API simpler to learn and use.

### Purpose

Mark functions and classes as safe for remote execution. Validates at import time. One decorator for both.

### Usage

```python
import nnsight

# Works on functions
@nnsight.remote
def normalize(x):
    return x / x.norm(dim=-1, keepdim=True)

@nnsight.remote
def project_to_vocab(h, unembed):
    return h @ unembed.T

# Works on classes
@nnsight.remote
class LogitLensKit:
    def __init__(self, model, top_k=10):
        self.model = model
        self.top_k = top_k

    def get_hidden(self, layer):
        return self.model.transformer.h[layer].output[0]

    def project(self, h):
        return h @ self.model.lm_head.weight.T

# For library authors - with version metadata:
@nnsight.remote(library="nnterp", version="0.1.0")
class StandardizedTransformer:
    ...
```

### Version-Aware Serialization & Source Hashing

Library authors can specify version metadata. To handle cases where a client might use a **patched or modified version** of a standard library, we include a hash of the client's source code.

```python
@nnsight.remote(library="nnterp", version="0.1.0")
class StandardizedTransformer:
    ...
```

**Resolution Logic (Server-Side):**
1. Check if `library` (e.g., "nnterp") and `version` (e.g., "0.1.0") are installed.
2. If installed, compute the hash of the **installed class's source**.
3. Compare with the **client's provided source hash**.
4. **Match:** Use the server's installed class (Fast path, safe).
5. **Mismatch:** Use the client's transmitted source code (Slow path, flexible).

This ensures that even if versions nominally match, local patches by researchers are respected.

### Import-Time Validation

The decorator performs these checks when the function/class is defined:

```python
@nnsight.remote
def my_func():
    ...

@nnsight.remote
class MyClass:
    ...

# At decoration time:
# âœ“ Source is available (inspect.getsource works)
# âœ“ No disallowed imports (only torch, numpy, math, builtins)
# âœ“ Module-level references are JSON-serializable (auto-captured)
# âœ“ Base classes are @nnsight.remote or object (for classes)
# âœ“ No metaclass (for classes)
# âœ“ __slots__ classes supported (serialized by iterating slots)
# âœ“ All code uses only allowed operations
# âœ“ No external side effects (file I/O, network, etc.)
# âœ“ Closure variables in methods captured (including __init_subclass__)
```

### Module-Level Reference Handling

The decorator intelligently handles module-level references by checking what they resolve to:

```python
# my_library/analyzer.py

import numpy as np
import torch
import torch.nn.functional as F

DEFAULT_TOP_K = 10
VOCAB_SIZE = 50257
COMPLEX_CONFIG = SomeClass()  # Not JSON-serializable

@nnsight.remote
class Analyzer:
    def __init__(self, top_k=DEFAULT_TOP_K):
        self.top_k = top_k

    def analyze(self, h):
        # Module aliases - automatically recognized as allowed
        h_norm = h / np.linalg.norm(h, axis=-1, keepdims=True)
        logits = F.softmax(h_norm @ self.weights, dim=-1)

        # Constants - auto-captured
        return logits.topk(VOCAB_SIZE)

    def broken(self):
        return COMPLEX_CONFIG.value  # ERROR: not serializable
```

**How references are resolved:**

| Reference | Resolves to | Action |
|-----------|-------------|--------|
| `np` | `numpy` module | âœ“ Skip (available on server) |
| `F` | `torch.nn.functional` module | âœ“ Skip (available on server) |
| `torch` | `torch` module | âœ“ Skip (available on server) |
| `DEFAULT_TOP_K` | `10` (int) | âœ“ Capture: `{"DEFAULT_TOP_K": 10}` |
| `VOCAB_SIZE` | `50257` (int) | âœ“ Capture: `{"VOCAB_SIZE": 50257}` |
| `COMPLEX_CONFIG` | `SomeClass` instance | âœ— Error: not serializable |

At decoration time:
```
@nnsight.remote validation for 'Analyzer':
  âœ“ Module alias 'np' -> numpy (available on server)
  âœ“ Module alias 'F' -> torch.nn.functional (available on server)
  âœ“ Will capture constant: DEFAULT_TOP_K = 10
  âœ“ Will capture constant: VOCAB_SIZE = 50257
  âœ— ERROR: Reference 'COMPLEX_CONFIG' (type 'SomeClass') is not JSON-serializable
    Options:
      - Make it a class/instance attribute instead
      - Pass it as a function/method argument
      - Use a JSON-serializable type (int, float, str, list, dict)
```

### Error Messages

```
ImportError: @nnsight.remote validation failed for 'MyClass':

  Line 3: imports 'pandas' (not available on NDIF server)
    Allowed imports: torch, numpy, math, builtins

  Line 12: references 'COMPLEX_CONFIG' (type 'SomeClass', not serializable)
    Options: make it an argument, or use JSON-serializable type

  Line 25: calls 'open()'
    @nnsight.remote code cannot perform file I/O
```

### Implementation Sketch

```python
```python
# nnsight/remote.py

import ast
import inspect
import sys
from typing import Union, Type, Callable

ALLOWED_MODULES = {
    'torch', 'numpy', 'math', 'builtins',
    'collections', 'itertools', 'functools', 'operator'
}
DISALLOWED_CALLS = {'open', 'exec', 'eval', 'compile', 'input'}
DISALLOWED_ATTR_PATTERNS = {
    ('os', 'system'), ('subprocess', 'run'), ('pathlib', 'Path', 'read_text')
}

def remote(obj: Union[Type, Callable]) -> Union[Type, Callable]:
# ... (omitted for brevity)

def validate_ast(tree: ast.AST, name: str) -> list:
    """Validate AST for disallowed patterns."""
    errors = []

    class Validator(ast.NodeVisitor):
        def visit_Import(self, node):
            for alias in node.names:
                module = alias.name.split('.')[0]
                if module not in ALLOWED_MODULES:
                    errors.append((node.lineno, f"imports '{alias.name}'"))

        def visit_ImportFrom(self, node):
            module = (node.module or '').split('.')[0]
            if module not in ALLOWED_MODULES:
                errors.append((node.lineno, f"imports from '{node.module}'"))

        def visit_Call(self, node):
            # Check function calls
            if isinstance(node.func, ast.Name):
                if node.func.id in DISALLOWED_CALLS:
                    errors.append((node.lineno, f"calls '{node.func.id}()'"))
            
            # Check attribute chains (e.g. os.system)
            if isinstance(node.func, ast.Attribute):
                chain = self._get_attr_chain(node.func)
                for pattern in DISALLOWED_ATTR_PATTERNS:
                    if len(chain) >= len(pattern) and chain[:len(pattern)] == pattern:
                        errors.append((node.lineno, f"calls '{'.'.join(chain)}()'"))
            
            self.generic_visit(node)

        def _get_attr_chain(self, node):
            if isinstance(node, ast.Attribute):
                return self._get_attr_chain(node.value) + (node.attr,)
            elif isinstance(node, ast.Name):
                return (node.id,)
            return ()

    Validator().visit(tree)
    return errors
```


def validate_class(cls: type) -> list:
    """Additional validation for classes."""
    errors = []

    # Check base classes
    for base in cls.__bases__:
        if base is not object and not getattr(base, '_remote_validated', False):
            errors.append(f"Base class '{base.__name__}' is not @nnsight.remote")

    # Check for metaclass
    if type(cls) is not type:
        errors.append(f"Uses metaclass '{type(cls).__name__}'")

    # Note: __slots__ classes are now supported via special serialization handling
    # in serialize_instance_state() - we iterate through slots instead of __dict__

    return errors
```

---

## Serialization Changes

The serialization logic has been refined with several key behaviors:

1.  **Flattened Object Graph**: To handle circular references and shared objects, variables are not serialized as a tree but as a flat map of objects.
2.  **Binary Attachments**: Tensors and Numpy arrays are extracted into a separate list of binary buffers. The JSON payload references them by index (`__tensor_ref__`).
3.  **Internal Variable Filtering**: Local variables starting with `_nnsight` or `nnsight` are automatically filtered out.
4.  **Strict Model Heuristics**: "Model access" is strictly defined by type checking to avoid ambiguity.

### New serialization.py

```python
def serialize_for_remote(tracer) -> Tuple[str, List[bytes]]:
    """
    Serialize source + variables + buffers.
    Returns: (json_payload, list_of_buffers)
    """
    source = tracer.info.source  # Now includes file/line info
    
    # Flatten variables into a graph and extract binary buffers
    flattener = GraphFlattener()
    root_id = flattener.add(tracer.info.frame.f_locals)
    
    payload = {
        "version": "2.1",
        "source": source,
        "root": root_id,
        "objects": flattener.objects,  # Flat map {id: state}
        "remote_objects": flattener.remote_objects,
    }
    
    return json.dumps(payload), flattener.buffers

class GraphFlattener:
    def __init__(self):
        self.objects = {}
        self.buffers = []
        self.memo = {} # id -> ref_id

    def add(self, obj):
        if id(obj) in self.memo:
            return {"__ref__": self.memo[id(obj)]}
        
        # Handle Tensors/Numpy (Binary)
        if is_tensor(obj):
            self.buffers.append(save_tensor_bytes(obj))
            return {"__tensor_ref__": len(self.buffers) - 1}
            
        # Handle Primitives (pass through)
        if is_json_serializable(obj):
            return obj
            
        # Handle Complex Objects
        ref_id = str(id(obj))
        self.memo[id(obj)] = ref_id
        
        if isinstance(obj, dict):
            state = {k: self.add(v) for k, v in obj.items() if not is_internal(k)}
            self.objects[ref_id] = state
        elif is_remote_object(obj):
            state = {k: self.add(v) for k, v in obj.__dict__.items()}
            self.objects[ref_id] = {"__type__": type(obj).__name__, "state": state}
            # ... capture remote class source ...
            
        return {"__ref__": ref_id}
```

---

## Server-Side Changes

### Deserialization

```python
# Server-side: reconstruct and execute

import json
import torch
import numpy

def deserialize_and_execute(payload: bytes, model) -> Any:
    data = json.loads(payload.decode('utf-8'))

    # Build namespace with allowed modules
    namespace = {
        'torch': torch,
        'numpy': numpy,
        'np': numpy,
        'model': model,
    }

    # Reconstruct @nnsight.remote functions and classes
    for obj_name, obj_data in data.get('remote_objects', {}).items():
        # Add captured module-level references to namespace
        namespace.update(obj_data.get('module_refs', {}))

        # Execute function/class definition
        exec(obj_data['source'], namespace)

        # For classes, reconstruct instances
        if obj_data['type'] == 'class':
            cls = namespace[obj_name]
            for instance_id, instance_data in obj_data.get('instances', {}).items():
                obj = object.__new__(cls)
                obj.__dict__ = reconstruct_state(instance_data['state'], namespace)
                namespace[instance_data['var_name']] = obj

    # Add simple variables
    namespace.update(resolve_refs(data.get('variables', {}), namespace))

    # Compile and execute trace source
    source = ''.join(data['source'])
    code = compile(source, '<nnsight-remote>', 'exec')
    exec(code, namespace)

    return namespace.get('__nnsight_result__')


def reconstruct_state(state: dict, namespace: dict, reconstructed_ids: dict) -> dict:
    """Reconstruct instance state, resolving references."""
    result = {}
    for key, value in state.items():
        if isinstance(value, dict):
            if '__model_ref__' in value:
                result[key] = namespace['model']
            elif '__remote_ref__' in value:
                # Resolve reference to other remote object
                ref_id = str(value['__remote_ref__'])
                result[key] = reconstructed_ids.get(ref_id, value)
            elif '__callable_ref__' in value:
                # Resolve function reference (e.g. "torch.nn.functional.relu")
                mod_name, func_name = value['__callable_ref__'].rsplit('.', 1)
                mod = __import__(mod_name, fromlist=[func_name])
                result[key] = getattr(mod, func_name)
            else:
                result[key] = value
        else:
            result[key] = value
    return result
```

---

## Migration Path

### Phase 1: Add new serialization (non-breaking)

1. Implement `@nnsight.remote` decorator
2. Implement new serialization format
3. Add `serialization_version` header to requests
4. Server supports both old (cloudpickle) and new (source+JSON) formats

### Phase 2: Encourage adoption

1. Add warnings when using non-remote objects in traces
2. Document the new approach
3. Update nnterp and other libraries to use `@nnsight.remote`

### Phase 3: Deprecate cloudpickle

1. Emit deprecation warnings for cloudpickle serialization
2. Set timeline for removal
3. Eventually require source-based serialization

---

## Backward Compatibility

During the transition period, nnsight will maintain full backward compatibility by falling back to cloudpickle when source-based serialization isn't possible.

### Client-Side: Try New Path, Fall Back with Warning

```python
# nnsight/intervention/serialization.py

import warnings
from typing import Tuple

def save(tracer) -> Tuple[bytes, str]:
    """
    Serialize tracer for remote execution.

    Attempts source-based serialization first. Falls back to cloudpickle
    with a deprecation warning if any variables can't be serialized.

    Returns:
        Tuple of (serialized_bytes, format_string)
        format_string is either "source" or "cloudpickle"
    """
    try:
        return serialize_source_based(tracer), "source"
    except SerializationError as e:
        warnings.warn(
            f"Falling back to cloudpickle serialization:\n"
            f"  {e}\n\n"
            f"This requires matching Python versions between client and server.\n"
            f"To use version-agnostic serialization:\n"
            f"  - Use JSON-serializable variables (int, float, str, list, dict)\n"
            f"  - Mark functions and classes with @nnsight.remote\n\n"
            f"Cloudpickle fallback will be removed in nnsight 2.0.",
            DeprecationWarning,
            stacklevel=4
        )
        return serialize_cloudpickle(tracer), "cloudpickle"
```

### Request Header Indicates Format

```python
# nnsight/intervention/backends/remote.py

def request(self, tracer) -> Tuple[bytes, Dict[str, str]]:
    interventions = super().__call__(tracer)

    # Serialize with automatic fallback
    data, serialization_format = save(tracer)

    headers = {
        "nnsight-model-key": self.model_key,
        "nnsight-version": __version__,
        "nnsight-serialization": serialization_format,  # "source" or "cloudpickle"
        "python-version": python_version,  # Still sent for cloudpickle compat
        ...
    }

    return data, headers
```

### Server-Side: Route to Appropriate Deserializer

```python
# NDIF server

def handle_request(request_data: bytes, headers: Dict[str, str], model) -> Any:
    serialization_format = headers.get("nnsight-serialization", "cloudpickle")

    if serialization_format == "source":
        # New path: source + JSON
        return deserialize_source_based(request_data, model)
    else:
        # Legacy path: cloudpickle
        # Verify Python version match
        client_version = headers.get("python-version", "")
        if not versions_compatible(client_version, sys.version):
            raise RemoteException(
                f"Python version mismatch: client={client_version}, "
                f"server={sys.version}. Use @nnsight.remote for "
                f"version-agnostic serialization."
            )
        return deserialize_cloudpickle(request_data, model)
```

### What Users See

**Case 1: Simple trace with primitives only**
```python
with model.trace("Hello") as t:
    h = model.layers[10].output[0]
    result = h.mean().save()
# Uses source path silently, no warning
```

**Case 2: @nnsight.remote function**
```python
@nnsight.remote
def normalize(x):
    return x / x.norm(dim=-1, keepdim=True)

with model.trace("Hello") as t:
    h = normalize(model.layers[10].output[0])
    result = h.mean().save()
# Uses source path silently, no warning
```

**Case 3: @nnsight.remote class**
```python
@nnsight.remote
class Analyzer:
    def __init__(self, top_k):
        self.top_k = top_k

analyzer = Analyzer(10)
with model.trace("Hello") as t:
    result = analyzer.analyze(h)
# Uses source path silently, no warning
```

**Case 4: Non-remote class (legacy code)**
```python
class LegacyAnalyzer:  # No decorator
    def __init__(self, top_k):
        self.top_k = top_k

analyzer = LegacyAnalyzer(10)
with model.trace("Hello") as t:
    result = analyzer.analyze(h)

# Warning:
# DeprecationWarning: Falling back to cloudpickle serialization:
#   Variable 'analyzer' of type 'LegacyAnalyzer' cannot be serialized.
#
# This requires matching Python versions between client and server.
# To use version-agnostic serialization:
#   - Use JSON-serializable variables (int, float, str, list, dict)
#   - Mark functions and classes with @nnsight.remote
#
# Cloudpickle fallback will be removed in nnsight 2.0.
```

**Case 5: @nnsight.remote on non-conforming code**
```python
@nnsight.remote
def bad_function():
    import pandas  # Not allowed
    return pandas.read_csv("data.csv")

# ImportError at import time (loud, fast):
# ImportError: @nnsight.remote validation failed for 'bad_function':
#   Line 2: imports 'pandas' (not available on NDIF server)
```

### Compatibility Matrix

| Scenario | Serialization | Warning | Works? |
|----------|---------------|---------|--------|
| Primitives only | source | No | âœ… |
| @nnsight.remote functions | source | No | âœ… |
| @nnsight.remote classes | source | No | âœ… |
| Non-decorated custom code | cloudpickle | Yes | âœ… (if Python matches) |
| Non-decorated + version mismatch | cloudpickle | Yes | âŒ (clear error) |
| @nnsight.remote with violations | N/A | N/A | âŒ (import-time error) |

### Gradual Migration Path

1. **Phase 1** (nnsight 1.x): Both paths work, warnings guide users
2. **Phase 2** (nnsight 1.x+): Increase warning visibility, add docs
3. **Phase 3** (nnsight 2.0): Remove cloudpickle, source-only

This allows existing code to keep working while nudging users toward the better path.

---

## Testing Strategy

### Unit Tests

```python
def test_remote_valid_function():
    @nnsight.remote
    def normalize(x):
        return x / x.norm()

    assert hasattr(normalize, '_remote_source')
    assert hasattr(normalize, '_remote_module_refs')

def test_remote_valid_class():
    @nnsight.remote
    class ValidClass:
        def __init__(self, x):
            self.x = x
        def compute(self, y):
            return self.x + y

    assert hasattr(ValidClass, '_remote_source')

def test_remote_captures_module_constants():
    # In a module with TOP_K = 10
    @nnsight.remote
    class Analyzer:
        def __init__(self, k=TOP_K):
            self.k = k

    assert Analyzer._remote_module_refs == {'TOP_K': 10}

def test_remote_rejects_external_import():
    with pytest.raises(ImportError, match="imports 'pandas'"):
        @nnsight.remote
        def bad():
            import pandas
            return pandas.DataFrame()

def test_remote_rejects_non_serializable_module_ref():
    # In a module with COMPLEX = SomeClass()
    with pytest.raises(ImportError, match="not JSON-serializable"):
        @nnsight.remote
        def bad():
            return COMPLEX.value

def test_serialization_round_trip():
    @nnsight.remote
    class Kit:
        def __init__(self, top_k):
            self.top_k = top_k

    kit = Kit(10)
    serialized = serialize_for_remote(mock_tracer_with(kit))
    reconstructed = deserialize(serialized)

    assert reconstructed.top_k == 10
```

### Integration Tests

```python
def test_remote_class_in_trace():
    @nnsight.remote
    class Analyzer:
        def __init__(self, model, layers):
            self.model = model
            self.layers = layers

        def get_hidden(self, layer):
            return self.model.transformer.h[layer].output[0]

    model = nnsight.LanguageModel("gpt2")
    analyzer = Analyzer(model, [5, 10, 15])

    with model.trace("Hello world", backend="remote"):
        results = []
        for layer in analyzer.layers:
            h = analyzer.get_hidden(layer)
            results.append(h.mean().save())

    assert len(results) == 3

def test_remote_function_in_trace():
    @nnsight.remote
    def normalize(x):
        return x / x.norm(dim=-1, keepdim=True)

    model = nnsight.LanguageModel("gpt2")

    with model.trace("Hello world", backend="remote"):
        h = model.transformer.h[10].output[0]
        h_norm = normalize(h)
        result = h_norm.mean().save()

    assert result is not None
```

## Exhaustive Test Plan

To ensure the robustness of the source-based serialization, the following test cases must be implemented.

### 1. Validation Logic (Unit Tests)
**`validate_ast` & Decorator Logic**
*   **Allowed Imports**: Verify `torch`, `numpy`, `math`, `collections`, `itertools`, `functools`, `operator` are accepted.
*   **Disallowed Imports**: Verify `pandas`, `sklearn`, `requests` raise `ImportError` with correct line number.
*   **Allowed Attribute Chains**: Verify `os.path.join`, `pathlib.Path("x")`, `random.randint` (if allowed) are accepted.
*   **Disallowed Attribute Chains**: Verify `os.system`, `subprocess.run`, `pathlib.Path.read_text` raise errors.
*   **Disallowed Calls**: Verify `open()`, `exec()`, `eval()` raise errors.
*   **Class Validation**:
    *   Verify inheritance from `object` or another `@nnsight.remote` class is allowed.
    *   Verify inheritance from a non-decorated class raises an error.
    *   Verify usage of metaclasses or `__slots__` raises an error.
*   **Module References**:
    *   Verify capturing simple constants (`TOP_K = 10`).
    *   Verify rejection of complex non-serializable constants (`CONFIG = ComplexObj()`).
    *   Verify ignoring aliases to allowed modules (`import numpy as np`).

### 2. Serialization Logic (Unit Tests)
**`extract_variables` & `serialize_instance_state`**
*   **Internal Filtering**: Ensure variables starting with `_nnsight` or `nnsight` are NOT present in the serialized output.
*   **Model Heuristics**:
    *   Pass a mock object named `Envoy` -> Verify serialized as `{"__model_ref__": True}`.
    *   Pass a mock object with module `nnsight.modeling.LanguageModel` -> Verify serialized as `{"__model_ref__": True}`.
    *   Pass a random object -> Verify `SerializationError` or fallback behavior.
*   **Callable References**:
    *   Assign `self.act = torch.nn.functional.relu` -> Verify serialized as `{"__callable_ref__": "torch.nn.functional.relu"}`.
    *   Assign `self.act = custom_func` (undecorated) -> Verify error.
*   **Nested Remote Objects**:
    *   Create `ClassA` holding instance of `ClassB` (both decorated). Verify `ClassA` state contains `{"__remote_ref__": ID_OF_B}`.
    *   Create `ClassA` holding instance of `ClassC` (undecorated). Verify `SerializationError`.
*   **Circular References**: (If supported) Create two instances referencing each other. Verify no infinite recursion in serialization (though JSON dump might fail if not handled).

### 3. Server-Side Reconstruction (Unit Tests)
**`deserialize_and_execute` & `reconstruct_state`**
*   **Callable Resolution**: Verify `{"__callable_ref__": "torch.nn.functional.relu"}` resolves to the actual function.
*   **Model Injection**: Verify `{"__model_ref__": True}` is replaced by the actual model object passed to the deserializer.
*   **Shared Instances**:
    *   Serialize two variables pointing to the same `@nnsight.remote` instance.
    *   Deserialize and verify `var1 is var2` (identity preservation).
*   **Module Scope**: Verify that reconstructed classes have access to their captured module-level constants (e.g., `TOP_K`).

### 4. Integration Tests (End-to-End)
*   **Simple Function**: Trace using a `@nnsight.remote` function. Verify execution on "server" (mocked).
*   **Simple Class**: Trace using a `@nnsight.remote` class instance.
*   **Library Simulation**:
    *   Define a "library" module with version metadata.
    *   Use it in a trace.
    *   Verify serialization includes library/version info.
*   **Complex Flow**:
    *   Trace using `itertools.chain`, `functools.partial`, and `collections.namedtuple` (allowed modules).
    *   Verify correct execution.
*   **Standard Library Usage**: Use `os.path.join` inside a remote function. Verify it works.

## Error Handling & Source Mapping

To allow server-side errors to be mapped back to client-side code, the serialization format tracks origin information.

1.  **Client-Side**: When capturing source, we annotate each block with its filename and starting line number.
    ```json
    "source": [
        {"code": "def foo():...", "file": "myscript.py", "line": 42}
    ]
    ```
2.  **Server-Side**: The execution engine wraps user code in a `try/except` block. If an exception occurs, it inspects the traceback, finds the line number in the generated server code, maps it back to the client's original file/line using the metadata, and re-raises the exception with the corrected location.

## Lambda Support

**Status: Supportable with limitations (Python 3.8+)**

Lambda functions were thought to be fragile for source extraction. After investigation, the situation is more nuanced:

### The Classic Problem (Solvable)

When multiple lambdas appear on the same line, `inspect.getsource` returns the entire line for all of them:

```python
f1, f2 = lambda x: x + 1, lambda x: x - 1
# inspect.getsource(f1) returns: "f1, f2 = lambda x: x + 1, lambda x: x - 1"
# inspect.getsource(f2) returns: "f1, f2 = lambda x: x + 1, lambda x: x - 1"  # Same!
```

**Solution: AST parsing + bytecode matching**

1. Parse the source line with `ast.parse()` to find all `Lambda` nodes
2. Use `col_offset` and `end_col_offset` (Python 3.8+) to extract each lambda's text
3. Compile each extracted lambda and compare bytecode to match the right one

```python
# AST gives us precise positions:
# Lambda 0: cols 9-24  -> "lambda x: x + 1"
# Lambda 1: cols 26-41 -> "lambda x: x - 1"

# Bytecode differs (+ vs -), so we can match correctly
```

This handles:
- âœ“ Multiple lambdas on same line: `f1, f2 = lambda x: x+1, lambda x: x-1`
- âœ“ Lambdas in collections: `{'add': lambda a,b: a+b, 'mul': lambda a,b: a*b}`
- âœ“ Comprehension lambdas: `[lambda x, i=i: x**i for i in range(3)]`
- âœ“ Complex expressions: `lambda x: "yes" if x > 0 else "no"`
- âœ“ Default arguments: `lambda x, y=10: x + y`
- âœ“ Closures: `lambda x: x * external_var`

### Truly Problematic Cases (Reject)

These cases should be explicitly rejected with clear error messages:

**A. Multi-line lambdas:**
```python
f = (
    lambda x:
        x * 2 + 1
)
```
Source extraction returns partial/unparseable text. Rare in practice.

**B. Nested inner lambdas:**
```python
outer = lambda x: (lambda y: x + y)
inner = outer(10)  # inner's source can't be extracted
```
The inner lambda returns the outer's source line. Bytecode won't match due to closure differences.

**C. Identical lambdas on same line:**
```python
f1, f2 = lambda x: x, lambda x: x  # Can't distinguish
```
Bytecode is identical, so matching returns the first one. Very rare edge case.

### Implementation Recommendation

**Support single-line lambdas with clear limitations:**

1. Lambdas must be defined on a single line
2. Lambdas must not be inner nested lambdas (returned from another lambda)
3. When extraction fails, provide a clear error asking for a named function

This covers 90%+ of real-world lambda usage while avoiding fragile edge cases.

**Algorithm:**
```python
def extract_lambda_source(func):
    full_source = inspect.getsource(func).strip()
    tree = ast.parse(full_source)

    lambdas = []
    for node in ast.walk(tree):
        if isinstance(node, ast.Lambda) and hasattr(node, 'end_col_offset'):
            lambdas.append(full_source[node.col_offset:node.end_col_offset])

    if len(lambdas) == 1:
        return lambdas[0]

    # Multiple lambdas - match by bytecode
    target_bytecode = func.__code__.co_code
    for lambda_text in lambdas:
        compiled = compile(lambda_text, '<lambda>', 'eval')
        if compiled.co_consts[0].co_code == target_bytecode:
            return lambda_text

    raise ExtractionError("Could not extract lambda source")
```

### 5. Security Model (Critical)

**Defense in Depth**

NDIF is a research platform for vetted researchers who agree to Terms of Service. Security is implemented in layers:

1. **Layer 1: AST Validation (Client-side linting)** - Catches accidental misuse
2. **Layer 2: RestrictedPython (Server-side runtime guards)** - Blocks known escape routes + audit logging
3. **Layer 3: OS-level Sandbox (Infrastructure)** - Hard boundary via containers/gVisor

**Layer 1: AST Validation (NOT a security boundary)**

The AST validation in `@nnsight.remote` acts as a **linter** to prevent accidental usage of unsupported features. It blocks obvious dangerous patterns like `os.system()` or `subprocess.run()` at the source level. However:
- Dynamic code execution (e.g., `getattr(os, "sys"+"tem")`) can bypass static analysis
- This layer is for developer experience, not security

**Layer 2: RestrictedPython Runtime Guards + Audit Logging**

Server-side code execution uses [RestrictedPython](https://restrictedpython.readthedocs.io/) to add runtime guards. This provides:

1. **Compile-time blocking** of dangerous patterns:
   - All dunder attribute access (`__class__`, `__bases__`, `__subclasses__`, `__globals__`, etc.)
   - These are common sandbox escape routes and are rejected before execution

2. **Runtime guards** with audit logging:
   - All attribute access (`x.attr`) is wrapped with `_getattr_()`
   - All item access (`x[key]`) is wrapped with `_getitem_()`
   - Custom guards log suspicious activity before blocking

**Audited Activities**

The following activities are logged to the security audit system with user ID and job ID:

| Activity | Trigger | Audit Log Entry |
|----------|---------|-----------------|
| Dangerous attribute access | `getattr(os, 'system')` or `os.system` | `SUSPICIOUS_GETATTR \| user=X \| job=Y \| attr=system` |
| Frame inspection | Access to `f_back`, `f_locals`, `f_globals` | `SUSPICIOUS_GETATTR \| user=X \| job=Y \| attr=f_back` |
| Code object access | Access to `__code__`, `gi_frame`, `cr_frame` | `SUSPICIOUS_GETATTR \| user=X \| job=Y \| attr=__code__` |
| Blocked imports | `import subprocess` | `BLOCKED_IMPORT \| user=X \| job=Y \| module=subprocess` |
| Namespace introspection | `globals()`, `locals()`, `vars()` | `SUSPICIOUS_CALL \| user=X \| job=Y \| func=globals` |

**Suspicious Attribute Blocklist:**

```python
SUSPICIOUS_ATTRS = {
    # Sandbox escape routes
    'system', 'popen', 'spawn', 'fork', 'exec', 'eval',
    # Frame inspection (can leak local variables)
    'f_back', 'f_locals', 'f_globals', 'f_code', 'f_builtins',
    # Generator/coroutine frame access
    'gi_frame', 'gi_code', 'cr_frame', 'cr_code',
    # Code object manipulation
    '__code__', '__globals__', '__builtins__', '__closure__',
}
```

**Audit Response Protocol:**

1. **Single attempt**: Log and block, no action needed
2. **Repeated attempts (>3 in session)**: Flag user for review
3. **Pattern of probing across sessions**: Notify security team

**Layer 3: OS-Level Sandbox (Required)**

RestrictedPython is defense-in-depth, not a complete sandbox. The [pysandbox project](https://wiki.python.org/moin/SandboxedPython) concluded that "putting a sandbox in CPython is the wrong design" due to Python's extensive introspection capabilities.

**Requirement:** The NDIF server must execute user code in a **hard sandbox**:
- Ephemeral containers or gVisor
- No network access (except to GPU cluster)
- Read-only filesystem (except temp buffers)
- Resource limits (CPU, memory, time)

**Trust Model:**
- Users are vetted researchers with ToS agreement
- Server treats all code as potentially buggy, not necessarily malicious
- Audit logging provides visibility into unusual behavior
- Hard sandbox prevents damage even if Python-level escapes exist

**Local Validation (Client-side)**

Users can test whether their code is remote-safe locally before submitting to NDIF. This catches obvious issues early and provides immediate feedback for honest mistakes.

**Security design principle:** Local validation only performs *static analysis*. Runtime guards (getattr, getitem, dynamic imports) run only server-side with audit logging. This helps legitimate users fix issues while not telegraphing runtime checks to potential attackers.

**API Design:**

```python
# Option 1: Trace-level validation (recommended)
with model.trace("Hello", validate_remote=True):
    h = model.layers[10].output[0]
    result = analyze(h).save()
# Raises SecurityAuditError for static violations
```

```python
# Option 2: Function-level check
from nnsight import check_remote_safe

@nnsight.remote
def my_func(x):
    import subprocess  # Would be caught
    return x

safe, errors = check_remote_safe(my_func)
# Returns (False, ["Import of 'subprocess' is not allowed..."])
```

**What local validation checks (static analysis):**

1. **AST import analysis** - Scans all function bodies for blocked/unauthorized imports
2. **RestrictedPython compilation** - Blocks dunder access (`__class__`, `__bases__`, `_private`)
3. **Closure variable serialization** - Ensures captured variables are JSON-serializable
4. **Tensor serialization** - Validates tensor types are supported

**What local validation does NOT check (server-side only):**

- Dynamic attribute access (`getattr(obj, 'system')`)
- Dynamic imports (`__import__('subprocess')`)
- Runtime frame inspection attempts

These runtime checks run server-side with audit logging. Attackers who bypass static analysis are caught and logged on the server without advance warning.

**Error messages (client-side):**

```
SecurityAuditError: Import of 'subprocess' is not allowed in remote code.

Allowed modules: torch, numpy, math, collections, functools, itertools,
                 operator, os (path only), pathlib (path only), random
```

### 6. Robustness Details

**Closures & Free Variables**
Serialization must extend beyond local variables to include **closure variables**.
*   If a `@nnsight.remote` function is defined inside another function and captures variables, the serializer must inspect `func.__closure__` and `func.__code__.co_freevars` to serialize those captured values recursively.

**Class Method Closures (including `__init_subclass__`)**

For `@nnsight.remote` classes, closures in methods are also captured. This is especially important for `__init_subclass__` hooks that capture variables from the defining scope:

```python
def make_tracked_base():
    registered = []  # Closure variable

    @remote
    class TrackedBase:
        def __init_subclass__(cls, **kwargs):
            super().__init_subclass__(**kwargs)
            registered.append(cls.__name__)  # Uses closure

    return TrackedBase, registered

Base, registry = make_tracked_base()
```

The `extract_closure_variables()` function iterates through all class methods (including `__init__`, `__init_subclass__`, `__new__`, `__call__`, and custom methods) to find and serialize closure variables.

**Special handling:**
- `__class__` is skipped (implicit closure from `super()` calls)
- Module references from allowed modules are skipped (available on server)
- `@nnsight.remote` objects in closures are skipped (serialized separately)
- Only JSON-serializable values are captured; non-serializable closures trigger validation errors

**Serialization Hierarchy & Strictness**
We do **not** support arbitrary pickle-able objects, as this reintroduces version fragility. The serialization follows a strict hierarchy:

1.  **Primitives**: `int`, `str`, `list`, `dict`, etc. (Pass-through).
2.  **Tensors**: `torch.Tensor`, `numpy.ndarray` (Base64 + optional zlib compression).
3.  **Enums**: `enum.Enum` instances (serialized as class/module/member, reconstructed via importlib).
4.  **Model References**: Explicit `Envoy` / `LanguageModel` types (Serialized as reference).
5.  **@nnsight.remote Objects**:
    *   We ship the source code.
    *   We serialize `__dict__` or `__slots__` recursively (handles nested dicts, tensors, child modules).
    *   Reconstruction uses `object.__new__()` + state population.
6.  **Everything Else**: **Error**. "Object of type X is not serializable. Mark it `@nnsight.remote` or convert to primitive."

**Enum Serialization**

Enum instances from `enum.Enum` subclasses are serialized with their class, module, and member name:

```python
from enum import Enum

class Color(Enum):
    RED = 1
    GREEN = 2
    BLUE = 3

@remote
class WithEnum:
    def __init__(self, color):
        self.color = color  # Color.RED
```

**Wire format:**
```json
{
    "__enum__": true,
    "class": "Color",
    "module": "__main__",
    "member": "RED"
}
```

**Reconstruction:** The server uses `importlib` to import the enum class and access the member by name:
```python
import importlib
module = importlib.import_module(module_name)
enum_class = getattr(module, class_name)
value = enum_class[member_name]  # Color.RED
```

**Fallback:** If the enum class isn't available on the server (e.g., defined locally in test code), a fallback dict is returned with the class and member names, allowing graceful degradation.

**Instance Reconstruction via `object.__new__()` + State Population**

Most Python classes store their state in `__dict__`. We reconstruct instances by:

1. Creating an empty instance with `object.__new__(cls)` (bypasses `__init__`)
2. Populating state from `__dict__` or `__slots__`

```python
# Serialization
state = serialize_instance_state(obj)  # Recursively serialize __dict__ or __slots__

# Deserialization
restored = object.__new__(cls)
restored.__dict__ = reconstruct_state(state, namespace, model, {})
```

**Why this works for `nn.Module`:**

While `nn.Module` stores parameters in special attributes like `_parameters` and `_buffers`, these ARE in `__dict__`:

```python
model.__dict__ = {
    '_parameters': {'weight': Parameter(...), 'bias': Parameter(...)},
    '_buffers': {},
    '_modules': {'layer1': Linear(...)},
    'training': True,
    ...
}
```

The recursive serialization handles nested dicts, `nn.Parameter` objects, and child `nn.Module` instances automatically.

**`__slots__` Class Support**

Classes using `__slots__` for memory optimization are fully supported. The serialization iterates through the MRO to find all slots and serializes their values:

```python
@remote
class Point:
    __slots__ = ['x', 'y']

    def __init__(self, x, y):
        self.x = x
        self.y = y

# Serialization iterates slots instead of __dict__
state = {'x': 1.0, 'y': 2.0}
```

**Implementation details:**
- Walks `cls.__mro__` to collect all `__slots__` from parent classes
- Skips `__dict__` and `__weakref__` slots (internal Python machinery)
- Uses `getattr()` to read slot values (handles unset slots gracefully)
- If a slotted class also has `__dict__`, both are serialized

```python
# In serialize_instance_state():
for klass in cls.__mro__:
    if hasattr(klass, '__slots__'):
        for slot in klass.__slots__:
            if slot not in ('__dict__', '__weakref__'):
                if hasattr(obj, slot):
                    state[slot] = serialize_value(getattr(obj, slot), ...)
```

**torch.nn.Module Support**

`@nnsight.remote` allows `torch.nn.Module` as a base class:

```python
@nnsight.remote
class MyModule(torch.nn.Module):
    def __init__(self, in_features, out_features):
        super().__init__()
        self.linear = torch.nn.Linear(in_features, out_features)
```

**Serialization handles:**
- `nn.Parameter` objects (serialized with `requires_grad` flag)
- Nested `_parameters`, `_buffers`, `_modules` dicts
- Built-in torch modules like `nn.Linear` (class path + `__dict__`)
- Training mode (`training` attribute)

**This approach:**
- **Works automatically** - No special code required from users
- **Preserves trained weights** - Parameters serialized with tensor data
- **Handles nested modules** - Recursive `__dict__` serialization
- **Supports custom modules** - Any `nn.Module` subclass works
- **No `__init__` arg capture** - Simpler, more robust

**Notebook & REPL Support**
`inspect.getsource` is brittle in interactive environments.
*   **Strategy**: If standard source extraction fails, the system will attempt to retrieve source code from the IPython history manager (if available) or use robust third-party helpers (like `dill.source`) to locate the definition.

---

## Open Questions

1. **Lambda handling**: Should we support lambdas via source extraction? Or require explicit functions? (See "Lambda Support" section above - currently proceeding with caution).

2. **Async support**: Does the design need to change to support `async/await` in remote traces?

3. **Global state**: How do we handle libraries that depend on global state (e.g., `torch.set_grad_enabled`)? Should we reset state between requests?

---

## Appendix: Allowed Operations in @nnsight.remote

### Always allowed
- Python builtins (`len`, `range`, `zip`, `enumerate`, `hasattr`, `getattr`, `isinstance`, etc.)
- Control flow (`if`, `for`, `while`, `try/except`)
- Arithmetic and comparison operators
- Attribute access and introspection
- Method calls
- Object construction

**Note:** Introspection of the model (e.g., `hasattr(model, 'transformer')`, `model.config.hidden_size`) is fully allowed. This enables libraries like nnterp to detect model architecture dynamically, whether that detection runs client-side or server-side.

### Allowed modules
- `torch.*`
- `numpy.*` (including alias `np`)
- `math.*`
- `collections.*`
- `itertools.*`
- `functools.*`
- `operator.*`
- `os.*` (path manipulation only; I/O and process operations blocked)
- `pathlib.*` (path manipulation only; I/O operations blocked)
- `random.*`

### Disallowed
- File I/O (`open`, `Path.read_text`, etc.)
- Network (`requests`, `urllib`, `socket`)
- Subprocess (`os.system`, `subprocess`)
- Dynamic code (`exec`, `eval`, `compile`)
- Threading (`threading`, `multiprocessing`)
- Non-allowed imports (e.g., `pandas`, `sklearn`)

---

## Summary

This design replaces cloudpickle with source-based serialization:

| Aspect | Current (cloudpickle) | Proposed (source+JSON) |
|--------|----------------------|------------------------|
| Python version | Must match exactly | Any version |
| Third-party libs | Must be on server | @nnsight.remote works anywhere |
| Failure mode | Runtime, mysterious | Import-time, clear |
| Flexibility | Appears unlimited | Explicitly constrained |
| Library ecosystem | Requires NDIF coordination | Organic growth |

The key insight is that nnsight already captures source as text. We're simply changing the serialization of closure variables from cloudpickle to JSON, with the `@nnsight.remote` decorator providing validation and enabling classes/functions to be shipped with their source.

**One decorator for everything**: `@nnsight.remote` works on both functions and classes. Module-level constants are auto-captured if JSON-serializable. The mental model is simple: "Mark things you use in traces with `@nnsight.remote`."

---

## Deferred Features: Assessment and Future Plans

This section documents features from the design that are **intentionally deferred** to future work, along with rationale and implementation paths.

### 1. Tensor/Array Serialization

**Status**: IMPLEMENTED (v2.1)

**What it does**: Serialize `torch.Tensor` and `numpy.ndarray` objects as base64-encoded bytes within the JSON payload, with optional zlib compression for compressible data (zeros, sparse patterns).

**Wire Format**:
```json
{
    "__tensor__": "base64-encoded-bytes...",
    "dtype": "float32",
    "shape": [1024, 768],
    "compressed": true
}
```

**Special Cases Handled**:

| Tensor Type | How Handled | Wire Format |
|-------------|-------------|-------------|
| Dense float32/64 | Direct numpy bytes | `dtype: "float32"` |
| bfloat16 | View as int16 (no numpy equiv) | `dtype: "int16", torch_dtype: "bfloat16"` |
| Sparse COO | Indices + values separately | `sparse: {indices: ..., dense_shape: [...]}` |
| Quantized | Preserve int repr + params | `quantization: {scale, zero_point, qtype}` |
| Nested tensors | Rejected with clear error | N/A |

**Compression Strategy**:
- Try zlib level=1 (fast)
- Only use if saves â‰¥10% (avoids overhead for random floats)
- Zeros/sparse compress to ~1% of original size

**Size Overhead**:
| Data Pattern | Compression | Final Size vs Raw |
|--------------|-------------|-------------------|
| Random floats | Not used | ~133% (base64 only) |
| All zeros | Used | ~1% |
| Mostly zeros (1% nonzero) | Used | ~1-2% |
| Repeated patterns | Used | ~1% |

**Typical Use Cases**:
- Steering vectors (768-4096 floats): trivial, <100KB
- Probe weights: small, <1MB
- Linear layer for patching: medium, 1-50MB
- Full models: NOT a use case (model lives on server)

**Future Optimization**: Binary sidecars could eliminate the ~33% base64 overhead:
```
payload = {
    "json": {..., "tensors": {"my_vec": {"sidecar": 0, "dtype": "float32", ...}}},
    "sidecars": [compressed_bytes_0, compressed_bytes_1, ...]
}
```
This would use a multipart format (JSON header + raw binary blobs) instead of embedding base64 in JSON. The current approach prioritizes simplicity and pure-JSON transport compatibility.

### 2. Flat Graph with GraphFlattener

**Status**: Not yet implemented

**What it does**: Restructure the JSON payload to use a flat `objects` map instead of a tree structure:
```json
{
  "objects": {
    "root": {...},
    "obj_123": {"__type__": "MyClass", ...}
  }
}
```

This enables support for circular references and shared objects.

**Why deferred**: The current tree structure works for 99% of real-world use cases. Circular references in trace variables are rare, and shared references are already handled via `__remote_ref__`.

**Current behavior**: Circular references trigger a clear error: "Circular reference detected. Circular references between @nnsight.remote objects are not yet supported."

**Future plan**: Implement the flat graph structure when concrete use cases emerge. The main benefit would be memory efficiency for large object graphs with sharing.

### 3. `__getstate__`/`__setstate__` Pickle Hooks

**Status**: Not yet implemented

**What it does**: Check for `__getstate__` and `__setstate__` methods on `@nnsight.remote` objects, enabling custom serialization logic.

**Why deferred**: The default behavior (`__dict__` serialization) covers most use cases. Custom pickle hooks add complexity and could introduce hard-to-debug issues if the hooks aren't themselves remote-safe.

**Current behavior**: All `@nnsight.remote` objects are serialized via `__dict__`.

**Future plan**: Add support when library authors request it. Should validate that hooks only return JSON-serializable data.

### 4. Source Hashing for Cache Verification

**Status**: Not yet implemented

**What it does**: Hash the client's source code and include it in the payload. Server verifies the hash matches its installed version before using cached code.

**Why deferred**: The `version` parameter on `@nnsight.remote(library="...", version="...")` provides sufficient cache invalidation for most cases. Hash verification adds overhead and complexity.

**Current behavior**: Version string is included in the payload. Server can use this for cache decisions.

**Future plan**: Consider implementing if version-based caching proves insufficient (e.g., users frequently patch installed libraries).

### 5. Notebook/REPL Support

**Status**: Not yet implemented

**What it does**: Fall back to IPython history manager when `inspect.getsource` fails in interactive environments.

**Why deferred**: IPython source extraction is fragile and version-dependent. The current implementation works with `.py` files, which is the primary use case for production traces.

**Current behavior**: Source extraction may fail in notebooks, triggering cloudpickle fallback with deprecation warning.

**Future plan**: Investigate `dill.source` or IPython's `_ih` history list as fallback mechanisms. Consider requiring explicit file-based code for remote execution.

### 6. Full Circular Reference Support via Flat Graph

**Status**: Partial (cycle detection with clear error)

**What it does**: Instead of raising an error for circular references, serialize them correctly using the flat graph approach.

**Why deferred**: Requires flat graph implementation (item #2 above).

**Current behavior**: Self-references trigger `SourceSerializationError`. References between different `@nnsight.remote` objects use `__remote_ref__` which avoids recursion.

**Future plan**: Implement as part of flat graph work.

### 7. Lambda Support

**Status**: IMPLEMENTED (v2.1)

**What it does**: Support lambda expressions via AST parsing + bytecode matching.

**Investigation findings**: The "lambda fragility" concern is partially overblown:
- **Solvable**: Multiple lambdas on same line can be disambiguated via bytecode comparison
- **Truly problematic**: Multi-line lambdas, nested inner lambdas, identical lambdas on same line

**Implementation**: The `extract_lambda_source()` function uses:
1. AST parsing to find Lambda nodes with `col_offset`/`end_col_offset`
2. Bytecode matching for disambiguation
3. Clear error for unsupported cases (multi-line, nested inner)
4. Closure variable capture with JSON serialization

### Lambda Closure Serialization: What It Looks Like On the Wire

When a lambda captures closure variables, those values are serialized alongside the lambda source. Here's a concrete walkthrough:

**Client-side code:**
```python
def make_scaler(factor):
    return lambda x: x * factor

scale_by_5 = make_scaler(5)  # Captures factor=5 in closure
```

**What the serializer does:**

1. Extract lambda source: `"lambda x: x * factor"`
2. Inspect closure via `func.__closure__` and `func.__code__.co_freevars`
3. Find captured variable: `factor = 5`
4. Serialize to JSON:

```json
{
  "__lambda_scale_by_5": {
    "type": "lambda",
    "var_name": "scale_by_5",
    "source": {
      "code": "lambda x: x * factor",
      "file": "user_script.py",
      "line": 4
    },
    "closure_vars": {
      "factor": 5
    }
  }
}
```

**Server-side reconstruction:**

```python
# Inject closure variables into namespace
namespace = {"factor": 5}

# Execute lambda definition
exec("scale_by_5 = lambda x: x * factor", namespace)

# Now scale_by_5 is a working function
result = namespace["scale_by_5"](10)  # Returns 50
```

**Closure Variable Handling Rules:**

| Closure Value Type | Action |
|-------------------|--------|
| JSON-serializable (int, str, list, dict) | Captured in `closure_vars` |
| Module from allowed list (`torch`, `numpy`) | Skipped (available on server) |
| `@nnsight.remote` object | Skipped (serialized separately) |
| Tensor, non-serializable object | Error with clear message |
| Nested lambda | Error with clear message |

**Example error for non-serializable closure:**

```python
def make_biased(bias_tensor):
    return lambda x: x + bias_tensor

bias = torch.tensor([1.0, 2.0, 3.0])
biased_func = make_biased(bias)  # Captures tensor in closure
```

```
SourceSerializationError: Lambda 'biased_func' has non-serializable closure variable
'bias_tensor' of type 'Tensor'.

Options:
  - Pass the value as a function parameter instead of capturing it
  - Convert to a JSON-serializable type (int, float, str, list, dict)
  - Use a @nnsight.remote function instead of a lambda
```

This ensures that closure behavior is transparent and predictable, with clear guidance when unsupported patterns are detected.

---

## LanguageModel Subclasses and Remote Execution

### The Problem: Third-Party Model Wrappers

Libraries like **nnterp** provide `LanguageModel` subclasses that add convenience features:

```python
from nnterp import StandardizedTransformer

model = StandardizedTransformer("gpt2")

# Standardized accessors (nnterp-specific)
model.layers[5]       # Access any layer uniformly
model.num_layers      # Get layer count
model.hidden_size     # Get hidden dimension
layer.self_attn       # Standardized attention accessor
layer.mlp             # Standardized MLP accessor
```

**Challenge**: How can this code work remotely when the server doesn't have nnterp installed?

### Solution: Full Subclass Serialization

The solution automatically serializes the entire LanguageModel subclass (including helper classes and all dependencies) so it can be reconstructed on the server:

1. **Auto-discovery** finds all classes needed to reconstruct the subclass
2. **Server imports tracking** records module-level references by name and module path
3. **Instance state** captures custom attributes (distinguishing server-provided vs custom)
4. **Reconstruction** creates a working subclass instance on the server

#### How It Works

When a trace uses a `LanguageModel` subclass, the serialization system:

1. **Detects** the subclass via `is_languagemodel_subclass()`
2. **Auto-discovers** the class source and all non-server-available dependencies
3. **Records server imports** - maps variable names to their module paths
4. **Serializes instance state** - custom attributes that aren't server-provided
5. **Sends** the class definitions + state + model key to server

```python
# Client-side serialization
subclass_data = serialize_model_subclass(model)
# Returns: {
#   "class_name": "StandardizedTransformer",
#   "discovered_classes": {...},  # 8 classes with source code
#   "state": {...},               # Custom instance attributes
#   "model_key": "..."            # For base model reconstruction
# }
```

#### Server-Available Classes

The system distinguishes between classes that need serialization and those already on the server:

```python
def is_server_available_class(cls: type) -> bool:
    """Check if a class is from a module available on the server."""
    root = cls.__module__.split('.')[0]

    # nnsight classes are on the server
    if 'nnsight' in cls.__module__:
        return True

    # Standard library
    if root in {'builtins', 'abc', 'enum', 'typing', 'collections',
                'functools', 'dataclasses', ...}:
        return True

    # Major ML libraries on server
    if root in {'torch', 'numpy', 'transformers', 'einops', ...}:
        return True

    return False
```

Classes from nnterp, user code, or other third-party libraries are NOT server-available and must be serialized.

#### Server Imports Tracking

A key insight: discovered classes reference types, functions, and modules from server-available packages. These references must be recorded **by their name in the source code** (handling aliases correctly):

```python
# If nnterp has: import torch as th
# And uses: th.Tensor
# We record: {"th": {"type": "module", "module": "torch"}}

# If it has: from abc import ABC
# And uses: class Foo(ABC):
# We record: {"ABC": {"type": "class", "module": "abc", "name": "ABC"}}
```

**Server imports captured for StandardizedTransformer:**

| Class | Count | Imports |
|-------|-------|---------|
| StandardizedTransformer | 7 | `th` (torch module), `Module`, `Size`, `LanguageModel`, `TraceTensor`, `AutoTokenizer`, `PreTrainedTokenizerBase` |
| AttentionProbabilitiesAccessor | 5 | `th`, `TraceTensor`, `GPT2LMHeadModel`, `GPTJForCausalLM`, `BloomForCausalLM` |
| AttnProbFunction | 2 | `ABC`, `abstractmethod` |
| LayerAccessor | 2 | `Envoy`, `TraceTensor` |
| RenameConfig | 1 | `dataclass` |
| IOType | 1 | `Enum` |
| DummyCache | 0 | â€” |
| RenamingError | 0 | â€” |

**Total: 18 server imports across 8 classes**

#### Server-Side Reconstruction

On the server, `reconstruct_model_subclass()`:

1. Resolves server imports via `importlib` (using recorded module paths)
2. Executes discovered class definitions in order
3. Creates instance with `object.__new__(cls)` (bypasses `__init__`)
4. Copies server-provided attributes from the base model
5. Overrides with custom state from serialized data

```python
def reconstruct_model_subclass(subclass_data, server_model, namespace, exec_func):
    # 1. Execute class definitions with resolved imports
    for cls_name, cls_data in discovered_classes.items():
        # Resolve server imports (handles aliases correctly)
        for ref_name, import_info in cls_data['server_imports'].items():
            if import_info['type'] == 'module':
                namespace[ref_name] = importlib.import_module(import_info['module'])
            elif import_info['type'] in ('class', 'callable'):
                mod = importlib.import_module(import_info['module'])
                namespace[ref_name] = getattr(mod, import_info['name'])

        # Execute class definition
        exec_func(cls_data['source']['code'], namespace)

    # 2. Create instance and populate state
    subclass = namespace[class_name]
    reconstructed = object.__new__(subclass)

    # Copy server-provided attrs from base model
    for key, value in server_model.__dict__.items():
        reconstructed.__dict__[key] = value

    # Override with custom state
    for key, value in custom_state.items():
        if not value.get('__server_provided__'):
            reconstructed.__dict__[key] = value

    return reconstructed
```

### Rename Dicts (Complementary Mechanism)

In addition to class serialization, StandardizedTransformer uses **rename dicts** to map model-specific names to standardized names:

```python
# Included in model_key, sent to server
{
    "h": "layers",           # GPT2's layer list
    "attn": "self_attn",     # Attention module
    "ln_f": "ln_final",      # Final layer norm
    ...  # 36 mappings total for GPT-2
}
```

The server reconstructs the base model with this rename dict, enabling `model.layers`, `layer.self_attn`, etc.

### The `_server_provided` Attribute

The class hierarchy uses `_server_provided` to mark which attributes come from server infrastructure:

```python
class Envoy:
    _server_provided: frozenset = frozenset({
        '_module', '_source', '_interleaver', '_default_mediators',
        '_children', '_alias', '_fake_inputs', '_fake_output',
    })

class NNsight(Envoy):
    _server_provided = Envoy._server_provided | frozenset({'_model'})

class HuggingFaceModel(NNsight):
    _server_provided = NNsight._server_provided | frozenset({'repo_id', 'revision'})

class LanguageModel(HuggingFaceModel):
    _server_provided = HuggingFaceModel._server_provided | frozenset({'tokenizer', 'generator'})
```

**Purpose**: When serializing, `_server_provided` identifies which attributes:
- Should NOT be serialized (they come from server's model)
- Should be replaced with server's values on reconstruction

### Identity-Based Model Reference Detection

When serializing instance state that contains references to the traced model (e.g., `self.model = model`), we use **identity checking**:

```python
def is_the_traced_model(value: Any, traced_model: Any) -> bool:
    """Check if value IS the specific model being traced (by identity)."""
    return traced_model is not None and value is traced_model
```

This ensures `self.model` serializes as `{"__model_ref__": True}` and gets replaced with the server's model.

### Serialization Overhead

For nnterp's StandardizedTransformer with GPT-2:

| Component | Size |
|-----------|------|
| **Total JSON size** | **~33.6 KB** |
| `discovered_classes` (8 classes) | 30,716 bytes |
| `state` (35 attributes) | 1,508 bytes |
| `model_key` (rename dict, etc.) | 1,179 bytes |

**Per-class breakdown:**

| Class | Source Size | Imports |
|-------|-------------|---------|
| StandardizedTransformer | 13.0 KB | 7 |
| AttentionProbabilitiesAccessor | 6.9 KB | 5 |
| RenameConfig | 3.2 KB | 1 |
| LayerAccessor | 2.6 KB | 2 |
| AttnProbFunction | 0.6 KB | 2 |
| IOType | 0.1 KB | 1 |
| RenamingError | 0.1 KB | 0 |
| DummyCache | 0.1 KB | 0 |

This is a one-time cost per model type. ~33KB to transmit an entire third-party library's model wrapper (8 classes, 18 import references) is reasonable overhead.

### End-to-End Tests

The implementation is validated by `tests/test_e2e_remote_nnterp.py` which runs **8 tests** (~3 minutes total). Each test spawns a subprocess with nnterp blocked via import hook, simulating a server without nnterp installed:

**Test 1: Rename Dict Accessors**
1. Client creates `StandardizedTransformer("gpt2")`
2. Client extracts model_key (includes rename dict with 36 mappings)
3. Server (subprocess with nnterp **blocked**) reconstructs base model
4. Tests: `model.layers`, `layer.self_attn`, `layer.mlp` all work

**Test 2: Full Subclass Reconstruction**
1. Client serializes StandardizedTransformer class and state
2. Server reconstructs the full subclass without nnterp
3. Tests custom properties work:
   - `model.num_layers` returns 12 âœ“
   - `model.hidden_size` returns 768 âœ“
   - `model.num_heads` returns 12 âœ“
   - `model.vocab_size` returns 50257 âœ“
4. Tests trace execution with custom properties âœ“

**Test 3: Logit Lens Pattern**
1. Client serializes StandardizedTransformer for logit lens analysis
2. Server iterates all layers using `model.num_layers` and `model.layers[i]`
3. Applies final norm via `model.ln_final` and projection via `model.lm_head`
4. Simulates the exact pattern used in neural-mechanics course notebooks âœ“

**Test 4: Probe Training Workflow**
1. Client creates StandardizedTransformer and probe configuration
2. Server extracts hidden states using:
   - `model.layers[layer_idx].output[0]` for layer outputs
   - `model.hidden_size` for probe input dimension
3. Trains a linear probe classifier on extracted hidden states
4. Verifies training happened (weights changed, accuracy improved) âœ“

This test validates a realistic research workflow where:
- Model wrappers (nnterp) provide standardized accessors
- Hidden states are extracted from specific layers
- External classifiers are trained on those representations
- All without nnterp installed on the server

**Test 5: Steering Vectors**
1. Client creates a steering vector (768-dim tensor)
2. Server applies steering to hidden states: `layer_output[:] += scale * steering_vec`
3. Verifies output changes when steering is applied âœ“

**Test 6: Lambda Closures Over Model**
1. Tests lambda patterns: `get_layer = lambda i: model.layers[i].output[0]`
2. Tests closures with variables: `scale_output = lambda h: h * scale_factor`
3. Tests lambdas using model properties: `lambda: model.layers[model.num_layers // 2]`
4. Tests multi-layer extraction in loops âœ“

**Test 7: @remote Probe with Trained Weights**
1. Client defines `@nnsight.remote class SentimentProbe(nn.Module)`
2. Probe weights are set to specific values (simulating training)
3. Server reconstructs probe class and instance with weights
4. Verifies weights transferred correctly and inference works âœ“

**Test 8: Gradient Computation**
1. Tests extracting hidden states with gradient tracking enabled
2. Tests saliency map pattern: `torch.randn(..., requires_grad=True)`
3. Tests model properties in gradient context âœ“

### Test Cases Verified âœ“

| Test Case | Status | Notes |
|-----------|--------|-------|
| **Steering vectors** | âœ“ Tested | `h[:] = h + steering_vec` works |
| **Lambda closures over model** | âœ“ Tested | All common patterns work |
| **@remote probe class** | âœ“ Tested | Weights serialize/deserialize correctly |
| **Gradient computation** | âœ“ Tested | Tensors are gradient-ready |
| **Multi-model traces** | âœ— Not supported | See limitation below |
| **Attention patterns** | Untested | Low risk |
| **Activation patching** | âœ“ Tested | Edge case test suite |
| **Dataclass with tensors** | âœ“ Tested | Edge case test suite |
| **collect_residuals()** | Untested | Low risk |
| **Batched prompts** | Untested | Low risk |

### Edge Case Unit Tests (20 tests)

In addition to the E2E tests, `tests/test_serialization_edge_cases.py` provides comprehensive unit test coverage for serialization edge cases:

| Test | Description |
|------|-------------|
| `test_slots_class` | `__slots__` classes serialize correctly via slot iteration |
| `test_model_in_container` | Model references in lists/dicts detected correctly |
| `test_numpy_array_serialization` | numpy arrays serialize as tensors |
| `test_properties_not_in_dict` | Properties excluded from state (recomputed on access) |
| `test_custom_pickle_protocol` | `__getstate__`/`__setstate__` honored if present |
| `test_closure_in_method` | Closures in methods detected and documented |
| `test_staticmethod_classmethod` | Static/class methods work in reconstructed classes |
| `test_metaclass` | Metaclasses rejected with clear error |
| `test_init_subclass` | `__init_subclass__` closures captured and serialized |
| `test_relative_import_detection` | Source with relative imports documented |
| `test_type_checking_imports` | TYPE_CHECKING imports handled correctly |
| `test_cross_reference_timing` | Cross-referencing @remote classes works |
| `test_empty_class` | Empty classes serialize correctly |
| `test_lambda_default_argument` | Lambda defaults captured |
| `test_deeply_nested_state` | Deeply nested dicts/lists serialize |
| `test_circular_reference` | Circular refs via `__ref__` mechanism |
| `test_nn_module_with_buffer` | nn.Module with register_buffer works |
| `test_dataclass_serialization` | Dataclasses serialize via `__dict__` |
| `test_enum_in_state` | Enum instances serialize as class+member |
| `test_mixed_tensor_types` | Mixed torch/numpy tensors work |

### Limitation: Single Model Context

The current implementation assumes a **single model context** per trace session. Multi-model traces (using two different LanguageModels in the same session) are **not supported**.

```python
# NOT SUPPORTED
model1 = StandardizedTransformer("gpt2")
model2 = StandardizedTransformer("gpt2-medium")

with model1.trace("Hello"):
    h1 = model1.layers[5].output[0]
    # Cannot reference model2 here - only one model per trace
```

**Why**: The `__model_ref__` serialization mechanism identifies model references by identity (`value is traced_model`). When serializing instance state, any reference to the traced model is replaced with `{"__model_ref__": True}`, which the server replaces with its reconstructed model. This assumes exactly one model is being traced.

**Workaround**: Run separate traces for each model:
```python
with model1.trace("Hello"):
    h1 = model1.layers[5].output[0].save()

with model2.trace("Hello"):
    h2 = model2.layers[5].output[0].save()
```

**Already Covered** (by existing tests):
- Layer iteration (Test 3, 4)
- Custom properties (Test 2)
- Rename dict accessors (Test 1)
- Nested class dependencies (8 classes auto-discovered)

### What Works

| Feature | Status | Mechanism |
|---------|--------|-----------|
| `model.layers` accessor | âœ“ Works | Rename dict |
| `model.layers[i].self_attn` | âœ“ Works | Rename dict |
| `model.num_layers` property | âœ“ Works | Subclass reconstruction |
| `model.hidden_size` property | âœ“ Works | Subclass reconstruction |
| `model.vocab_size` property | âœ“ Works | Subclass reconstruction |
| Custom methods | âœ“ Works | Subclass reconstruction |
| Full trace with properties | âœ“ Works | Combined mechanisms |

### Implementation Notes

**No `@nnsight.remote` Required**: LanguageModel subclasses are automatically detected and serialized without requiring the `@nnsight.remote` decorator. This means third-party libraries like nnterp work without modification.

**Alias Handling**: The `server_imports` mechanism correctly handles module aliases. If nnterp uses `import torch as th`, the serialization records `{"th": {"type": "module", "module": "torch"}}` and the server imports torch under the name `th`.

**Dependency Chain**: The auto-discovery recursively finds all non-server-available dependencies. For StandardizedTransformer, this includes helper classes like `RenameConfig`, `LayerAccessor`, `AttnProbFunction`, etc.

**Graceful Degradation**: If class serialization fails for any reason, the system falls back to just using rename dicts (accessors work, but custom methods/properties don't).

### Potential Optimization: Server-Side Library Detection

**Current Behavior**: The system always transmits source code for:
- LanguageModel subclasses (like nnterp's StandardizedTransformer)
- `@nnsight.remote` decorated code (even with `library=` and `version=` metadata)

This works reliably but has ~33KB overhead per subclass, even if the library is already installed server-side.

**The Optimization**: If the server has the library installed with a compatible version, skip transmission:

```python
# Hypothetical future behavior
if server_has_library("nnterp", version=">=0.1.0"):
    # Fast path: just send import reference
    return {"type": "import", "path": "nnterp.StandardizedTransformer"}
else:
    # Current path: serialize full source
    return serialize_model_subclass(model)
```

**What's Needed**:

1. **Server package manifest**: The NDIF API would expose installed packages:
   ```json
   GET /api/packages
   {"nnterp": "0.1.5", "einops": "0.6.1", ...}
   ```

2. **Client-side check before serialize**: Query the manifest and skip serialization when package exists

3. **Version compatibility**: Handle cases where client has newer/older version than server

4. **Source hash verification** (optional): Even with matching versions, verify source hash to detect local patches

**Same Issue for `@nnsight.remote`**:

The `@nnsight.remote(library="mylib", version="1.0.0")` decorator already captures version metadata:

```python
@nnsight.remote(library="nnterp", version="0.1.0")
class StandardizedTransformer:
    ...
```

The serialization includes this metadata but always sends the source anyway. With server-side detection, we could:
1. Check if `nnterp==0.1.0` is on server
2. If yes, send just `{"import": "nnterp.StandardizedTransformer"}`
3. If no (or version mismatch), send full source

**Trade-offs**:

| Approach | Pros | Cons |
|----------|------|------|
| Always transmit source (current) | Simple, reliable, client version always used | ~33KB overhead even when unnecessary |
| Detect server availability | Minimal overhead when library exists | Requires API changes, version compat logic |
| Hash verification | Catches local patches | Requires computing/comparing hashes |

**Recommendation**: The current "always transmit" approach is acceptable for now (~33KB is trivial compared to model weights). Consider the optimization when:
- The package manifest API exists in NDIF
- Users request faster execution for library-heavy workflows
- Bandwidth becomes a concern for high-volume usage

---

## Implemented Features (v2.1)

The following features from the design document have been implemented:

1. **`@nnsight.remote` decorator** - Validates at import time, captures source and module refs
2. **Module-level constant capture** - JSON-serializable values auto-captured
3. **Module alias detection** - `np`, `F`, `torch` recognized and skipped
4. **AST validation** - Blocks disallowed imports and function calls
5. **Allowed attribute chains** - `os.path.join`, `pathlib.Path()` allowed; I/O operations blocked
6. **Closure variable support** - `__closure__` and `co_freevars` extracted and validated
7. **File/line metadata** - Source payload includes original file and line number
8. **Callable references** - Functions from allowed modules serialized as `__callable_ref__`
9. **Cycle detection** - Clear error for self-referential objects
10. **Version metadata** - `library` and `version` params on decorator for caching
11. **Backward compatibility** - Falls back to cloudpickle with deprecation warning
12. **Lambda support** - Single-line lambdas with AST + bytecode matching, closure variable capture, clear errors for unsupported patterns (multi-line, nested inner, identical on same line)
13. **Tensor serialization** - torch.Tensor and numpy.ndarray as base64+zlib in JSON; handles bfloat16 (via int16 view), sparse COO (preserves sparsity), quantized (preserves int repr + params); nested tensors rejected with clear error
14. **nn.Module support** - `torch.nn.Module` as allowed base class; recursive `__dict__` serialization handles `_parameters`, `_buffers`, `_modules`; `nn.Parameter` preserved with `requires_grad`; built-in torch modules (nn.Linear, etc.) serialized via class path + `__dict__`
15. **nn.Parameter handling** - Serialized with tensor data + `requires_grad` flag; properly reconstructed as `torch.nn.Parameter` on deserialization
16. **`__slots__` class support** - Classes using `__slots__` fully supported; serialization iterates slots via MRO instead of relying on `__dict__`; handles mixed slots+dict classes
17. **Class method closures** - Closures in class methods (including `__init_subclass__`, `__init__`, custom methods) are detected and serialized; `__class__` implicit closure from `super()` is automatically skipped
18. **Enum serialization** - `enum.Enum` instances serialized as `{__enum__, class, module, member}` and reconstructed via `importlib`; graceful fallback if enum class unavailable on server
