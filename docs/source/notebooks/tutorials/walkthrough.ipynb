{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Walkthrough"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Scfr942GwO4"
      },
      "source": [
        "In this era of large-scale deep learning, the most interesting AI models are massive black boxes that are hard to run. Ordinary commercial inference service APIs let you interact with huge models, but they do not let you access model internals.\n",
        "\n",
        "The nnsight library is different: it gives you full access to all the neural network internals. When used together with a remote service like the [National Deep Inference Facility](https://thevisible.net/docs/NDIF-proposal.pdf) (NDIF), it lets you run complex experiments on huge open source models easily, with fully transparent access.\n",
        "\n",
        "Our team wants to enable entire labs and independent researchers alike, as we believe a large, passionate, and collaborative community will produce the next big insights on a profoundly important field."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1OemD2VGyZx"
      },
      "source": [
        "# But first, let's start small\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WyLMmhrAKTNM"
      },
      "source": [
        "## The Tracing Context\n",
        "\n",
        "To demonstrate the core funtionality and syntax of nnsight, we'll define and use a tiny two layer neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgydw7i3HmIH"
      },
      "source": [
        "Our little model here is composed of four sub-modules, two linear layers ('layer1', 'layer2'). We spcecify the sizes of each of these modules, and create some complementary example input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2pX2Wg8Ceo6N"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "input_size = 5\n",
        "hidden_dims = 10\n",
        "output_size = 2\n",
        "\n",
        "net = torch.nn.Sequential(OrderedDict([\n",
        "    ('layer1', torch.nn.Linear(input_size, hidden_dims)),\n",
        "    ('layer2', torch.nn.Linear(hidden_dims, output_size)),\n",
        "])).requires_grad_(False)\n",
        "\n",
        "input = torch.rand((1, input_size))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIPa2h2pJIwl"
      },
      "source": [
        "The core object of the nnsight package is `NNsight`. This wraps around a given pytorch model to enable the capabilites nnsight provides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8H9R_ynTJI5y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/interp/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from nnsight import NNsight\n",
        "\n",
        "model = NNsight(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NfISlQ_Ilvp"
      },
      "source": [
        "Pytorch models when printed, show a named hierarchy of modules which is very useful when accessing sub-components directly. NNsight models work just the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYtnbJHvlGZV",
        "outputId": "5eb7c572-451c-43c6-956f-af224ae1867c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (layer2): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djC5kyJWLuUH"
      },
      "source": [
        "Before we actually get to using the model we just created, let's talk about what a `context` is in Python.\n",
        "\n",
        "Often enough when coding, you want to create some object, or initiate some logic, that you later want to destroy or conclude.\n",
        "\n",
        "The most common application is opening files like the following example:\n",
        "\n",
        "```python\n",
        "with open('myfile.txt', 'r') as file:\n",
        "  text = file.read()\n",
        "```\n",
        "\n",
        "Python uses the `with` keyword to enter a context-like object. This object defines logic to be ran at the start of the with block, and logic to be ran when exiting. In this case, entering the context opens a file and exiting the context closes it. Being within the context means we can read from file. Simple enough! Now we can discuss how `nnsight` uses contexts to enable powerful and intuitive access into the internals of model computation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNvuCOeyojcA"
      },
      "source": [
        "Introducing the tracing context. Just like before, something happens upon entering the tracing context, something happens when exiting, and being inside enables some functionality.\n",
        "\n",
        "We enter the tracing context by calling `.trace(<input>)` on the `NNsight` model we created before. Entering it denotes we want to run the model given our input... but not yet! The model is only ran upon exiting the tracing context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "qEXQ4auPSL-m"
      },
      "outputs": [],
      "source": [
        "with model.trace(input) as tracer:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZQsHinjqicJ"
      },
      "source": [
        "But where's the output? To get that, we'll have to learn how to request it from within the tracing conext."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMfBpYzDPMoB"
      },
      "source": [
        "## Getting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5_aFwFRv0ax"
      },
      "source": [
        "Earlier, when we wrapped our little net with the `NNsight` class, this added a couple properties on to each module in the model (including the root model itself). The ones we care about are `.input` and `.output`.\n",
        "\n",
        "```python\n",
        "model.input\n",
        "model.output\n",
        "```\n",
        "\n",
        "The names are pretty self explainatory. They correspond to the inputs and outputs of their respective modules during some forward pass of an input through the model. These are what we're going to interact with in the tracing context.\n",
        "\n",
        "However, remember how the model isnt executed until the end of the tracing context? So how can we access their inputs and outputs during computation from within the context? Well, we can't.\n",
        "\n",
        "`.input` and `.output` are Proxies for the eventual inputs and outputs of a module. In other words, when you access `model.output` what you're communicating to `nnsight` is \"When you compute the output of `model`, please grab it for me and put the value into it's corresponding Proxy object's `.value` attribute.\" Let's try just that:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "cYe8-r9ptGaG",
        "outputId": "adf25efe-d2cf-4f65-9eaf-15b155762f4b"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Accessing Proxy value before it's been set.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtrace(\u001b[38;5;28minput\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m tracer:\n\u001b[1;32m      3\u001b[0m   output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39moutput\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m)\n",
            "File \u001b[0;32m~/Programming/nnsight/src/nnsight/tracing/Proxy.py:49\u001b[0m, in \u001b[0;36mProxy.value\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Property to return the value of this proxy's node.\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m    Any: The stored value of the proxy, populated during execution of the model.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing Proxy value before it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms been set.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mvalue\n",
            "\u001b[0;31mValueError\u001b[0m: Accessing Proxy value before it's been set."
          ]
        }
      ],
      "source": [
        "with model.trace(input) as tracer:\n",
        "\n",
        "  output = model.output\n",
        "\n",
        "print(output.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBz5qfwuR-6F"
      },
      "source": [
        "Oh no an error! \"Accessing Proxy value before it's been set.\"\n",
        "\n",
        "If `.value` isn't filled in after leaving the tracing context, accessing the value will give you this error.  In reality however, the value was filled in, it was just immediately removed. Why?\n",
        "\n",
        "Proxy objects track their listeners (as in other Proxy object that rely on it), and when their listeners are all complete, it deletes the `.value` associated with the Proxy in order to save memory. To prevent this, we call `.save()` on the Proxy objects we want to access outisde of the tracing context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_bXRd5dvsBu",
        "outputId": "a8e8985c-2661-46a8-975b-d43f67cfc162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.1988, -0.0418]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input) as tracer:\n",
        "\n",
        "  output = model.output.save()\n",
        "\n",
        "print(output.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5C0UZCwvrYn"
      },
      "source": [
        "Success! We now have the model output meaning you just completed your first intervention request using Proxies.\n",
        "\n",
        "These requests are handled at the soonest possible moment they can be completed. In this case, right after the model's output was computed. Collectively these requests form the `intervention graph` and we call the process of executing it alongside the model's normal computation graph, `interleaving`.\n",
        "\n",
        "What else can we request? There's nothing special about the model itself vs it's submodules. Just like we saved the output of the model as a whole, we can save the output of any of it's submodules. To get to them we use normal Python attribute syntax, and we know where the modules are becuase we printed out the model earlier:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WcVUSP-0CJi",
        "outputId": "7fd9d6ac-152d-4431-9a4d-0b60321638eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (layer1): Linear(in_features=5, out_features=10, bias=True)\n",
            "  (layer2): Linear(in_features=10, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akWr-cNqy-9O",
        "outputId": "68b9e071-c609-4b70-906d-2f2cd93b4495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.4802, -0.0706, -0.4357,  0.0113,  0.0720, -0.1803,  0.4475, -0.1772,\n",
            "         -0.1301,  0.3921]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input) as tracer:\n",
        "\n",
        "  l1_output = model.layer1.output.save()\n",
        "\n",
        "print(l1_output.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85A-aP_03ht6"
      },
      "source": [
        "Let's do the same for the input of layer2. While we're at it, let's also drop the `as tracer`, as we won't be needing the tracer object itself for a few sections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3EHEN38N3nXR",
        "outputId": "5afd17a7-0bcc-4d1a-c131-7038bde9455d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "((tensor([[ 0.4802, -0.0706, -0.4357,  0.0113,  0.0720, -0.1803,  0.4475, -0.1772,\n",
            "         -0.1301,  0.3921]]),), {})\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  l2_input = model.layer2.input.save()\n",
        "\n",
        "print(l2_input.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk-U8zi33Gi-"
      },
      "source": [
        "<details>\n",
        "  <summary>On module inputs</summary>\n",
        "\n",
        "  ---\n",
        "\n",
        "  Notice how the value for l2_input, was not just a single tensor.\n",
        "  The type/shape of values from .input is in the form of:\n",
        "\n",
        "      tuple(tuple(args), dictionary(kwargs))\n",
        "\n",
        "  Where the first index of the tuple is itself a tuple of all positional arguments, and the second index is a dictionary of the keyword arguments.\n",
        "\n",
        "  ---\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "Now that we can access activations, we also want to do some post-processing on it. Let's find out which dimension of layer1's output has the highest value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7Xo_PHyPr4p"
      },
      "source": [
        "## Functions, Methods, and Operations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehSofWbx5DSx"
      },
      "source": [
        "We could do this by calling `torch.argmax(...)` after the tracing context... or we can just leverage the fact that `nnsight` handles functions and methods within the tracing context, by creating a Proxy request for it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5XCiSZn2p3k",
        "outputId": "97d34dee-7bb5-4735-8c53-6aac6377af9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0)\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  # Note we don't need to call .save() on the output,\n",
        "  # as we're only using it's value within the tracing context.\n",
        "  l1_output = model.layer1.output\n",
        "\n",
        "  l1_amax = torch.argmax(l1_output, dim=1).save()\n",
        "\n",
        "print(l1_amax[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGPUJvWq_bOp"
      },
      "source": [
        "Nice! That worked seamlessly, but hold on, how come we didn't need to call `.value[0]` on the result? In previous sections, we were just being explicit to get an understaing of Proxies and their value. In practice however, `nnsight` knows that when outside of the tracing context we only care about the actual value, and so printing, indexing, and applying functions all immediately return and reflect the data in `.value`. So for the rest of the tutorial we won't use it.\n",
        "\n",
        "The same principles work for methods and operations as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIcOYeEjFuln",
        "outputId": "d836abd3-b713-44c5-aedc-95bf85abf11a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.1685)\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  value = (model.layer1.output.sum() + model.layer2.output.sum()).save()\n",
        "\n",
        "print(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0vNOJtJ2oFR"
      },
      "source": [
        "By default torch functions and methods, as well as all operators work with `nnsight`. We also enable the use of the `einops` library.\n",
        "\n",
        "So to recap, this cell is saying to `nnsight`, \"Run the model with the given `input`. When the output of layer1 is computed, take it's sum. Then do the same for layer2. Now that both of those are computed, add them and make sure not to delete this value as I wish to use it outisde of the tracing context.\"\n",
        "\n",
        "Getting and analyzing the activations from various points in a model can be really insightful, and a number of ML techniques do exactly that. However, often times we not only want to view the computation of a model, but influence it as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_5qH5gHPOT_"
      },
      "source": [
        "## Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgju-b_IOLlq"
      },
      "source": [
        "To demonstrate editing the flow of information through the model, let's set the first dimension of the first layer's output to 0. `NNsight` makes this really easy using the familiar assignment '=' operator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6y2wzJqOz3a",
        "outputId": "6274e5c5-c5fe-4e07-87c4-a119cfd09742"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: tensor([[ 0.4802, -0.0706, -0.4357,  0.0113,  0.0720, -0.1803,  0.4475, -0.1772,\n",
            "         -0.1301,  0.3921]])\n",
            "After: tensor([[ 0.0000, -0.0706, -0.4357,  0.0113,  0.0720, -0.1803,  0.4475, -0.1772,\n",
            "         -0.1301,  0.3921]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  # Save the output before the edit to compare.\n",
        "  # Notice we apply .clone() before saving as the setting operation is in-place.\n",
        "  l1_output_before = model.layer1.output.clone().save()\n",
        "\n",
        "  # Access the 0th index of the hidden state dimension and set it to 0.\n",
        "  model.layer1.output[:, 0] = 0\n",
        "\n",
        "  # Save the output after to see our edit.\n",
        "  l1_output_after = model.layer1.output.save()\n",
        "\n",
        "print(\"Before:\", l1_output_before)\n",
        "print(\"After:\", l1_output_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZz9SMs3Y_iS"
      },
      "source": [
        "Seems our change was reflected. Now the same for the last dimension:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "qwlqvHFcxld2",
        "outputId": "28ae7175-5af0-4bd0-f9f0-21680a4fd748"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index 10 is out of bounds for dimension 1 with size 10",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m   model\u001b[38;5;241m.\u001b[39mlayer1\u001b[38;5;241m.\u001b[39moutput[:, hidden_dims] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;66;03m# Save the output after to see our edit.\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m   l1_output_after \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayer1\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBefore:\u001b[39m\u001b[38;5;124m\"\u001b[39m, l1_output_before)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter:\u001b[39m\u001b[38;5;124m\"\u001b[39m, l1_output_after)\n",
            "File \u001b[0;32m~/Programming/nnsight/src/nnsight/contexts/Runner.py:40\u001b[0m, in \u001b[0;36mRunner.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"On exit, run and generate using the model whether locally or on the server.\"\"\"\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc_val, \u001b[38;5;167;01mBaseException\u001b[39;00m):\n\u001b[0;32m---> 40\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc_val\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39mtracing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mremote:\n",
            "Cell \u001b[0;32mIn[13], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m l1_output_before \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayer1\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39msave()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Access the last index of the hidden state dimension and set it to 0.\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dims\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Save the output after to see our edit.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m l1_output_after \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mlayer1\u001b[38;5;241m.\u001b[39moutput\u001b[38;5;241m.\u001b[39msave()\n",
            "File \u001b[0;32m~/Programming/nnsight/src/nnsight/tracing/Proxy.py:92\u001b[0m, in \u001b[0;36mProxy.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__setitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: Union[Proxy, Any], value: Union[Self, Any]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetitem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Programming/nnsight/src/nnsight/tracing/Graph.py:146\u001b[0m, in \u001b[0;36mGraph.add\u001b[0;34m(self, target, value, args, kwargs, name)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m FakeTensorMode(\n\u001b[1;32m    140\u001b[0m     allow_non_fake_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    141\u001b[0m     shape_env\u001b[38;5;241m=\u001b[39mShapeEnv(assume_static_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    142\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m fake_mode:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mtarget\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mNode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_proxy_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_args\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mNode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_proxy_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 10 is out of bounds for dimension 1 with size 10"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  # Save the output before the edit to compare.\n",
        "  # Notice we apply .clone() before saving as the setting operation is in-place.\n",
        "  l1_output_before = model.layer1.output.clone().save()\n",
        "\n",
        "  # Access the last index of the hidden state dimension and set it to 0.\n",
        "  model.layer1.output[:, hidden_dims] = 0\n",
        "\n",
        "  # Save the output after to see our edit.\n",
        "  l1_output_after = model.layer1.output.save()\n",
        "\n",
        "print(\"Before:\", l1_output_before)\n",
        "print(\"After:\", l1_output_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCNR_Jkpxr8l"
      },
      "source": [
        "Ah of course, we needed to index at `hidden_dims - 1` not `hidden_dims`. How did `nnsight` know there was this indexing error before leaving the tracing context?\n",
        "\n",
        "Earlier when discussing contexts in Python, we learned some logic happens upon entering, and some logic happens upon exiting. We know the model is actually ran on exit, but what happens on enter? Our input IS actually ran though the model, however under its own \"fake\" context. This means the input makes its way through all of the models operations, allowing `nnsight` to record the shapes and data types of module inputs and outputs! The operations are never executed using tensors with real values so it dosen't incur any memory costs. Then, when creating proxy requests like the setting one above, `nnsight` also attempts to execute the request on the \"fake\" values we recorded, letting us know if our request is feasible before even running the model.\n",
        "\n",
        "<details>\n",
        "<summary>On scanning</summary>\n",
        "\n",
        "---\n",
        "\n",
        "\"Scanning\" is what we call running \"fake\" inputs throught the model to collect information like shapes and types. \"validating\" is what we call trying to execute your intervention proxies with \"fake\" inputs to see if they work. If youre doing anything in a loop where efficiency is import, you should turn off scanning an validating. You can turn off validating in `.trace(...)` like `.trace(..., validate=False)`. You can turn off scanning in `Tracer.invoke(...)` ([see the Batching section](#batching-id)) like `Tracer.invoke(..., scan=False)`\n",
        "\n",
        "---\n",
        "\n",
        "</details>\n",
        "\n",
        "Let's try again with the correct indexing, and view the shape of the output before leaving the tracing context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wf8ugJKB2mru",
        "outputId": "4eed6c19-cb8c-4deb-e0e1-25aa80959c64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "layer1 output shape: torch.Size([1, 10])\n",
            "Before: tensor([[ 0.4802, -0.0706, -0.4357,  0.0113,  0.0720, -0.1803,  0.4475, -0.1772,\n",
            "         -0.1301,  0.3921]])\n",
            "After: tensor([[ 0.4802, -0.0706, -0.4357,  0.0113,  0.0720, -0.1803,  0.4475, -0.1772,\n",
            "         -0.1301,  0.0000]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  # Save the output before the edit to compare.\n",
        "  # Notice we apply .clone() before saving as the setting operation is in-place.\n",
        "  l1_output_before = model.layer1.output.clone().save()\n",
        "\n",
        "  print(f\"layer1 output shape: {model.layer1.output.shape}\")\n",
        "\n",
        "  # Access the last index of the hidden state dimension and set it to 0.\n",
        "  model.layer1.output[:, hidden_dims - 1] = 0\n",
        "\n",
        "  # Save the output after to see our edit.\n",
        "  l1_output_after = model.layer1.output.save()\n",
        "\n",
        "print(\"Before:\", l1_output_before)\n",
        "print(\"After:\", l1_output_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DH7PeL13-WU"
      },
      "source": [
        "We can also just replace proxy inputs and outputs with tensors of the same shape and type. Let's use the shape information we have at our disposal to add noise to the output, and replace it with this new noised tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AP5wRmz_4YE7",
        "outputId": "4c30dc1d-232d-4900-bfdd-0c93a9b23ad7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before: tensor([[ 0.4802, -0.0706, -0.4357,  0.0113,  0.0720, -0.1803,  0.4475, -0.1772,\n",
            "         -0.1301,  0.3921]])\n",
            "After: tensor([[ 0.5208, -0.0984, -0.4089,  0.0626,  0.0895, -0.2086,  0.4192, -0.1822,\n",
            "         -0.1020,  0.4061]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  # Save the output before the edit to compare.\n",
        "  # Notice we apply .clone() before saving as the setting operation is in-place.\n",
        "  l1_output_before = model.layer1.output.clone().save()\n",
        "\n",
        "  # Create random noise with variance of .001\n",
        "  noise = (0.001**0.5)*torch.randn(l1_output_before.shape)\n",
        "\n",
        "  # Add to original value and replace.\n",
        "  model.layer1.output = l1_output_before + noise\n",
        "\n",
        "  # Save the output after to see our edit.\n",
        "  l1_output_after = model.layer1.output.save()\n",
        "\n",
        "print(\"Before:\", l1_output_before)\n",
        "print(\"After:\", l1_output_after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s016CelFP8sx"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "`NNsight` can also let you apply backprop and access gradients with respect to a loss. Like `.input` and `.output` on modules, `nnsight` also exposes `.grad` on Proxies themselves (assuming they are proxies of tensors):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-cYUNNyRDn1",
        "outputId": "d03dafa1-ef1e-40f1-de28-4dc2f77fe4db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1 output gradient: tensor([[-0.1807, -0.0932, -0.0272, -0.1585,  0.0117, -0.1133, -0.1660, -0.3290,\n",
            "          0.3369, -0.0236]])\n",
            "Layer 2 output gradient: tensor([[1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  # We need to explicitly have the tensor require grad\n",
        "  # as the model we definded earlier turned off requiring grad.\n",
        "  model.layer1.output.requires_grad = True\n",
        "\n",
        "  # We call .grad on a tensor Proxy to communicate we want to store its gradient.\n",
        "  # We need to call .save() of course as .grad is it's own Proxy.\n",
        "  layer1_output_grad = model.layer1.output.grad.save()\n",
        "  layer2_output_grad = model.layer2.output.grad.save()\n",
        "\n",
        "  # Need a loss to propagate through the later modules in order to have a grad.\n",
        "  loss = model.output.sum()\n",
        "  loss.backward()\n",
        "\n",
        "print(\"Layer 1 output gradient:\", layer1_output_grad)\n",
        "print(\"Layer 2 output gradient:\", layer2_output_grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o0JaaYvWlHG"
      },
      "source": [
        "All of the features we learned previously apply to `.grad`, meaning we can apply operations and edit the gradients. Let's zero the grad of layer1 and double the grad of layer 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bFoaJpOWlRb",
        "outputId": "1c026534-9ffe-40e7-d016-e4565ddde8ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 1 output gradient: tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
            "Layer 2 output gradient: tensor([[2., 2.]])\n"
          ]
        }
      ],
      "source": [
        "with model.trace(input):\n",
        "\n",
        "  # We need to explicitly have the tensor require grad\n",
        "  # as the model we definded earlier turned off requiring grad.\n",
        "  model.layer1.output.requires_grad = True\n",
        "\n",
        "  model.layer1.output.grad[:] = 0\n",
        "  model.layer2.output.grad = model.layer2.output.grad.clone() * 2\n",
        "\n",
        "  layer1_output_grad = model.layer1.output.grad.save()\n",
        "  layer2_output_grad = model.layer2.output.grad.save()\n",
        "\n",
        "  # Need a loss to propagate through the later modules in order to have a grad.\n",
        "  loss = model.output.sum()\n",
        "  loss.backward()\n",
        "\n",
        "print(\"Layer 1 output gradient:\", layer1_output_grad)\n",
        "print(\"Layer 2 output gradient:\", layer2_output_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvQ1nZgYQDG3"
      },
      "source": [
        "# Bigger"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miQgUY4NAmDQ"
      },
      "source": [
        "Now that we have the basics of `nnsight` under our belt, we can scale our model up and combine the techniques we've learned into more interesting experiments.\n",
        "\n",
        "The `NNsight` class is very bare bones. It wraps a pre-defined model and does no pre-processing on the inputs we enter. It's designed to be extended with more complex and powerful types of models and we're excited to see what can be done to leverage the base line features of `nnsight`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TJDblHiQpp1"
      },
      "source": [
        "## LanguageModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9l9mZOY5HFH2"
      },
      "source": [
        "`LanguageModel` is a subclass of `NNsight`.  While we could define and create a model to pass in directly, `LanguageModel` includes special support for Huggingface language models, including automatically loading models from a Huggingface ID, and loading the model together with the appropriate tokenizer.\n",
        "\n",
        "Here is how you can use `LanguageModel` to load `GPT-2`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769,
          "referenced_widgets": [
            "0a676b8f2dc34fed9731f19a03e4bfa1",
            "74358c341daa44faa41a92b0eb073615",
            "c40e17a994ed44d79ef54421f4772858",
            "431ffcb0dbb1419dbbb237a29842b905",
            "c28c185a508e459b95edb29afedd9c79",
            "7a0f3d2deeb14bbba7f0df208133a7e2",
            "94cdcf1fb0914dfe99c3c615e68b7ec8",
            "f3e24aa6619e4185a3e2f53f2e473e45",
            "b7be20bd738940658f710309729d8ccd",
            "562a287082eb4882af864062ae13c91b",
            "11e99924bdf84739b3cfc7d676b797e5",
            "60fe026e908741209637604edfeb0563",
            "e7a73cfc3fa04425afc08368532ef0be",
            "d839da1c31a34659988e3420ba520725",
            "96a3b8c765374a5e98eaf713dd5e85d8",
            "b49ce0d00e5b4df097b3a9fc4c0bd8e7",
            "b2748df9aff540df954f002310606a56",
            "f4caf2bc58704464b6ac7bf6da31b71d",
            "3731a5747724403e86ef1abaf4fb389b",
            "688bf676958a4050b73cc9597f5110de",
            "8776d04bfd3748b69e10708466e9c132",
            "56490c20aef04fe4a63ba7c990e71e77",
            "b8ad433cc3dd4a8689cb06655158d545",
            "e51620404d244a4e806eeaf3b96a474f",
            "e4568f72971749ac92cc5577cb9c4f9c",
            "1a1ada8b122d4502949c4c75861a82bf",
            "18d4c2302f424bc18f9215f2d5b83393",
            "f28f557210b7402b8b2d0fd83f038dc1",
            "bc8190c700e949ebb6f874c8f8055161",
            "ceebda1d07b545f5b605551c08e73be8",
            "6aa96b80e3b746b48f5c13bcafe1134e",
            "8de63414803f41c29e75242756cfd67f",
            "f693dfed91df492c986afbd4ff925f89",
            "cef2516aabc24f6d852079984afa8090",
            "95d303d83d3348a0adfdf6b5cb94c41c",
            "9373fcd054014760b23e3acc928da0a5",
            "7e6b41ec535648b190bfc655d9e430dd",
            "d0198fa98e5f412380f2ba3e9448aef3",
            "eb0c489b913744a28304e5334bb66532",
            "c80603555c634e338f16c0021f752420",
            "0fddebfb42e847e08890d09f30161984",
            "d07a0a49a2924092affc0e07bf0b5ef0",
            "beb4231b49024d8a9bb7ead19647d99d",
            "26d09aa072d64af0bf4df9a89309093a"
          ]
        },
        "id": "1OD2z7d3HQJU",
        "outputId": "7a53864c-6965-4c25-9668-4b0d56db9f6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            "  (generator): WrapperModule()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from nnsight import LanguageModel\n",
        "\n",
        "model = LanguageModel('openai-community/gpt2', device_map=\"auto\")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hw2VaAr_ezkj"
      },
      "source": [
        "<details>\n",
        "<summary>On Model Initialization</summary>\n",
        "\n",
        "---\n",
        "\n",
        "A few important things to note:\n",
        "\n",
        "  Keyword arguments passed to the initialization of `LanguageModel` make its way to the huffingface specific loading logic. In this case, `device_map` specifies which devices to use and it's value `auto` indicates to evenly distribute it to all available GPUs (and cpu if no GPUs available). Other arguments can be found here: https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM\n",
        "\n",
        "\n",
        "  When we initialize `LanguageModel`, we aren't yet loading the parameters of the model into memory. We are actually loading a 'meta' version of the model which dosen't take up any memory, but still allows us to view and trace actions on it. Only when the first tracing context exits is the real model with full parameters (using the keyword arguments we defined on init) loaded into memory. To load into memory on initialization, you can pass `dispatch=True` into `LanguageModel` like `LanguageModel('openai-community/gpt2', device_map=\"auto\", dispatch=True)`.\n",
        "\n",
        "---\n",
        "\n",
        "</details>\n",
        "\n",
        "\n",
        "Let's put together some of the features we applied to the small model, but now on `GPT-2`. Unlike `NNsight`, `LanguageModel` does define logic to pre-process inputs upon entering the tracing context which makes interacting with the model simpler without having to directly access the tokenizer.\n",
        "\n",
        "In this example, we ablate the value coming from the last layer's MLP module and decode the logits to see what token the model predicts without infucence from the module:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLSapaMCgLNU",
        "outputId": "192b8998-ecca-4f25-ecf2-5196c0d2d672"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token IDs: tensor([[ 262,   12,  417, 8765,   11,  257,  262, 3504,  338, 3576]],\n",
            "       device='mps:0')\n",
            "Prediction:  London\n"
          ]
        }
      ],
      "source": [
        "with model.trace('The Eiffel Tower is in the city of'):\n",
        "\n",
        "  # Access the last layer using h[-1] as it's a ModuleList\n",
        "  # Access the first index of .output as that's where the hidden states are.\n",
        "  model.transformer.h[-1].mlp.output[0][:] = 0\n",
        "\n",
        "  # Logits come out of model.lm_head and we apply argmax to get the predicited token ids.\n",
        "  token_ids = model.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "print(\"Token IDs:\", token_ids)\n",
        "\n",
        "# Apply the tokenizer to decode the ids into words after the tracing context.\n",
        "print(\"Prediction:\", model.tokenizer.decode(token_ids[0][-1]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X2-HiCQhkv-B"
      },
      "source": [
        "You just ran a little intervention on a much more complex model with orders of magnitude more parameters! An import piece of information we're missing though is what the prediction would look like without our ablation.\n",
        "\n",
        "Of course we could just run two tracing contexts and compare the outputs, however this would require two forward passes through the model. `NNsight` can do better than that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLjHnRyRPXjp"
      },
      "source": [
        "<a name=\"batching-id\"></a>\n",
        "## Batching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gc-Zr6NmEOn"
      },
      "source": [
        "It's time to bring back the `Tracer` object we dropped before. See, when you call `.trace(...)` with some input, it's actually creating two different contexts behind the scenes. The second one is the invoker context. Being within this context just means that `.input` and `.output` should refer only to the input you've given invoke, and calling `.trace(...)` with some input just means there's only one input and therefore only one invoker context.\n",
        "\n",
        "We can call `.trace()` without input and call `Tracer.invoke(...)` to manually create the invoker context with our input. Now every subsequent time we call `.invoke(...)`, new interventions will only refer to the input in that invoke. Then when exiting the tracing context, the inputs from all of the invokers will be batched together, and it along with your Proxies will be executed in one forward pass! So let's do the ablation experiment, and compute a 'control' output to compare to:\n",
        "\n",
        "<details>\n",
        "<summary>On the invoker context</summary>\n",
        "\n",
        "---\n",
        "\n",
        "Note that injecting  data to only the relevant invoker interventions, `nnsight` tries, but can't guarantee it can narrow the data into the right batch idxs (in the case of an object as input or output). So there are cases where all invokes will get all of the data.\n",
        "\n",
        "Just like `.trace(...)` created a `Tracer` object, `.invoke(...)` creates an `Invoker` object. One thing the `Invoker` object has is the post-processed inputs at `invoker.inputs` which can be useful for seeing information about your input. If youre using `.trace(...)` with inputs, you can still access the invoker object at `tracer._invoker`.\n",
        "\n",
        "Keyword arguments given to `.invoke(..)` make its way to the input pre-processing. For example in `LanguageModel`, the keyword arguments are used to tokenize like max_length and truncation. If you need to pass in keyword arguments directly to a one input `.trace(...)`, you can pass an invoker_args keyword argument that should be a dictionary of keyword arguments for the invoker. `.trace(..., invoker_args={...})`\n",
        "\n",
        "---\n",
        "\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kdcq4oCNmEua",
        "outputId": "e6d33c07-d22a-41af-b84a-91c9681c7524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original token IDs: tensor([[ 198,   12,  417, 8765,  318,  257,  262, 3504, 7372, 6342]],\n",
            "       device='mps:0')\n",
            "Intervention token IDs: tensor([[ 262,   12,  417, 8765,   11,  257,  262, 3504,  338, 3576]],\n",
            "       device='mps:0')\n",
            "Original prediction:  Paris\n",
            "Intervention prediction:  London\n"
          ]
        }
      ],
      "source": [
        "with model.trace() as tracer:\n",
        "\n",
        "  with tracer.invoke('The Eiffel Tower is in the city of'):\n",
        "\n",
        "    # Ablate the last MLP for only this batch.\n",
        "    model.transformer.h[-1].mlp.output[0][:] = 0\n",
        "\n",
        "    # Get the output for only the intervened on batch.\n",
        "    token_ids_intervention = model.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "  with tracer.invoke('The Eiffel Tower is in the city of'):\n",
        "\n",
        "    # Get the output for only the original batch.\n",
        "    token_ids_original = model.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "print(\"Original token IDs:\", token_ids_original)\n",
        "print(\"Intervention token IDs:\", token_ids_intervention)\n",
        "\n",
        "print(\"Original prediction:\", model.tokenizer.decode(token_ids_original[0][-1]))\n",
        "print(\"Intervention prediction:\", model.tokenizer.decode(token_ids_intervention[0][-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BsybgsK2Bbr"
      },
      "source": [
        "So it did actually end up effecting what the model predicted. That's pretty neat.\n",
        "\n",
        "Another cool thing with multiple invokes is that the Proxies can interact between them! Here we transfer the word token embeddings from a real prompt into one of all blanks. Therefore the blank prompt produces the output of the real prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syKTI_KhpvCY",
        "outputId": "b2238c5c-d2d8-47bf-b73a-730f1e03cf99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original prediction:  _\n",
            "Intervention prediction:  Paris\n"
          ]
        }
      ],
      "source": [
        "with model.trace() as tracer:\n",
        "\n",
        "  with tracer.invoke(\"The Eiffel Tower is in the city of\"):\n",
        "\n",
        "    embeddings = model.transformer.wte.output\n",
        "\n",
        "  with tracer.invoke(\"_ _ _ _ _ _ _ _ _ _\"):\n",
        "\n",
        "    model.transformer.wte.output = embeddings\n",
        "\n",
        "    token_ids_intervention = model.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "  with tracer.invoke(\"_ _ _ _ _ _ _ _ _ _\"):\n",
        "\n",
        "    token_ids_original = model.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "print(\"Original prediction:\", model.tokenizer.decode(token_ids_original[0][-1]))\n",
        "print(\"Intervention prediction:\", model.tokenizer.decode(token_ids_intervention[0][-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvWA-CWqQtah"
      },
      "source": [
        "## .next()\n",
        "\n",
        "Some Huggingface models define methods to generate multiple outputs at a time. `LanguageModel` wraps that functionality to provide the same tracing features just by using `.generate(...)` instead of `.trace(...)`. This calls the underlying model's `.generate` method and passes the output through a `model.generator` module that we've added onto the model, allowing you to get the generate output at `model.generator.output`.\n",
        "\n",
        "In a case like this, the underlying model is called more than once. Meaning the modules of said model produce more than one output. So which iteration should a given `module.output` refer to? That's where `Module.next()` comes in.\n",
        "\n",
        "Each module has a call idx associated with it and `.next()` simply increments that attribute. Come execution time, data is injected into the intervention graph only at the call idx defined."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy5Z9NE1GkaN",
        "outputId": "0b4c1bd3-a6f6-4ba1-9cb8-6019c222311f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction 1:   Paris\n",
            "Prediction 2:  ,\n",
            "Prediction 3:   and\n",
            "All token ids:  tensor([[ 464,  412,  733,  417, 8765,  318,  287,  262, 1748,  286, 6342,   11,\n",
            "          290]], device='mps:0')\n",
            "All prediction:  ['The Eiffel Tower is in the city of Paris, and']\n"
          ]
        }
      ],
      "source": [
        "with model.generate(\"The Eiffel Tower is in the city of\", max_new_tokens=3):\n",
        "\n",
        "  token_ids_1 = model.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "  token_ids_2 = model.lm_head.next().output.argmax(dim=-1).save()\n",
        "\n",
        "  token_ids_3 = model.lm_head.next().output.argmax(dim=-1).save()\n",
        "\n",
        "  output = model.generator.output.save()\n",
        "\n",
        "print(\"Prediction 1: \", model.tokenizer.decode(token_ids_1[0][-1]))\n",
        "print(\"Prediction 2: \", model.tokenizer.decode(token_ids_2[0][-1]))\n",
        "print(\"Prediction 3: \", model.tokenizer.decode(token_ids_3[0][-1]))\n",
        "\n",
        "print(\"All token ids: \", output)\n",
        "\n",
        "print(\"All prediction: \", model.tokenizer.batch_decode(output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpkX-LwBQZHo"
      },
      "source": [
        "# I thought you said huge models?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCZR65VmILEb"
      },
      "source": [
        "`NNsight` is only one half our project to democratize access to AI internals. The other half being `NDIF` (National Deep Inference Facility).\n",
        "\n",
        "The interaction between the two is fairly straightforward. The `intervention graph` we create via the tracing context can be encoded into a custom json format and sent via an http request to the `NDIF` servers. `NDIF` then decodes the `intervention graph` and `interleaves` it alongside the specified model. That's it!\n",
        "\n",
        "To see which models are currently being hosted, checkout out this status page: https://nnsight.net/status/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65Ks1LUvQaER"
      },
      "source": [
        "## Remote execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aa-ZuVOFKS5k"
      },
      "source": [
        "In it's current state, `NDIF` requires you to recieve an API key. Therefore to run the rest of this colab you need one of your own. To get one simply join the [NDIF discord](https://discord.gg/6uFJmCSwW7) and introduce yourself on the `#introductions` channel. Then DM either @JadenFK or @caden and we'll create one for you.\n",
        "\n",
        "Once you have one, to register your api key with `nnsight`, do the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ax1NWoS9MJeZ"
      },
      "outputs": [],
      "source": [
        "from nnsight import CONFIG\n",
        "\n",
        "CONFIG.set_default_api_key(\"<your api key here>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvXLOh65MXmF"
      },
      "source": [
        "This only needs to be ran once as it will save this api key as the default in a config file along with the `nnsight` installation.\n",
        "\n",
        "To amp things up a few levels, let's demonstrate using `nnsight`'s tracing context with one of the larger open source language models, Llama 2 70b!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "oqVzjNoyNGc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65cebeafa7c4e61fa5de838a - RECEIVED: Your job has been received and is waiting approval.\n",
            "65cebeafa7c4e61fa5de838a - APPROVED: Your job was approved and is waiting to be run.\n",
            "65cebeafa7c4e61fa5de838a - COMPLETED: Your job has been completed.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Downloading result: 100%|| 9.03M/9.03M [00:00<00:00, 10.6MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(tensor([[[ -0.1491,  -5.8323,   4.3558,  ...,   3.4249,  23.8446, -13.8308],\n",
            "         [  9.9500,  -1.2191,   1.3571,  ...,   0.2700,  31.3980,  -8.7688],\n",
            "         [ -1.6217,   9.1563,  -1.9623,  ...,   4.9550,  14.7141,  27.5813],\n",
            "         ...,\n",
            "         [ -2.7155,   5.6727,   0.7352,  ...,   5.7620,  26.6322,  -1.6128],\n",
            "         [  2.9816,   0.3416,  -0.7144,  ...,   2.7997,  32.5098,   4.9818],\n",
            "         [  0.2935,   1.6404,   6.5683,  ...,   0.1217,  53.6284,  -0.1982]]]), <transformers.cache_utils.DynamicCache object at 0x1680ca670>)\n",
            "tensor([[[ -9.5869,  -3.8338,   1.8843,  ...,  -4.9517,  -4.6490,  -4.3777],\n",
            "         [-10.0180, -10.1247,  -2.0404,  ...,  -6.5920,  -7.4882,  -5.7967],\n",
            "         [ -5.3571, -10.4557,   0.8910,  ...,  -3.3766,  -2.3204,  -1.7122],\n",
            "         ...,\n",
            "         [-11.2018, -11.9971,  -0.2768,  ...,  -7.3000,  -6.6654,  -5.2826],\n",
            "         [ -6.1399,  -4.2103,   7.6792,  ...,  -3.6305,  -3.2480,  -2.1176],\n",
            "         [-12.5268, -12.0461,   5.5539,  ...,  -5.2163,  -7.8048,  -5.4413]]],\n",
            "       requires_grad=True)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# We'll never actually load the parameters so no need to specify a device_map.\n",
        "model = LanguageModel(\"meta-llama/Llama-2-70b-hf\")\n",
        "\n",
        "# All we need to specify using NDIF vs executing locally is remote=True.\n",
        "with model.trace('The Eiffel Tower is in the city of', remote=True) as runner:\n",
        "\n",
        "    hidden_states = model.model.layers[-1].output.save()\n",
        "\n",
        "    output = model.output.save()\n",
        "\n",
        "print(hidden_states)\n",
        "\n",
        "print(output['logits'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IceVpnZcZ9F0"
      },
      "source": [
        "It really is as simple as `remote=True`. All of the techniques we went through in earlier sections work just the same when running locally and remotely.\n",
        "\n",
        "Note that both `nnsight`, but especially `NDIF` is in active development and therfore there may be caveats, changes, and errors to work through."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3zRm-7VRRov"
      },
      "source": [
        "# Getting Involved!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BnCc9xgjvxEP"
      },
      "source": [
        "If you're interested in following updates to `nnsight`, contributing, giving feedback, or finding collaborators, please join the [NDIF discord](https://discord.gg/6uFJmCSwW7)!\n",
        "\n",
        "The [Mech Interp discord](https://discord.gg/km2RQBzaUn) is also a fantastic place to discuss all things mech interp with a really cool community.\n",
        "\n",
        "Our website [nnsight.net](https://nnsight.net/), has a bunch more tutorials detailing more complex interpretablity techniques using `nnsight`. If you want to share any of the work you do using `nnsight`, let others know on either of the discords above and we might turn it into a tutorial on our website.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a676b8f2dc34fed9731f19a03e4bfa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74358c341daa44faa41a92b0eb073615",
              "IPY_MODEL_c40e17a994ed44d79ef54421f4772858",
              "IPY_MODEL_431ffcb0dbb1419dbbb237a29842b905"
            ],
            "layout": "IPY_MODEL_c28c185a508e459b95edb29afedd9c79"
          }
        },
        "0fddebfb42e847e08890d09f30161984": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11e99924bdf84739b3cfc7d676b797e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "18d4c2302f424bc18f9215f2d5b83393": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a1ada8b122d4502949c4c75861a82bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8de63414803f41c29e75242756cfd67f",
            "placeholder": "",
            "style": "IPY_MODEL_f693dfed91df492c986afbd4ff925f89",
            "value": " 456k/456k [00:00&lt;00:00, 1.87MB/s]"
          }
        },
        "26d09aa072d64af0bf4df9a89309093a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3731a5747724403e86ef1abaf4fb389b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "431ffcb0dbb1419dbbb237a29842b905": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_562a287082eb4882af864062ae13c91b",
            "placeholder": "",
            "style": "IPY_MODEL_11e99924bdf84739b3cfc7d676b797e5",
            "value": " 665/665 [00:00&lt;00:00, 12.8kB/s]"
          }
        },
        "562a287082eb4882af864062ae13c91b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56490c20aef04fe4a63ba7c990e71e77": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60fe026e908741209637604edfeb0563": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e7a73cfc3fa04425afc08368532ef0be",
              "IPY_MODEL_d839da1c31a34659988e3420ba520725",
              "IPY_MODEL_96a3b8c765374a5e98eaf713dd5e85d8"
            ],
            "layout": "IPY_MODEL_b49ce0d00e5b4df097b3a9fc4c0bd8e7"
          }
        },
        "688bf676958a4050b73cc9597f5110de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6aa96b80e3b746b48f5c13bcafe1134e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "74358c341daa44faa41a92b0eb073615": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a0f3d2deeb14bbba7f0df208133a7e2",
            "placeholder": "",
            "style": "IPY_MODEL_94cdcf1fb0914dfe99c3c615e68b7ec8",
            "value": "config.json: 100%"
          }
        },
        "7a0f3d2deeb14bbba7f0df208133a7e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e6b41ec535648b190bfc655d9e430dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_beb4231b49024d8a9bb7ead19647d99d",
            "placeholder": "",
            "style": "IPY_MODEL_26d09aa072d64af0bf4df9a89309093a",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 4.16MB/s]"
          }
        },
        "8776d04bfd3748b69e10708466e9c132": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8de63414803f41c29e75242756cfd67f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9373fcd054014760b23e3acc928da0a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fddebfb42e847e08890d09f30161984",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d07a0a49a2924092affc0e07bf0b5ef0",
            "value": 1355256
          }
        },
        "94cdcf1fb0914dfe99c3c615e68b7ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "95d303d83d3348a0adfdf6b5cb94c41c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb0c489b913744a28304e5334bb66532",
            "placeholder": "",
            "style": "IPY_MODEL_c80603555c634e338f16c0021f752420",
            "value": "tokenizer.json: 100%"
          }
        },
        "96a3b8c765374a5e98eaf713dd5e85d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8776d04bfd3748b69e10708466e9c132",
            "placeholder": "",
            "style": "IPY_MODEL_56490c20aef04fe4a63ba7c990e71e77",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 3.16MB/s]"
          }
        },
        "b2748df9aff540df954f002310606a56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b49ce0d00e5b4df097b3a9fc4c0bd8e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7be20bd738940658f710309729d8ccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b8ad433cc3dd4a8689cb06655158d545": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e51620404d244a4e806eeaf3b96a474f",
              "IPY_MODEL_e4568f72971749ac92cc5577cb9c4f9c",
              "IPY_MODEL_1a1ada8b122d4502949c4c75861a82bf"
            ],
            "layout": "IPY_MODEL_18d4c2302f424bc18f9215f2d5b83393"
          }
        },
        "bc8190c700e949ebb6f874c8f8055161": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "beb4231b49024d8a9bb7ead19647d99d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c28c185a508e459b95edb29afedd9c79": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c40e17a994ed44d79ef54421f4772858": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3e24aa6619e4185a3e2f53f2e473e45",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7be20bd738940658f710309729d8ccd",
            "value": 665
          }
        },
        "c80603555c634e338f16c0021f752420": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ceebda1d07b545f5b605551c08e73be8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cef2516aabc24f6d852079984afa8090": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95d303d83d3348a0adfdf6b5cb94c41c",
              "IPY_MODEL_9373fcd054014760b23e3acc928da0a5",
              "IPY_MODEL_7e6b41ec535648b190bfc655d9e430dd"
            ],
            "layout": "IPY_MODEL_d0198fa98e5f412380f2ba3e9448aef3"
          }
        },
        "d0198fa98e5f412380f2ba3e9448aef3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d07a0a49a2924092affc0e07bf0b5ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d839da1c31a34659988e3420ba520725": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3731a5747724403e86ef1abaf4fb389b",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_688bf676958a4050b73cc9597f5110de",
            "value": 1042301
          }
        },
        "e4568f72971749ac92cc5577cb9c4f9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ceebda1d07b545f5b605551c08e73be8",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6aa96b80e3b746b48f5c13bcafe1134e",
            "value": 456318
          }
        },
        "e51620404d244a4e806eeaf3b96a474f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f28f557210b7402b8b2d0fd83f038dc1",
            "placeholder": "",
            "style": "IPY_MODEL_bc8190c700e949ebb6f874c8f8055161",
            "value": "merges.txt: 100%"
          }
        },
        "e7a73cfc3fa04425afc08368532ef0be": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2748df9aff540df954f002310606a56",
            "placeholder": "",
            "style": "IPY_MODEL_f4caf2bc58704464b6ac7bf6da31b71d",
            "value": "vocab.json: 100%"
          }
        },
        "eb0c489b913744a28304e5334bb66532": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f28f557210b7402b8b2d0fd83f038dc1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3e24aa6619e4185a3e2f53f2e473e45": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4caf2bc58704464b6ac7bf6da31b71d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f693dfed91df492c986afbd4ff925f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
