{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ndif-team/nnsight/blob/main/NNsight_Walkthrough.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<p align=\"center\">\n",
        "  <img src=\"https://nnsight.net/_static/images/nnsight_logo.svg\" alt=\"nnsight\" width=\"300\"/>\n",
        "</p>\n",
        "\n",
        "# **NNsight Walkthrough**\n",
        "\n",
        "## Interpret and Manipulate the Internals of Deep Learning Models\n",
        "\n",
        "**nnsight** is a Python library that gives you full access to the internals of neural networks during inference. Whether you're running models locally or remotely via [NDIF](https://ndif.us/), nnsight lets you:\n",
        "\n",
        "- **Access activations** at any layer during forward passes\n",
        "- **Modify activations** to study causal effects  \n",
        "- **Compute gradients** with respect to intermediate values\n",
        "- **Batch interventions** across multiple inputs efficiently\n",
        "\n",
        "This walkthrough will teach you nnsight from the ground up, starting with the core mental model and building to advanced features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Table of Contents\n",
        "\n",
        "1. [Getting Started](#getting-started) - Setup and your first trace\n",
        "2. [Accessing Activations](#accessing-activations) - Reading module outputs\n",
        "3. [Modifying Activations](#modifying-activations) - Intervention basics\n",
        "4. [Invokers and Batching](#invokers-and-batching) - Multiple inputs, serial interventions\n",
        "5. [Multi-Token Generation](#multi-token-generation) - Iterating over generation steps\n",
        "6. [Gradients](#gradients) - Accessing and modifying gradients\n",
        "7. [Advanced Features](#advanced-features) - Source tracing, caching, barriers, and more\n",
        "8. [Model Editing](#model-editing) - Persistent modifications\n",
        "9. [Remote Execution](#remote-execution) - Running on NDIF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"getting-started\"></a>\n",
        "# 1. Getting Started\n",
        "\n",
        "Let's set up nnsight and run our first trace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install nnsight\n",
        "!pip install nnsight\n",
        "!pip install --upgrade transformers torch\n",
        "\n",
        "from IPython.display import clear_output\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A Tiny Model\n",
        "\n",
        "We'll start with a simple two-layer neural network to demonstrate the core concepts clearly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "import torch\n",
        "\n",
        "input_size = 5\n",
        "hidden_dims = 10\n",
        "output_size = 2\n",
        "\n",
        "net = torch.nn.Sequential(\n",
        "    OrderedDict([\n",
        "        (\"layer1\", torch.nn.Linear(input_size, hidden_dims)),\n",
        "        (\"layer2\", torch.nn.Linear(hidden_dims, output_size)),\n",
        "    ])\n",
        ").requires_grad_(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wrapping with NNsight\n",
        "\n",
        "The `NNsight` class wraps any PyTorch model to enable intervention on its internals. It reflects the same module hierarchy as the original model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import nnsight\n",
        "from nnsight import NNsight\n",
        "\n",
        "model = NNsight(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Printing a PyTorch model shows a named hierarchy of modules, which is very useful for knowing how to access sub-components directly. NNsight reflects the same hierarchy:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(model)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Python Contexts\n",
        "\n",
        "Before we actually get to using the model, let's talk about Python contexts.\n",
        "\n",
        "Python contexts define a scope using the `with` statement and are often used to create some object, or initiate some logic, that you later want to destroy or conclude.\n",
        "\n",
        "The most common application is opening files:\n",
        "\n",
        "```python\n",
        "with open('myfile.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "```\n",
        "\n",
        "Python uses the `with` keyword to enter a context-like object. This object defines logic to be run at the start of the `with` block, as well as logic to be run when exiting. When using `with` for a file, entering the context opens the file and exiting the context closes it. Being within the context means we can read from the file.\n",
        "\n",
        "Simple enough! Now we can discuss how nnsight uses contexts to enable intuitive access into the internals of a neural network.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"accessing-activations\"></a>\n",
        "# 2. Accessing Activations\n",
        "\n",
        "Now let's access the model's internals using the tracing context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Tracing Context\n",
        "\n",
        "The main tool in nnsight is a context for tracing. We enter the tracing context by calling `model.trace(<input>)` on an NNsight model, which defines how we want to run the model. Inside the context, we will be able to customize how the neural network runs. The model is actually run upon exiting the tracing context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "input = torch.rand((1, input_size))\n",
        "\n",
        "with model.trace(input):\n",
        "    # Your intervention code goes here\n",
        "    # The model runs when the context exits\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "But where's the output? To get that, we'll have to learn how to request it from within the tracing context.\n",
        "\n",
        "## The `.input` and `.output` Properties\n",
        "\n",
        "When we wrapped our neural network with the `NNsight` class, this added a couple of properties to each module in the model (including the root model itself). The two most important ones are `.input` and `.output`:\n",
        "\n",
        "```python\n",
        "model.input   # The input to the model\n",
        "model.output  # The output from the model\n",
        "```\n",
        "\n",
        "The names are self-explanatory. They correspond to the inputs and outputs of their respective modules during a forward pass. We can use these attributes inside the `with` block to access values at any point in the network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's try accessing the model's output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.trace(input):\n",
        "    output = model.output\n",
        "\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Oh no, an error! \"Accessing value before it's been set.\"\n",
        "\n",
        "Why doesn't our `output` have a value? Values accessed inside a trace only exist during the trace. They will only persist after the context if we call `.save()` on them. This helps reduce memory costs - we only keep what we explicitly ask for.\n",
        "\n",
        "## Saving Values with `.save()`\n",
        "\n",
        "Adding `.save()` fixes the error:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.trace(input):\n",
        "    output = model.output.save()\n",
        "\n",
        "print(output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Success! We now have the model output. We just completed our first intervention using nnsight.\n",
        "\n",
        "The `.save()` method tells nnsight \"I want to use this value after the trace ends.\"\n",
        "\n",
        "> **üí° Tip:** There's also `nnsight.save(value)` which is the preferred alternative. It works on any value and doesn't require the object to have a `.save()` method:\n",
        "> ```python\n",
        "> output = nnsight.save(model.output)\n",
        "> ```\n",
        "> Both approaches work, but `nnsight.save()` is more explicit and works in more cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accessing Submodule Outputs\n",
        "\n",
        "Just like we saved the model's output, we can access any submodule's output. Remember when we printed the model earlier? That showed us `layer1` and `layer2` - we can access those directly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.trace(input):\n",
        "    layer1_output = model.layer1.output.save()\n",
        "    layer2_output = model.layer2.output.save()\n",
        "\n",
        "print(\"Layer 1 output:\", layer1_output)\n",
        "print(\"Layer 2 output:\", layer2_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Accessing Module Inputs\n",
        "\n",
        "We can also access the inputs to any module using `.input`:\n",
        "\n",
        "| Property | Returns |\n",
        "|----------|---------|\n",
        "| `.output` | The module's return value |\n",
        "| `.input` | The first positional argument to the module |\n",
        "| `.inputs` | All inputs as `(args_tuple, kwargs_dict)` |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.trace(input):\n",
        "    layer2_input = model.layer2.input.save()\n",
        "\n",
        "print(\"Layer 2 input:\", layer2_input)\n",
        "print(\"(Notice it equals layer1 output!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Operations on Values\n",
        "\n",
        "Since you're working with real tensors, you can apply any PyTorch operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.trace(input):\n",
        "    layer1_out = model.layer1.output\n",
        "    \n",
        "    # Apply operations - these are real tensor operations!\n",
        "    max_idx = torch.argmax(layer1_out, dim=1).save()\n",
        "    total = (model.layer1.output.sum() + model.layer2.output.sum()).save()\n",
        "\n",
        "print(\"Max index:\", max_idx)\n",
        "print(\"Total:\", total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Core Paradigm: Interleaving\n",
        "\n",
        "When you write intervention code inside a `with model.trace(...)` block, here's what actually happens:\n",
        "\n",
        "1. **Your code is captured** - nnsight extracts the code inside the `with` block\n",
        "2. **The code is compiled** into an executable function  \n",
        "3. **Your code runs in parallel with the model** - as the model executes its forward pass, your intervention code runs alongside it\n",
        "4. **Your code waits for values** - when you access `.output`, your code pauses until the model reaches that point\n",
        "5. **The model provides values via hooks** - PyTorch hooks inject values into your waiting code\n",
        "6. **Your code can modify values** - before the forward pass continues, you can change activations\n",
        "\n",
        "This process is called **interleaving** - your intervention code and the model's forward pass take turns executing, synchronized at specific points (module inputs and outputs).\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Forward Pass (main)              Intervention Code (your code)     ‚îÇ\n",
        "‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ     ‚îÇ\n",
        "‚îÇ                                                                     ‚îÇ\n",
        "‚îÇ  model(input)                     # Your code starts                ‚îÇ\n",
        "‚îÇ       ‚îÇ                                    ‚îÇ                        ‚îÇ\n",
        "‚îÇ       ‚ñº                                    ‚ñº                        ‚îÇ\n",
        "‚îÇ  layer1.forward()                 hs = model.layer1.output          ‚îÇ\n",
        "‚îÇ       ‚îÇ                                    ‚îÇ                        ‚îÇ\n",
        "‚îÇ       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ hook provides value ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ                        ‚îÇ\n",
        "‚îÇ       ‚îÇ                                    ‚îÇ                        ‚îÇ\n",
        "‚îÇ       ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ your code continues ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ ‚îÇ                        ‚îÇ\n",
        "‚îÇ       ‚îÇ     (can modify value)             ‚îÇ                        ‚îÇ\n",
        "‚îÇ       ‚ñº                                    ‚ñº                        ‚îÇ\n",
        "‚îÇ  layer2.forward()                 out = model.layer2.output         ‚îÇ\n",
        "‚îÇ       ‚îÇ                                    ‚îÇ                        ‚îÇ\n",
        "‚îÇ       ‚ñº                                    ‚ñº                        ‚îÇ\n",
        "‚îÇ  return output                    # Your code finishes              ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "**Key insight:** \n",
        "\n",
        "Because your code waits for values as the forward pass progresses, you **must access modules in the order they execute**.\n",
        "\n",
        "‚úÖ **Correct:** Access layer 0, then layer 5\n",
        "```python\n",
        "with model.trace(\"Hello\"):\n",
        "    layer0_out = model.layers[0].output.save()  # Waits for layer 0\n",
        "    layer5_out = model.layers[5].output.save()  # Then waits for layer 5\n",
        "```\n",
        "\n",
        "‚ùå **Wrong:** Access layer 5, then layer 0\n",
        "```python\n",
        "with model.trace(\"Hello\"):\n",
        "    layer5_out = model.layers[5].output.save()  # Waits for layer 5\n",
        "    layer0_out = model.layers[0].output.save()  # ERROR! Layer 0 already executed\n",
        "    # Raises OutOfOrderError\n",
        "```\n",
        "\n",
        "When you try to access a module that has already executed, nnsight raises an `OutOfOrderError`. This is because the forward pass has already moved past that point - you missed your chance to intercept that value."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"modifying-activations\"></a>\n",
        "# 3. Modifying Activations\n",
        "\n",
        "Now for the powerful part - modifying activations during the forward pass."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## In-Place Modification\n",
        "\n",
        "Use indexing with `[:]` for in-place modifications:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.trace(input):\n",
        "    # Save original (clone first since we'll modify in-place)\n",
        "    before = model.layer1.output.clone().save()\n",
        "    \n",
        "    # Zero out the first dimension\n",
        "    model.layer1.output[:, 0] = 0\n",
        "    \n",
        "    # Save modified\n",
        "    after = model.layer1.output.save()\n",
        "\n",
        "print(\"Before:\", before)\n",
        "print(\"After: \", after)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replacement\n",
        "\n",
        "You can also replace an output entirely:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.trace(input):\n",
        "    original = model.layer1.output.clone()\n",
        "    \n",
        "    # Add noise to the activation\n",
        "    noise = 0.1 * torch.randn_like(original)\n",
        "    model.layer1.output = original + noise\n",
        "    \n",
        "    modified = model.layer1.output.save()\n",
        "\n",
        "print(\"Modified output:\", modified)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Error Handling\n",
        "\n",
        "If you make an error (like invalid indexing), nnsight provides clear error messages with line numbers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This will fail because hidden_dims=10, so valid indices are 0-9\n",
        "try:\n",
        "    with model.trace(input):\n",
        "        model.layer1.output[:, hidden_dims] = 0  # Index 10 is out of bounds!\n",
        "except IndexError as e:\n",
        "    print(\"Caught error:\", e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can toggle detailed error messages with `nnsight.CONFIG.APP.DEBUG`:\n",
        "\n",
        "```python\n",
        "nnsight.CONFIG.APP.DEBUG = True  # Enable (default)\n",
        "nnsight.CONFIG.APP.DEBUG = False  # Disable\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"invokers-and-batching\"></a>\n",
        "# 4. Invokers and Batching\n",
        "\n",
        "So far we've traced single inputs. But often we want to:\n",
        "1. Process multiple inputs efficiently in one forward pass (batching)\n",
        "2. Define separate sets of intervention logic that run serially\n",
        "\n",
        "Both are achieved with **invokers**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The Invoker Context\n",
        "\n",
        "When you call `.trace(input)`, nnsight actually creates two contexts:\n",
        "1. The **tracer context** - manages the overall trace\n",
        "2. The **invoker context** - defines the input and interventions for that input\n",
        "\n",
        "You can create multiple invokers explicitly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.trace() as tracer:  # No input here\n",
        "    \n",
        "    with tracer.invoke(input):  # First invoker with input\n",
        "        out1 = model.output.save()\n",
        "    \n",
        "    with tracer.invoke(input * 2):  # Second invoker with different input\n",
        "        out2 = model.output.save()\n",
        "\n",
        "print(\"Output 1:\", out1)\n",
        "print(\"Output 2:\", out2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How Invokers Work: Serial Execution\n",
        "\n",
        "Each invoker runs its intervention code **serially** - one after another. They wait for values and execute in order:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ  Invoke 1 starts           ‚îÇ  Invoke 2 starts (after 1 finishes) ‚îÇ\n",
        "‚îÇ       ‚îÇ                    ‚îÇ       ‚îÇ                             ‚îÇ\n",
        "‚îÇ       ‚ñº                    ‚îÇ       ‚ñº                             ‚îÇ\n",
        "‚îÇ  Wait for layer1.output    ‚îÇ  Wait for layer1.output             ‚îÇ\n",
        "‚îÇ       ‚îÇ                    ‚îÇ       ‚îÇ                             ‚îÇ\n",
        "‚îÇ       ‚ñº                    ‚îÇ       ‚ñº                             ‚îÇ\n",
        "‚îÇ  Wait for layer2.output    ‚îÇ  Wait for layer2.output             ‚îÇ\n",
        "‚îÇ       ‚îÇ                    ‚îÇ       ‚îÇ                             ‚îÇ\n",
        "‚îÇ       ‚ñº                    ‚îÇ       ‚ñº                             ‚îÇ\n",
        "‚îÇ  Invoke 1 finishes         ‚îÇ  Invoke 2 finishes                  ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "```\n",
        "\n",
        "This serial execution means you can **reference values from earlier invokes**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.trace() as tracer:\n",
        "    \n",
        "    with tracer.invoke(input):\n",
        "        # Save layer1 output from first input\n",
        "        layer1_out = model.layer1.output.save()\n",
        "    \n",
        "    with tracer.invoke(input):\n",
        "        # Use it in the second invoker!\n",
        "        model.layer1.output[:] = layer1_out * 0  # Zero it out\n",
        "        modified_out = model.output.save()\n",
        "    \n",
        "    with tracer.invoke(input):\n",
        "        original_out = model.output.save()\n",
        "\n",
        "print(\"Original output:\", original_out)\n",
        "print(\"Modified output:\", modified_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching: Efficient Multi-Input Processing\n",
        "\n",
        "When you have multiple invokers, their inputs are **batched together** into one forward pass. This is much more efficient than running separate traces:\n",
        "\n",
        "```python\n",
        "# Inefficient: 2 forward passes\n",
        "with model.trace(input1):\n",
        "    out1 = model.output.save()\n",
        "with model.trace(input2):\n",
        "    out2 = model.output.save()\n",
        "\n",
        "# Efficient: 1 forward pass (batched)\n",
        "with model.trace() as tracer:\n",
        "    with tracer.invoke(input1):\n",
        "        out1 = model.output.save()\n",
        "    with tracer.invoke(input2):\n",
        "        out2 = model.output.save()\n",
        "```\n",
        "\n",
        "Each invoker's intervention code only sees its own slice of the batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt-less Invokers: Operating on the Full Batch\n",
        "\n",
        "You can call `.invoke()` with no arguments to create an invoker that operates on **all inputs**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with model.trace() as tracer:\n",
        "    with tracer.invoke(input):\n",
        "        pass  # First input\n",
        "    \n",
        "    with tracer.invoke(input * 2):\n",
        "        pass  # Second input\n",
        "    \n",
        "    # No-arg invoke: sees ALL inputs\n",
        "    with tracer.invoke():\n",
        "        all_outputs = model.output.save()\n",
        "\n",
        "print(\"All outputs shape:\", all_outputs.shape)  # Batch size = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prompt-less invokers are useful when you want to:\n",
        "- Apply the same intervention to all inputs\n",
        "- Collect all outputs together\n",
        "- Add another serial intervention set without changing the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"multi-token-generation\"></a>\n",
        "# 5. Multi-Token Generation\n",
        "\n",
        "Let's scale up to a real language model and handle multi-token generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading a Language Model\n",
        "\n",
        "`LanguageModel` is a subclass of `NNsight` with built-in tokenization and generation support:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nnsight import LanguageModel\n",
        "\n",
        "llm = LanguageModel(\"openai-community/gpt2\", device_map=\"auto\", dispatch=True)\n",
        "\n",
        "print(llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With `LanguageModel`, you can pass strings directly - tokenization happens automatically:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.trace(\"The Eiffel Tower is in the city of\"):\n",
        "    hidden_states = llm.transformer.h[-1].output[0].save()\n",
        "    logits = llm.lm_head.output.save()\n",
        "\n",
        "print(\"Hidden states shape:\", hidden_states.shape)\n",
        "print(\"Predicted token:\", llm.tokenizer.decode(logits[0, -1].argmax()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation with `.generate()`\n",
        "\n",
        "For multi-token generation, use `.generate()` instead of `.trace()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.generate(\"The Eiffel Tower is in\", max_new_tokens=3) as tracer:\n",
        "    output = llm.generator.output.save()\n",
        "\n",
        "print(llm.tokenizer.decode(output[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iterating Over Generation Steps\n",
        "\n",
        "During generation, the same modules are called multiple times (once per token). Use `tracer.iter` to iterate over these steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.generate(\"The Eiffel Tower is in\", max_new_tokens=3) as tracer:\n",
        "    tokens_per_step = list().save()\n",
        "    \n",
        "    # Iterate over ALL generation steps\n",
        "    with tracer.iter[:]:\n",
        "        token = llm.lm_head.output[0, -1].argmax(dim=-1)\n",
        "        tokens_per_step.append(token)\n",
        "\n",
        "print(\"Tokens:\", llm.tokenizer.batch_decode(tokens_per_step))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Iteration Patterns\n",
        "\n",
        "`tracer.iter` accepts slices, indices, or lists:\n",
        "\n",
        "| Pattern | Meaning |\n",
        "|---------|---------|\n",
        "| `tracer.iter[:]` | All steps |\n",
        "| `tracer.iter[0]` | First step only |\n",
        "| `tracer.iter[1:3]` | Steps 1 and 2 |\n",
        "| `tracer.iter[::2]` | Every other step |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conditional Per-Step Interventions\n",
        "\n",
        "Use `as step_idx` to get the current step index for conditional logic:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.generate(\"Hello\", max_new_tokens=5) as tracer:\n",
        "    hidden_states = list().save()\n",
        "    \n",
        "    with tracer.iter[:] as step_idx:\n",
        "        # Only intervene on step 2\n",
        "        if step_idx == 2:\n",
        "            llm.transformer.h[0].output[0][:] = 0\n",
        "        \n",
        "        hidden_states.append(llm.transformer.h[-1].output[0].clone())\n",
        "\n",
        "print(f\"Collected {len(hidden_states)} hidden states\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shorthand: `tracer.all()`\n",
        "\n",
        "`tracer.all()` is equivalent to `tracer.iter[:]`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.generate(\"Hello\", max_new_tokens=3) as tracer:\n",
        "    tokens = list().save()\n",
        "    \n",
        "    with tracer.all():\n",
        "        tokens.append(llm.lm_head.output[0, -1].argmax(dim=-1))\n",
        "\n",
        "print(\"All tokens:\", llm.tokenizer.batch_decode(tokens))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Manual Step Advancement: `.next()`\n",
        "\n",
        "For finer control, use `.next()` to manually advance through steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.generate(\"Hello\", max_new_tokens=3) as tracer:\n",
        "    # Step 0 (automatic)\n",
        "    hs0 = llm.transformer.h[-1].output[0].save()\n",
        "    \n",
        "    tracer.next()  # Move to step 1\n",
        "    \n",
        "    hs1 = llm.transformer.h[-1].output[0].save()\n",
        "\n",
        "print(\"Step 0 shape:\", hs0.shape)\n",
        "print(\"Step 1 shape:\", hs1.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"gradients\"></a>\n",
        "# 6. Gradients\n",
        "\n",
        "nnsight supports gradient access and modification through a backward tracing context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## The `with tensor.backward():` Context\n",
        "\n",
        "To access gradients, use the tensor's `.backward()` as a context manager:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.trace(\"Hello\"):\n",
        "    # Get hidden states and enable gradients\n",
        "    hs = llm.transformer.h[-1].output[0]\n",
        "    hs.requires_grad_(True)\n",
        "    \n",
        "    # Compute loss\n",
        "    logits = llm.lm_head.output\n",
        "    loss = logits.sum()\n",
        "    \n",
        "    # Access gradients inside backward context\n",
        "    with loss.backward():\n",
        "        grad = hs.grad.save()\n",
        "\n",
        "print(\"Gradient shape:\", grad.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Important rules for gradients:**\n",
        "\n",
        "1. `.grad` is only accessible **inside** a `with tensor.backward():` context\n",
        "2. `.grad` is a property of **tensors**, not modules\n",
        "3. Access the tensor's `.output` **before** entering the backward context"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Modifying Gradients\n",
        "\n",
        "You can modify gradients just like activations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.trace(\"Hello\"):\n",
        "    hs = llm.transformer.h[-1].output[0]\n",
        "    hs.requires_grad_(True)\n",
        "    \n",
        "    logits = llm.lm_head.output\n",
        "    loss = logits.sum()\n",
        "    \n",
        "    with loss.backward():\n",
        "        # Save original gradient\n",
        "        original_grad = hs.grad.clone().save()\n",
        "        \n",
        "        # Modify gradient\n",
        "        hs.grad[:] = 0\n",
        "        \n",
        "        # Save modified\n",
        "        modified_grad = hs.grad.save()\n",
        "\n",
        "print(\"Original grad mean:\", original_grad.mean().item())\n",
        "print(\"Modified grad mean:\", modified_grad.mean().item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"advanced-features\"></a>\n",
        "# 7. Advanced Features\n",
        "\n",
        "Let's explore some powerful advanced features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.1 Source Tracing\n",
        "\n",
        "Sometimes you need to access values **inside** a module's forward pass, not just its inputs and outputs. The `.source` property enables this by rewriting the forward method to hook every operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Print source to discover available operations\n",
        "print(llm.transformer.h[0].attn.source)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.trace(\"Hello\"):\n",
        "    # Access an internal operation by name\n",
        "    attn_output = llm.transformer.h[0].attn.source.attention_interface_0.output.save()\n",
        "\n",
        "print(\"Attention output type:\", type(attn_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Source operations have the same interface as modules: `.output`, `.input`, `.inputs`. You can even trace into nested calls with recursive `.source`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.2 Caching Activations\n",
        "\n",
        "Use `tracer.cache()` to automatically save all module outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.trace(\"Hello\") as tracer:\n",
        "    cache = tracer.cache()\n",
        "\n",
        "# Access cached values after the trace\n",
        "print(\"Layer 0 output shape:\", cache['model.transformer.h.0'].output[0].shape)\n",
        "\n",
        "# Attribute-style access also works\n",
        "print(\"Same thing:\", cache.model.transformer.h[0].output[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cache options:\n",
        "```python\n",
        "cache = tracer.cache(\n",
        "    include_inputs=True,   # Also cache inputs\n",
        "    include_output=True,   # Cache outputs (default)\n",
        "    modules=[model.layer1, model.layer2],  # Specific modules only\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.3 Barriers for Synchronization\n",
        "\n",
        "When sharing values between invokes, sometimes you need to ensure one invoke has computed a value before another uses it. Use `tracer.barrier()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.trace() as tracer:\n",
        "    barrier = tracer.barrier(2)  # Create barrier for 2 participants\n",
        "    \n",
        "    with tracer.invoke(\"The Eiffel Tower is in\"):\n",
        "        embeddings = llm.transformer.wte.output\n",
        "        barrier()  # Wait here\n",
        "    \n",
        "    with tracer.invoke(\"_ _ _ _ _ _ _\"):\n",
        "        barrier()  # Wait here too\n",
        "        llm.transformer.wte.output = embeddings  # Now safe to use\n",
        "        out = llm.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "print(\"Prediction:\", llm.tokenizer.decode(out[0, -1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.4 Getting the Trace Result\n",
        "\n",
        "Use `tracer.result()` to get the final output of the traced function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.trace(\"Hello\") as tracer:\n",
        "    hidden = llm.transformer.h[-1].output[0].save()\n",
        "    result = tracer.result().save()\n",
        "\n",
        "print(\"Result keys:\", result.keys() if hasattr(result, 'keys') else type(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.5 Module Skipping\n",
        "\n",
        "Skip a module's execution entirely and substitute your own value:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.trace(\"Hello\"):\n",
        "    # Get layer 0 output\n",
        "    layer0_out = llm.transformer.h[0].output\n",
        "    \n",
        "    # Skip layer 1 - use layer 0's output instead\n",
        "    llm.transformer.h[1].skip(layer0_out)\n",
        "    \n",
        "    # Layer 1's output now equals layer 0's output\n",
        "    layer1_out = llm.transformer.h[1].output.save()\n",
        "\n",
        "print(\"Skipped layer 1 - output substituted\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.6 Early Stopping\n",
        "\n",
        "Stop execution early when you only need early layers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.trace(\"Hello\") as tracer:\n",
        "    layer0 = llm.transformer.h[0].output[0].save()\n",
        "    tracer.stop()  # Don't execute remaining layers\n",
        "\n",
        "print(\"Early stop - only ran first layer\")\n",
        "print(\"Layer 0 shape:\", layer0.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7.7 Scanning (Shape Inference)\n",
        "\n",
        "Get shapes without running the full model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llm.scan(\"Hello\"):\n",
        "    dim = llm.transformer.h[0].output[0].shape[-1]\n",
        "\n",
        "print(\"Hidden dimension:\", dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"model-editing\"></a>\n",
        "# 8. Model Editing\n",
        "\n",
        "Create persistent model modifications that apply to all future traces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, get hidden states that predict \"Paris\"\n",
        "with llm.trace(\"The Eiffel Tower is in the city of\"):\n",
        "    paris_hidden = llm.transformer.h[-1].output[0][:, -1, :].save()\n",
        "\n",
        "# Create an edited model that always uses these hidden states\n",
        "with llm.edit() as llm_edited:\n",
        "    llm.transformer.h[-1].output[0][:, -1, :] = paris_hidden\n",
        "\n",
        "# Original model: normal prediction\n",
        "with llm.trace(\"Vatican is in the city of\"):\n",
        "    original = llm.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "# Edited model: always predicts \"Paris\"!\n",
        "with llm_edited.trace(\"Vatican is in the city of\"):\n",
        "    modified = llm.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "print(\"Original:\", llm.tokenizer.decode(original[0, -1]))\n",
        "print(\"Edited:  \", llm.tokenizer.decode(modified[0, -1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## In-Place Editing\n",
        "\n",
        "For edits that modify the original model:\n",
        "\n",
        "```python\n",
        "with llm.edit(inplace=True):\n",
        "    llm.transformer.h[0].output[0][:] = 0\n",
        "```\n",
        "\n",
        "Clear in-place edits with:\n",
        "```python\n",
        "llm.clear_edits()\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a name=\"remote-execution\"></a>\n",
        "# 9. Remote Execution (NDIF)\n",
        "\n",
        "nnsight can run interventions on large models hosted by the National Deep Inference Fabric (NDIF). Everything works the same - just add `remote=True`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Get your API key at https://login.ndif.us, then configure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nnsight import CONFIG\n",
        "\n",
        "CONFIG.set_default_api_key(\"YOUR_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check available models at https://nnsight.net/status/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Remote Tracing\n",
        "\n",
        "Load a large model and run remotely:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['HF_TOKEN'] = \"YOUR_HUGGING_FACE_TOKEN\"\n",
        "\n",
        "llama = LanguageModel(\"meta-llama/Meta-Llama-3.1-8B\")\n",
        "\n",
        "# Just add remote=True - everything else is the same!\n",
        "with llama.trace(\"The Eiffel Tower is in the city of\", remote=True):\n",
        "    hidden_states = llama.model.layers[-1].output.save()\n",
        "    output = llama.output.save()\n",
        "\n",
        "print(\"Hidden states shape:\", hidden_states[0].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sessions\n",
        "\n",
        "Group multiple traces for efficiency:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llama.session(remote=True) as session:\n",
        "    \n",
        "    with llama.trace(\"The Eiffel Tower is in the city of\"):\n",
        "        hs = llama.model.layers[31].output[0][:, -1, :]\n",
        "        t1_out = llama.lm_head.output.argmax(dim=-1).save()\n",
        "    \n",
        "    with llama.trace(\"Buckingham Palace is in the city of\"):\n",
        "        llama.model.layers[1].output[0][:, -1, :] = hs\n",
        "        t2_out = llama.lm_head.output.argmax(dim=-1).save()\n",
        "\n",
        "print(\"T1:\", llama.tokenizer.decode(t1_out[0, -1]))\n",
        "print(\"T2:\", llama.tokenizer.decode(t2_out[0, -1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Streaming with `nnsight.local()`\n",
        "\n",
        "Execute operations locally during remote execution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with llama.trace(\"Hello\", remote=True) as tracer:\n",
        "    \n",
        "    with nnsight.local():\n",
        "        # This runs on your machine, not the server\n",
        "        hs = llama.model.layers[-1].output[0]\n",
        "        print(\"Local computation:\", hs[0, 0, 0])  # Runs locally\n",
        "    \n",
        "    out = llama.lm_head.output.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Next Steps\n",
        "\n",
        "Congratulations! You've learned the core concepts of nnsight:\n",
        "\n",
        "- **The interleaving paradigm** - your code runs alongside the model, waiting for values\n",
        "- **Accessing and modifying activations** - `.output`, `.input`, `.save()`\n",
        "- **Invokers and batching** - efficient multi-input processing\n",
        "- **Multi-token generation** - `.iter`, `.all()`, `.next()`\n",
        "- **Gradients** - `with tensor.backward():`\n",
        "- **Advanced features** - source tracing, caching, barriers, and more\n",
        "- **Remote execution** - running on NDIF\n",
        "\n",
        "For more tutorials implementing classic interpretability techniques, visit [nnsight.net/tutorials](https://nnsight.net/tutorials).\n",
        "\n",
        "For deep technical details, see the [NNsight.md](https://github.com/ndif-team/nnsight/blob/main/NNsight.md) document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Getting Involved!\n",
        "\n",
        "Both nnsight and NDIF are in active development. Join us:\n",
        "\n",
        "- **Discord:** [discord.gg/6uFJmCSwW7](https://discord.gg/6uFJmCSwW7)\n",
        "- **Forum:** [discuss.ndif.us](https://discuss.ndif.us/)\n",
        "- **Twitter/X:** [@ndif_team](https://x.com/ndif_team)\n",
        "- **LinkedIn:** [National Deep Inference Fabric](https://www.linkedin.com/company/national-deep-inference-fabric/)\n",
        "\n",
        "We'd love to hear about your work using nnsight! üíü"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
