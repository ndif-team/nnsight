{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit Draft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TorchDynamo** is a Python-level Just-In-Time (JIT) compiler designed to make unmodified PyTorch programs faster. TorchDynamo hooks into the frame evaluation API in CPython ([PEP 523](https://peps.python.org/pep-0523/)) to dynamically modify Python bytecode right before it is executed. It rewrites Python bytecode to extract sequences of PyTorch operations into an [FX Graph](https://pytorch.org/docs/stable/fx.html) which is then compiled with a customizable backend. It creates this FX Graph through bytecode analysis and is designed to mix Python execution with compiled backends to get the best of both worlds â€” usability and performance.\n",
    "\n",
    "*From [TorchDynamo Deep Dive](https://pytorch.org/docs/stable/torch.compiler_deepdive.html)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (Ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/u/caden/.conda/envs/interp/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "from nnsight.util import WrapperModule\n",
    "from nnsight.edit import print_gm, Edit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple torch model to demonstrate how operations are translated into a Torch FX graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[39.3079]], grad_fn=<SplitBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "class WrappedLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = x * 100\n",
    "        return x\n",
    "\n",
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1, 1)\n",
    "        self.wrapped = WrappedLayer()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.wrapped(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.split(1, dim=-1)\n",
    "        return x\n",
    "\n",
    "mod = M()\n",
    "\n",
    "input_tensor = torch.tensor([[1.0]])\n",
    "output = mod(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch Compile is another method to speed up PyTorch code. It uses Dynamo under the hood to JIT compile arbitrary Python code. We'll use it as an easy interface for accessing FX GraphModules compiled by Dynamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name     target                       args           kwargs\n",
      "-------------  -------  ---------------------------  -------------  -----------\n",
      "placeholder    l_x_     L_x_                         ()             {}\n",
      "call_module    x        L__self___layer1             (l_x_,)        {}\n",
      "call_module    x_1      L__self___wrapped_layer1     (x,)           {}\n",
      "call_function  x_3      <built-in function mul>      (x_1, 100)     {}\n",
      "call_module    x_4      L__self___dropout            (x_3,)         {}\n",
      "call_method    split    split                        (x_4, 1)       {'dim': -1}\n",
      "call_function  getitem  <built-in function getitem>  (split, 0)     {}\n",
      "output         output   output                       ((getitem,),)  {}\n"
     ]
    }
   ],
   "source": [
    "def custom_backend(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "    gm.graph.print_tabular()\n",
    "    gm.recompile()\n",
    "\n",
    "    return gm.forward\n",
    "\n",
    "torch._dynamo.reset()\n",
    "\n",
    "opt_model = torch.compile(mod, backend=custom_backend, dynamic=True)\n",
    "gm = opt_model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the functions and components we declared in the module are translated into nodes and their respective operations in an FX graph. Dynamo will trace through user defined modules such as WrappedLayer, breaking apart the operations on its forward pass into separate nodes on the FX graph.\n",
    "\n",
    "Now, let's see what happens if we load this module into NNsight and trace it. Note how we call `torch._dynamo.reset()` to signify that we wish to **SOMETHING ABOUT NEW BACKEND**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name    target                   args       kwargs\n",
      "-------------  ------  -----------------------  ---------  --------\n",
      "placeholder    x       L_stack0_                ()         {}\n",
      "call_function  x_1     <built-in function mul>  (x, 100)   {}\n",
      "output         output  output                   ((x_1,),)  {}\n",
      "opcode         name     target                       args           kwargs\n",
      "-------------  -------  ---------------------------  -------------  -----------\n",
      "placeholder    x        L_stack0_                    ()             {}\n",
      "call_method    split    split                        (x, 1)         {'dim': -1}\n",
      "call_function  getitem  <built-in function getitem>  (split, 0)     {}\n",
      "output         output   output                       ((getitem,),)  {}\n"
     ]
    }
   ],
   "source": [
    "from nnsight import NNsight\n",
    "\n",
    "nn_model = NNsight(mod)\n",
    "\n",
    "torch._dynamo.reset()\n",
    "\n",
    "opt_model = torch.compile(nn_model._model, backend=custom_backend, dynamic=True)\n",
    "gm = opt_model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading our model with NNsight, we find that our Dynamo has produced two separate graphs. When TorchDynamo encounters unsupported Python features, such as data-dependent control flow, it breaks the computation graph, lets the default Python interpreter handle the unsupported code, then resumes capturing the graph. (*From [TorchDynamo Deep Dive](https://pytorch.org/docs/stable/torch.compiler_deepdive.html)*)\n",
    "\n",
    "We can see where TorchDynamo breaks the graph by using `torch._dynamo.explain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph Count: 2\n",
      "Graph Break Count: 1\n",
      "Op Count: 2\n",
      "Break Reasons:\n",
      "Ops per Graph:\n",
      "  Ops 1:\n",
      "    <built-in function mul>\n",
      "  Ops 2:\n",
      "    <built-in function getitem>\n",
      "Out Guards:\n",
      "  Guard 1:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140137739198016))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 2:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 3:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '439bf2fc23bdcb2fde33e5ca14d63313'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 4:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 5:\n",
      "    Name: \"L['self']\"\n",
      "    Source: local\n",
      "    Create Function: NN_MODULE\n",
      "    Guard Types: ['ID_MATCH']\n",
      "    Code List: [\"___check_obj_id(L['self'], 140137739077264)\"]\n",
      "    Object Weakref: <weakref at 0x7f745bdc2e30; to 'M' at 0x7f745c284690>\n",
      "    Guarded Class Weakref: <weakref at 0x7f745c25e7f0; to 'type' at 0x9514c20 (M)>\n",
      "  Guard 6:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 7:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: HAS_GRAPH_BREAK\n",
      "    Guard Types: ['HAS_GRAPH_BREAK']\n",
      "    Code List: ['not ___needs_nopython()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 8:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 9:\n",
      "    Name: \"L['x']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f745b97e250; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f74446b5710; to 'torch._C._TensorMeta' at 0x5e20cf0 (Tensor)>\n",
      "  Guard 10:\n",
      "    Name: \"L['self'].layer1\"\n",
      "    Source: local_nn_module\n",
      "    Create Function: TYPE_MATCH\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 11:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 12:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140137739198016))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 13:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 14:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '439bf2fc23bdcb2fde33e5ca14d63313'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 15:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 16:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 17:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: HAS_GRAPH_BREAK\n",
      "    Guard Types: ['HAS_GRAPH_BREAK']\n",
      "    Code List: ['not ___needs_nopython()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 18:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 19:\n",
      "    Name: \"L['input'][0]\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['input'][0], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f745b97e250; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f74446b5710; to 'torch._C._TensorMeta' at 0x5e20cf0 (Tensor)>\n",
      "  Guard 20:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 21:\n",
      "    Name: \"G['detect_fake_mode']\"\n",
      "    Source: global\n",
      "    Create Function: FUNCTION_MATCH\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 22:\n",
      "    Name: \"L['input']\"\n",
      "    Source: local\n",
      "    Create Function: LIST_LENGTH\n",
      "    Guard Types: ['LIST_LENGTH']\n",
      "    Code List: [\"___check_type_id(L['input'], 8810304)\", \"len(L['input']) == 1\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: <weakref at 0x7f75439df6a0; to 'type' at 0x866f40 (tuple)>\n",
      "  Guard 23:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140137739198016))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 24:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 25:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '439bf2fc23bdcb2fde33e5ca14d63313'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 26:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 27:\n",
      "    Name: \"L['self'].wrapped\"\n",
      "    Source: local_nn_module\n",
      "    Create Function: NN_MODULE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 28:\n",
      "    Name: \"L['self']\"\n",
      "    Source: local\n",
      "    Create Function: NN_MODULE\n",
      "    Guard Types: ['ID_MATCH']\n",
      "    Code List: [\"___check_obj_id(L['self'], 140137739077264)\"]\n",
      "    Object Weakref: <weakref at 0x7f745bdc2e30; to 'M' at 0x7f745c284690>\n",
      "    Guarded Class Weakref: <weakref at 0x7f745c25e7f0; to 'type' at 0x9514c20 (M)>\n",
      "  Guard 29:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 30:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: HAS_GRAPH_BREAK\n",
      "    Guard Types: ['HAS_GRAPH_BREAK']\n",
      "    Code List: ['not ___needs_nopython()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 31:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 32:\n",
      "    Name: \"L['___stack0']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['___stack0'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f745b95ba10; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f74446b5710; to 'torch._C._TensorMeta' at 0x5e20cf0 (Tensor)>\n",
      "  Guard 33:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 34:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140137739198016))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 35:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 36:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '439bf2fc23bdcb2fde33e5ca14d63313'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 37:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 38:\n",
      "    Name: \"L['x']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['x'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f745b95ba10; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f74446b5710; to 'torch._C._TensorMeta' at 0x5e20cf0 (Tensor)>\n",
      "  Guard 39:\n",
      "    Name: \"L['self']\"\n",
      "    Source: local\n",
      "    Create Function: NN_MODULE\n",
      "    Guard Types: ['ID_MATCH']\n",
      "    Code List: [\"___check_obj_id(L['self'], 140137954976784)\"]\n",
      "    Object Weakref: <weakref at 0x7f745ba9e8e0; to 'WrappedLayer' at 0x7f746906a410>\n",
      "    Guarded Class Weakref: <weakref at 0x7f745c25e7a0; to 'type' at 0x741f4e0 (WrappedLayer)>\n",
      "  Guard 40:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 41:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: HAS_GRAPH_BREAK\n",
      "    Guard Types: ['HAS_GRAPH_BREAK']\n",
      "    Code List: ['not ___needs_nopython()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 42:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 43:\n",
      "    Name: \"L['self'].layer1\"\n",
      "    Source: local_nn_module\n",
      "    Create Function: TYPE_MATCH\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 44:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 45:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140137739198016))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 46:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 47:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '439bf2fc23bdcb2fde33e5ca14d63313'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 48:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 49:\n",
      "    Name: \"L['input'][0]\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['input'][0], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f745b95ba10; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f74446b5710; to 'torch._C._TensorMeta' at 0x5e20cf0 (Tensor)>\n",
      "  Guard 50:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 51:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: HAS_GRAPH_BREAK\n",
      "    Guard Types: ['HAS_GRAPH_BREAK']\n",
      "    Code List: ['not ___needs_nopython()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 52:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 53:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 54:\n",
      "    Name: \"G['detect_fake_mode']\"\n",
      "    Source: global\n",
      "    Create Function: FUNCTION_MATCH\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 55:\n",
      "    Name: \"L['input']\"\n",
      "    Source: local\n",
      "    Create Function: LIST_LENGTH\n",
      "    Guard Types: ['LIST_LENGTH']\n",
      "    Code List: [\"___check_type_id(L['input'], 8810304)\", \"len(L['input']) == 1\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: <weakref at 0x7f75439df6a0; to 'type' at 0x866f40 (tuple)>\n",
      "  Guard 56:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140137739198016))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 57:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 58:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '439bf2fc23bdcb2fde33e5ca14d63313'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 59:\n",
      "    Name: \"L['___stack0']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['___stack0'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f745ba9eac0; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f74446b5710; to 'torch._C._TensorMeta' at 0x5e20cf0 (Tensor)>\n",
      "  Guard 60:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 61:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 62:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 63:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 64:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140137739198016))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 65:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 66:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '439bf2fc23bdcb2fde33e5ca14d63313'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 67:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 68:\n",
      "    Name: \"L['___stack0']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['___stack0'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f745bb42de0; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f74446b5710; to 'torch._C._TensorMeta' at 0x5e20cf0 (Tensor)>\n",
      "  Guard 69:\n",
      "    Name: \"L['self']\"\n",
      "    Source: local\n",
      "    Create Function: NN_MODULE\n",
      "    Guard Types: ['ID_MATCH']\n",
      "    Code List: [\"___check_obj_id(L['self'], 140137739077264)\"]\n",
      "    Object Weakref: <weakref at 0x7f745bdc2e30; to 'M' at 0x7f745c284690>\n",
      "    Guarded Class Weakref: <weakref at 0x7f745c25e7f0; to 'type' at 0x9514c20 (M)>\n",
      "  Guard 70:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 71:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: HAS_GRAPH_BREAK\n",
      "    Guard Types: ['HAS_GRAPH_BREAK']\n",
      "    Code List: ['not ___needs_nopython()']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 72:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 73:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 74:\n",
      "    Name: \"L['self'].dropout\"\n",
      "    Source: local_nn_module\n",
      "    Create Function: TYPE_MATCH\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 75:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: BACKEND_MATCH\n",
      "    Guard Types: ['BACKEND_MATCH']\n",
      "    Code List: ['(___skip_backend_check() or ___current_backend() == ___lookup_backend(140137739198016))']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 76:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: GRAD_MODE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 77:\n",
      "    Name: \"L['___stack0']\"\n",
      "    Source: local\n",
      "    Create Function: TENSOR_MATCH\n",
      "    Guard Types: ['TENSOR_MATCH']\n",
      "    Code List: [\"hasattr(L['___stack0'], '_dynamo_dynamic_indices') == False\"]\n",
      "    Object Weakref: <weakref at 0x7f745bae1210; dead>\n",
      "    Guarded Class Weakref: <weakref at 0x7f74446b5710; to 'torch._C._TensorMeta' at 0x5e20cf0 (Tensor)>\n",
      "  Guard 78:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: CONFIG_HASH_MATCH\n",
      "    Guard Types: ['CONFIG_HASH_MATCH']\n",
      "    Code List: [\"___compile_config_hash() == '439bf2fc23bdcb2fde33e5ca14d63313'\"]\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 79:\n",
      "    Name: ''\n",
      "    Source: shape_env\n",
      "    Create Function: SHAPE_ENV\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 80:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DETERMINISTIC_ALGORITHMS\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 81:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: TORCH_FUNCTION_STATE\n",
      "    Guard Types: None\n",
      "    Code List: None\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "  Guard 82:\n",
      "    Name: ''\n",
      "    Source: global\n",
      "    Create Function: DEFAULT_DEVICE\n",
      "    Guard Types: ['DEFAULT_DEVICE']\n",
      "    Code List: ['utils_device.CURRENT_DEVICE == None']\n",
      "    Object Weakref: None\n",
      "    Guarded Class Weakref: None\n",
      "Compile Times: TorchDynamo compilation metrics:\n",
      "Function                         Runtimes (s)\n",
      "-------------------------------  ----------------------------------------------------------------------\n",
      "_compile.<locals>.compile_inner  0.0284, 0.0089, 0.0028, 0.0334, 0.0245, 0.0085, 0.0147, 0.0264, 0.0070\n",
      "OutputGraph.call_user_compiler   0.0078, 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch._dynamo.reset()\n",
    "explain_output = torch._dynamo.explain(nn_model._model)(torch.tensor([[1.0]]))\n",
    "print(explain_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the graph broke once on the multiply `x = x * 100`. We already knew this by looking at the broken graph. \n",
    "\n",
    "We can force TorchDynamo to raise an error upon the first graph break encountered by using `fullgraph=True`. The stack trace will provide more details on exactly what is breaking our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2854454/1892653539.py\", line 5, in <module>\n",
      "    opt_bar(torch.tensor([[1.0]]))\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 489, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1561, in _call_impl\n",
      "    result = forward_call(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py\", line 655, in catch_errors\n",
      "    return callback(frame, cache_entry, hooks, frame_state)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 383, in _convert_frame_assert\n",
      "    compiled_product = _compile(\n",
      "                       ^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 646, in _compile\n",
      "    guarded_code = compile_inner(code, one_graph, hooks, transform)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/utils.py\", line 244, in time_wrapper\n",
      "    r = func(*args, **kwargs)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 562, in compile_inner\n",
      "    out_code = transform_code_object(code, transform)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py\", line 1033, in transform_code_object\n",
      "    transformations(instructions, code_options)\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 151, in _fn\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py\", line 527, in transform\n",
      "    tracer.run()\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2128, in run\n",
      "    super().run()\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\n",
      "    and self.step()\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 470, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1802, in CALL\n",
      "    self.call_function(fn, args, kwargs)\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 652, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/variables/nn_module.py\", line 702, in call_function\n",
      "    return variables.UserFunctionVariable(fn, source=source).call_function(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 248, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py\", line 81, in call_function\n",
      "    return tx.inline_user_function_return(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 688, in inline_user_function_return\n",
      "    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2261, in inline_call\n",
      "    return cls.inline_call_(parent, func, args, kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 2376, in inline_call_\n",
      "    tracer.run()\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 818, in run\n",
      "    and self.step()\n",
      "        ^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 781, in step\n",
      "    getattr(self, inst.opname)(inst)\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 470, in wrapper\n",
      "    return inner_fn(self, inst)\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 1802, in CALL\n",
      "    self.call_function(fn, args, kwargs)\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py\", line 652, in call_function\n",
      "    self.push(fn.call_function(self, args, kwargs))\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/variables/user_defined.py\", line 437, in call_function\n",
      "    return super().call_function(tx, args, kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/variables/base.py\", line 311, in call_function\n",
      "    unimplemented(f\"call_function {self} {args} {kwargs}\")\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/_dynamo/exc.py\", line 193, in unimplemented\n",
      "    raise Unsupported(msg)\n",
      "torch._dynamo.exc.Unsupported: call_function UserDefinedObjectVariable(_hook) [UnspecializedNNModuleVariable(Linear), TupleVariable(), ConstDictVariable(), TensorVariable()] {}\n",
      "\n",
      "from user code:\n",
      "   File \"/tmp/ipykernel_2854454/671300478.py\", line 19, in forward\n",
      "    x = self.layer1(x)\n",
      "  File \"/share/u/caden/.local/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1572, in _call_impl\n",
      "    hook_result = hook(self, args, kwargs, result)\n",
      "\n",
      "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
      "\n",
      "\n",
      "You can suppress this exception and fall back to eager by setting:\n",
      "    import torch._dynamo\n",
      "    torch._dynamo.config.suppress_errors = True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback as tb\n",
    "\n",
    "opt_bar = torch.compile(nn_model._model, fullgraph=True)\n",
    "try:\n",
    "    opt_bar(torch.tensor([[1.0]]))\n",
    "except:\n",
    "    tb.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expanding the error trace reveals this line toward the end. \n",
    "\n",
    "```\n",
    "torch._dynamo.exc.Unsupported: call_function UserDefinedObjectVariable(_hook) [UnspecializedNNModuleVariable(Linear), TupleVariable(), ConstDictVariable(), TensorVariable()] {}\n",
    "```\n",
    "\n",
    "This message indicates Dynamo ran into an unsupported Python feature - some forward_hook - and broke the graph. \n",
    "\n",
    "We can remove NNsight hooks by accessing the underlying `._envoy` and clearing the hooks with `.clear_hooks(propagate=True)`. Propagate tells NNsight to remove the hooks of an envoy's sub_envoys too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opcode         name                             target                                args                                                                 kwargs\n",
      "-------------  -------------------------------  ------------------------------------  -------------------------------------------------------------------  -----------\n",
      "placeholder    l_x_                             L_x_                                  ()                                                                   {}\n",
      "get_attr       l__self___layer1_weight          L__self___layer1_weight               ()                                                                   {}\n",
      "get_attr       l__self___layer1_bias            L__self___layer1_bias                 ()                                                                   {}\n",
      "call_function  x                                <built-in function linear>            (l_x_, l__self___layer1_weight, l__self___layer1_bias)               {}\n",
      "get_attr       l__self___wrapped_layer1_weight  L__self___wrapped_layer1_weight       ()                                                                   {}\n",
      "get_attr       l__self___wrapped_layer1_bias    L__self___wrapped_layer1_bias         ()                                                                   {}\n",
      "call_function  x_1                              <built-in function linear>            (x, l__self___wrapped_layer1_weight, l__self___wrapped_layer1_bias)  {}\n",
      "call_function  x_3                              <built-in function mul>               (x_1, 100)                                                           {}\n",
      "call_function  x_4                              <function dropout at 0x7f7443fabba0>  (x_3, 0.1, True, False)                                              {}\n",
      "call_method    split                            split                                 (x_4, 1)                                                             {'dim': -1}\n",
      "call_function  getitem                          <built-in function getitem>           (split, 0)                                                           {}\n",
      "output         output                           output                                ((getitem,),)                                                        {}\n"
     ]
    }
   ],
   "source": [
    "nn_model._envoy.clear_hooks(propagate=True)\n",
    "\n",
    "torch._dynamo.reset()\n",
    "\n",
    "opt_model = torch.compile(nn_model._model, backend=custom_backend, dynamic=True)\n",
    "gm = opt_model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Intervening on the FX Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TorchDynamo is a really powerful tool for compiling torch modules to improve performance and efficiency at scale. \n",
    "\n",
    "https://depyf.readthedocs.io/en/latest/walk_through.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we used torch compile to attach arbitrary modules at any point in an existing module's computation? There are a couple obvious benefits: \n",
    "\n",
    "1. Edit models to access arbitrary attributes that aren't normally availible.\n",
    "2. Add modules such as dictionaries or lora weights and access the hidden states of those modules - on a forward or backward pass - with hooks. \n",
    "3. We can just host one module on NDIF and use Torch compile to recompile existing modules. Compile simply returns an optimized module wrapper over the existing module, so we don't have to host multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's declare a simple model to see how we can wrap one of its attributes below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.0138]], grad_fn=<SplitBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "class M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(1, 1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "\n",
    "        value = x[:,0]\n",
    "        x = x * value\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = x.split(1, dim=-1)\n",
    "        return x\n",
    "\n",
    "mod = M()\n",
    "\n",
    "input_tensor = torch.tensor([[1.0]])\n",
    "output = mod(input_tensor)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we'd like to access the `value` attribute. We wouldn't normally be able to do this with hooks because its not declared as a class variable.\n",
    "\n",
    "We create the `WrapperModule` class which just passes an input through itself. By setting it as an attribute of the parent module, we can access the input and output of this wrapper with hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M(\n",
      "  (layer1): Linear(in_features=1, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (value_wrapper): WrapperModule()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class WrapperModule(torch.nn.Module):\n",
    "    \"\"\"Simple torch module which passes it's input through. Useful for hooking.\n",
    "    If there is only one argument, returns the first element.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if len(args) == 1:\n",
    "            args = args[0]\n",
    "\n",
    "        return args\n",
    "    \n",
    "wrapper_module = WrapperModule()\n",
    "wrapper_name = 'value_wrapper'\n",
    "\n",
    "setattr(mod, wrapper_name, wrapper_module)\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out at what node to insert the model, we can compile and print the graph module. This returns the recompiled bytecode from TorchDynamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule()\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, L_x_ : torch.Tensor):\n",
      "    l_x_ = L_x_\n",
      "    l__self___layer1_weight = self.L__self___layer1_weight\n",
      "    l__self___layer1_bias = self.L__self___layer1_bias\n",
      "    x = torch._C._nn.linear(l_x_, l__self___layer1_weight, l__self___layer1_bias);  l_x_ = l__self___layer1_weight = l__self___layer1_bias = None\n",
      "    value = x[(slice(None, None, None), 0)]\n",
      "    x_1 = x * value;  x = value = None\n",
      "    x_2 = torch.nn.functional.dropout(x_1, 0.1, True, False);  x_1 = None\n",
      "    split = x_2.split(1, dim = -1);  x_2 = None\n",
      "    getitem_1 = split[0];  split = None\n",
      "    return (getitem_1,)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "def custom_backend(gm: torch.fx.GraphModule, _: List[torch.Tensor]):\n",
    "    print(gm)\n",
    "    return gm.forward\n",
    "\n",
    "torch._dynamo.reset()\n",
    "opt_model = torch.compile(mod, backend=custom_backend, dynamic=True, fullgraph=True)\n",
    "gm = opt_model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "value = x[(slice(None, None, None), 0)]\n",
    "x_1 = x * value;  x = value = None\n",
    "```\n",
    "\n",
    "From these lines, we see that value is:\n",
    "1. A node, with args `x` and `slice(...)` representing some `call_method` operation.\n",
    "2. An argument to the node `x_1`. \n",
    "\n",
    "Let's try wrapping `value` as it's passeed as an arg into `x_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrapperModule(torch.nn.Module):\n",
    "    \"\"\"Simple torch module which passes it's input through. Useful for hooking.\n",
    "    If there is only one argument, returns the first element.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if len(args) == 1:\n",
    "            args = args[0]\n",
    "\n",
    "        return args\n",
    "    \n",
    "wrapper_module = WrapperModule()\n",
    "wrapper_name = 'value'\n",
    "\n",
    "setattr(mod, wrapper_name, wrapper_module)\n",
    "print(mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphModule(\n",
      "  (value_wrapper): WrapperModule()\n",
      ")\n",
      "\n",
      "\n",
      "\n",
      "def forward(self, L_x_ : torch.Tensor):\n",
      "    l_x_ = L_x_\n",
      "    l__self___layer1_weight = self.L__self___layer1_weight\n",
      "    l__self___layer1_bias = self.L__self___layer1_bias\n",
      "    x = torch._C._nn.linear(l_x_, l__self___layer1_weight, l__self___layer1_bias);  l_x_ = l__self___layer1_weight = l__self___layer1_bias = None\n",
      "    getitem_2 = x[(slice(None, None, None), 0)]\n",
      "    value_wrapper = self.value_wrapper(getitem_2);  getitem_2 = None\n",
      "    x_1 = x * value_wrapper;  x = value_wrapper = None\n",
      "    x_2 = torch.nn.functional.dropout(x_1, 0.1, True, False);  x_1 = None\n",
      "    split = x_2.split(1, dim = -1);  x_2 = None\n",
      "    getitem_1 = split[0];  split = None\n",
      "    return (getitem_1,)\n",
      "    \n",
      "# To see more debug info, please use `graph_module.print_readable()`\n"
     ]
    }
   ],
   "source": [
    "def custom_backend(gm: torch.fx.GraphModule, example_inputs: List[torch.Tensor]):\n",
    "\n",
    "    \n",
    "\n",
    "    if wrapper_name not in gm._modules:\n",
    "        gm.add_submodule(wrapper_name, wrapper_module)\n",
    "\n",
    "    # for node in gm.graph.nodes:    \n",
    "    #     if node.op == 'call_function' and node.name == \"x_1\":\n",
    "    #         arg_names = [arg.name for arg in node.args if hasattr(arg, \"name\")]\n",
    "\n",
    "    #         if \"value\" in arg_names:\n",
    "    #             arg_index = arg_names.index(\"value\")\n",
    "\n",
    "    #             with gm.graph.inserting_before(node):\n",
    "    #                 wrapper_args = (node.args[arg_index], )\n",
    "    #                 wrapper_node = gm.graph.call_module(wrapper_name, args=wrapper_args)\n",
    "                    \n",
    "    #                 node.update_arg(arg_index, wrapper_node)\n",
    "\n",
    "    for node in gm.graph.nodes:    \n",
    "        if node.name == \"value\":\n",
    "            with gm.graph.inserting_before(node):\n",
    "                new = gm.graph.create_node(node.op, node.target, args=node.args, kwargs=node.kwargs)\n",
    "\n",
    "                wrapper_node = gm.graph.call_module(wrapper_name, args=(new,))\n",
    "\n",
    "                node.replace_all_uses_with(wrapper_node)\n",
    "                gm.graph.erase_node(node)\n",
    "                \n",
    "                \n",
    "                    \n",
    "    gm.recompile()\n",
    "    print(gm)\n",
    "\n",
    "    # for node in gm.graph.nodes:    \n",
    "    #     if node.op == 'call_module' and node.name == \"value_wrapper\":\n",
    "    #         print(node)\n",
    "    #         print(type(node.target))\n",
    "\n",
    "\n",
    "\n",
    "    return gm.forward\n",
    "\n",
    "\n",
    "torch._dynamo.reset()\n",
    "opt_model = torch.compile(mod, backend=custom_backend, dynamic=True, fullgraph=True)\n",
    "gm = opt_model(torch.tensor([[1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot happened above, so let's go through it step by step. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Editing in NNsight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Behind the scenes, the Edits passed into an NNsight model are loaded into an NNsight Editor context manager and compiled with TorchDynamo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"cuda:0\", dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EditModule(torch.nn.Module):\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if len(args) == 1:\n",
    "            args = args[0]\n",
    "\n",
    "        value = args * 1000\n",
    "        \n",
    "        return value\n",
    "    \n",
    "edit = Edit(\n",
    "    model._envoy.transformer.h[3].attn._module_path, \n",
    "    \"value\", \n",
    "    \"value_wrapper\",\n",
    "    EditModule()\n",
    ")\n",
    "\n",
    "class WrapperModule(torch.nn.Module):\n",
    "    \"\"\"Simple torch module which passes it's input through. Useful for hooking.\n",
    "    If there is only one argument, returns the first element.\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        if len(args) == 1:\n",
    "            args = args[0]\n",
    "\n",
    "        return args\n",
    "    \n",
    "wrapper_edit = Edit(\n",
    "    model._envoy.transformer.h[3].attn._module_path, \n",
    "    \"query\", \n",
    "    \"query_wrapper\",\n",
    "    WrapperModule()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "edits = [wrapper_edit]\n",
    "\n",
    "model.load_edits(edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 2.9474e-01, -2.9457e-01, -1.7623e-01, -4.0702e-01, -5.0194e-01,\n",
      "           5.3164e-01, -1.0857e+00, -7.0488e-01, -1.0685e+00,  6.2466e-01,\n",
      "          -2.2498e-01,  4.4690e-01, -9.4095e-01,  4.2125e-01, -6.0461e-01,\n",
      "          -6.7794e-01, -1.6197e-01,  1.0716e+00,  1.4526e-01,  4.6085e-01,\n",
      "           6.4771e-02,  5.5589e-01, -1.0751e-01,  1.2204e-02, -5.1981e-01,\n",
      "          -5.8530e-01, -1.0644e-01,  6.0577e-01, -2.9360e-01,  8.3008e-01,\n",
      "           3.0922e-01, -1.0348e-01,  6.6434e-01,  3.5581e-01, -1.0595e+00,\n",
      "           5.0450e-01,  6.8544e-01, -2.0407e-01,  7.2883e-02,  1.1289e+00,\n",
      "          -1.7641e-01,  4.3492e-01, -6.4675e-01,  3.5470e-01, -3.1740e-01,\n",
      "          -8.8479e-01,  7.0754e-01,  1.5963e-01, -2.4453e-02,  1.4798e-01,\n",
      "          -7.1829e-01, -4.3326e-01, -1.2205e-01,  7.7209e-01, -3.1324e-01,\n",
      "          -1.5592e+00, -2.2029e-01, -5.4575e-01, -7.1221e-01, -7.0549e-01,\n",
      "           6.1355e-01,  1.0617e-01,  1.1756e+00, -1.5728e-01, -3.3079e-01,\n",
      "           5.2387e-01, -7.3582e-01,  1.0715e+00, -2.6963e-01, -2.8888e-01,\n",
      "          -1.7328e+00, -1.4382e-01, -1.1301e+00,  1.3869e+00,  5.5098e-01,\n",
      "           7.1600e-02, -2.3860e-02,  3.1172e-01, -9.8747e-01,  8.0747e-01,\n",
      "          -3.9031e-01, -1.2705e+00,  1.5374e+00,  4.8823e-01,  1.9274e-01,\n",
      "           1.4481e+00, -6.4960e-01,  4.9234e-02, -6.7085e-01, -1.0140e+00,\n",
      "          -1.4780e-01, -7.6717e-01, -1.0844e+00, -1.4491e+00, -6.1699e-01,\n",
      "           8.6943e-01,  3.4202e-02, -9.2045e-01, -3.5687e-01,  7.0777e-01,\n",
      "          -1.6182e+00, -7.1687e-01,  1.6620e+00, -9.9156e-01, -1.2504e+00,\n",
      "          -1.0037e-01,  2.8830e-02,  1.6791e-01, -7.9602e-01, -1.8408e-01,\n",
      "           1.3473e+00, -3.5242e-01, -1.3767e+00,  3.0414e-01,  7.9273e-01,\n",
      "          -1.8286e+00, -1.0347e+00, -1.3355e-01, -1.2478e+00,  2.1645e-01,\n",
      "          -5.4996e-01, -4.5057e-01,  1.1305e+00,  2.8972e+00, -6.0638e-01,\n",
      "          -1.0083e+00,  6.6864e-01, -1.6586e-01,  1.5923e-01,  7.0939e-01,\n",
      "           9.8359e-01, -3.6039e-01, -1.6318e-01,  1.8019e+00,  2.3878e-01,\n",
      "          -6.3221e-02,  1.5729e-01,  2.0233e-01, -3.1070e-01,  5.9919e-01,\n",
      "          -2.2085e-01, -2.2290e-01, -4.6596e-01,  2.2888e-01,  1.9197e-01,\n",
      "          -4.5660e-01, -2.1218e-01, -4.7887e-01, -6.6827e-01,  4.9828e-01,\n",
      "           6.2239e-01, -4.1999e-01, -2.8471e-01, -2.4607e-01,  8.2304e-01,\n",
      "           7.3738e-01,  4.0419e-01,  4.4265e-01, -1.9213e-01,  4.0825e-01,\n",
      "          -6.3520e-01, -6.3416e-01, -8.9184e-02, -7.5693e-01,  5.8201e-01,\n",
      "          -6.5146e-02, -5.7322e-01,  2.4218e-01,  9.0657e-01,  3.4621e-02,\n",
      "           3.4320e-01,  3.7221e-01,  7.9972e-02,  2.9484e-01, -3.2754e-01,\n",
      "           2.7377e-01,  6.0902e-01, -5.9578e-01, -1.9344e-01, -1.0039e+00,\n",
      "           4.4425e-01,  5.2727e-01, -3.9243e-01, -2.0764e-01,  4.8079e-01,\n",
      "           8.7299e-02, -4.6368e-01,  2.3198e-02,  1.1609e-01,  1.3284e+00,\n",
      "          -5.9925e-01, -5.8221e-01, -6.1854e-01,  1.3837e-01,  1.7306e+00,\n",
      "          -1.4242e+00,  1.3267e-01,  1.1751e+00, -4.4523e-01,  4.5551e-01,\n",
      "          -1.0568e+00, -1.3601e+00, -6.5884e-01, -6.9426e-01, -7.5811e-01,\n",
      "           3.2509e-02, -6.4557e-01,  6.4819e-02,  6.2471e-02, -9.6227e-01,\n",
      "           3.3293e-01, -1.0108e+00, -4.4604e-01, -1.1058e+00,  1.8781e+00,\n",
      "           5.9266e-01,  3.8256e-01,  1.4062e+00, -9.6375e-01,  1.6654e+00,\n",
      "          -6.7780e-01,  9.8577e-01,  1.6757e+00, -8.8619e-01, -7.3879e-01,\n",
      "          -1.6056e+00,  1.6092e+00, -1.7443e-01, -9.5915e-01,  3.2234e-01,\n",
      "           4.2334e-01,  6.0195e-01, -4.0554e-01,  1.3900e-01, -5.2405e-01,\n",
      "           7.2358e-02, -6.2327e-01, -7.3979e-01,  1.2836e-01, -4.6733e-01,\n",
      "          -3.3200e-01, -1.7563e+00,  1.0898e+00, -1.1559e+00, -7.6349e-01,\n",
      "           6.4930e-01, -2.1544e+00, -7.8754e-01,  3.8999e-01, -8.6487e-01,\n",
      "           8.2115e-01,  4.9506e-01,  7.6535e-02,  9.1825e-01,  1.1032e+00,\n",
      "           3.8673e-01,  6.9107e-01, -6.8724e-01,  2.0210e-01, -1.5255e+00,\n",
      "          -5.9363e-01,  1.0773e-01, -1.0590e-01,  2.9143e-02, -8.5484e-01,\n",
      "          -5.9772e-01, -4.4723e-02, -7.8983e-01, -7.9142e-01,  1.0814e+00,\n",
      "           7.3186e-01,  2.6352e-01, -1.0303e+00,  1.7882e-02, -1.4264e+00,\n",
      "          -9.7437e-01,  3.2323e-01, -1.2284e+00, -6.4266e-01, -1.0655e-01,\n",
      "           3.6068e-01,  4.5823e-01,  9.6884e-01,  5.2773e-01,  1.1885e-01,\n",
      "          -6.9896e-02, -4.1950e-01,  1.3237e+00,  2.9287e-01, -1.9740e-01,\n",
      "           1.4927e-01,  5.8836e-01, -1.3437e-01, -1.0837e-02, -2.3861e-01,\n",
      "          -3.1050e-01,  1.6727e+00, -1.0786e+00,  1.0449e-01, -1.3559e-01,\n",
      "          -5.5758e-01, -6.8208e-01,  1.1587e+00, -1.0299e-01,  2.3368e-01,\n",
      "           8.7766e-01, -4.8100e-01, -2.8500e-01,  3.2010e-01, -1.5270e-01,\n",
      "           3.9002e-01,  1.6694e+00,  8.6323e-01,  1.2456e+00,  4.3895e-01,\n",
      "          -5.9806e-01,  8.3081e-01,  3.7853e-01,  4.4832e-01, -1.0211e+00,\n",
      "          -4.8607e-01, -1.6294e+00, -1.1428e-01, -7.5189e-01,  6.8907e-01,\n",
      "           1.5818e+00, -9.3284e-01,  3.7201e-02,  1.4338e+00, -1.2076e+00,\n",
      "          -1.5475e-01, -1.5111e+00, -1.8836e+00,  1.0714e+00,  2.8463e+00,\n",
      "          -1.5468e+00, -9.2198e-01, -5.5011e-01,  1.4692e+00, -8.9348e-01,\n",
      "          -1.4439e+00,  1.2960e+00, -1.2940e+00, -4.3907e-01,  9.5864e-01,\n",
      "          -1.7204e+00,  1.3846e+00,  3.0120e-01,  1.2770e+00, -2.2739e+00,\n",
      "           2.2165e+00,  4.5347e-01, -1.7059e+00,  7.5314e-03,  6.7872e-01,\n",
      "           1.8995e-01, -1.3612e+00, -2.1565e+00, -1.0932e+00,  1.2161e+00,\n",
      "          -1.5329e+00, -1.6283e+00, -3.1500e-01,  1.6295e+00, -1.1051e+00,\n",
      "           1.5266e+00, -2.9407e+00,  4.3446e-01, -3.9948e-01, -6.8393e-01,\n",
      "          -6.9014e-01,  2.9238e-01,  9.3834e-01, -1.1133e+00,  8.5599e-01,\n",
      "          -5.9576e-01,  1.5811e-02, -3.0499e-01, -3.1022e-02,  1.2244e+00,\n",
      "          -1.1656e+00, -5.4312e-01,  1.4432e+00, -1.6135e+00,  2.2409e-01,\n",
      "          -2.3872e-01, -2.7959e-01, -5.4665e-01, -5.9751e-01,  1.8311e-01,\n",
      "           6.2361e-01, -1.3759e-01,  3.5776e-01,  8.3110e-01,  5.4407e-02,\n",
      "          -8.4606e-01,  4.1522e-01,  2.0260e-01,  5.3130e-01,  4.9783e-01,\n",
      "           1.1248e-01,  5.8841e-02,  2.4458e-01,  6.2830e-01,  2.4390e-01,\n",
      "          -2.9786e-01, -1.2500e+00, -5.6147e-01, -1.1400e+00,  3.1650e-01,\n",
      "           2.1330e-01, -3.5752e-01,  4.2084e-01,  3.6071e-01, -9.8784e-01,\n",
      "           3.5830e-01, -1.3110e+00, -5.6173e-01, -9.1173e-01, -3.0324e-02,\n",
      "          -1.3613e-01, -5.8384e-01, -4.8497e-01, -6.9229e-01,  6.1012e-01,\n",
      "          -8.0459e-01,  7.0785e-01, -5.7463e-01,  3.8691e-01,  2.9756e-01,\n",
      "           4.0862e-01,  1.1058e+00,  6.5534e-01, -2.2055e-01, -7.6719e-01,\n",
      "           1.4205e+00,  3.3717e-01, -2.2465e-01,  3.7888e-01, -6.7485e-02,\n",
      "          -6.3094e-01, -8.2366e-01, -1.0657e+00, -1.3032e-01,  4.0120e-01,\n",
      "          -8.0989e-01,  1.0954e-01,  1.1148e+00,  3.4996e-02, -1.2656e+00,\n",
      "           1.3955e+00, -7.0824e-01, -7.2828e-01,  2.5726e-01, -4.1253e-01,\n",
      "          -2.4562e-01, -4.7646e-01, -2.5133e-01,  1.4330e+00,  8.5679e-01,\n",
      "          -3.0282e-01,  5.7544e-01, -1.2564e-01, -3.8891e-01, -1.2355e+00,\n",
      "          -1.2286e+00,  1.1454e+00,  5.7986e-01, -1.0924e+00, -8.9823e-01,\n",
      "           1.5617e-01, -7.3449e-01, -5.2802e-01, -5.5737e-01, -9.5633e-01,\n",
      "           9.1878e-01, -4.6407e-01, -2.1822e+00, -4.6032e-01, -4.4666e-01,\n",
      "           9.7327e-02,  2.6458e-02, -1.7938e-01,  9.4908e-02,  2.4147e-01,\n",
      "          -1.1003e-02, -2.5433e-01,  5.4939e-01, -5.4397e-01, -3.5924e-01,\n",
      "          -7.7453e-01, -5.6269e-01,  1.6950e-01,  1.1347e+00,  7.9104e-01,\n",
      "          -1.2502e+00, -1.2258e+00, -1.0351e-01,  4.0436e-01, -3.4675e-01,\n",
      "           2.5538e-01, -2.9546e-01,  5.9789e-01,  8.5938e-02, -9.9910e-01,\n",
      "          -2.8022e-01, -1.8508e-01, -4.1328e-01, -4.8151e-01, -5.9791e-01,\n",
      "           5.0983e-01,  1.0190e+00, -2.3177e-01,  6.9086e-01,  1.1832e+00,\n",
      "           3.6865e-01,  3.5626e-01,  1.3002e+00, -3.5176e-01, -1.0646e+00,\n",
      "          -1.6935e-01,  1.8611e+00, -9.3484e-01,  3.0165e-01, -4.6170e-02,\n",
      "           9.3825e-01,  3.8519e-01, -1.8126e-01,  8.3955e-01,  8.4650e-02,\n",
      "          -2.1144e-01,  1.1616e+00,  1.4063e-01,  5.7157e-01,  1.9047e-01,\n",
      "          -2.3185e-01, -3.0874e-01, -3.5098e-01, -2.2016e-01, -4.0129e-01,\n",
      "           5.0228e-01,  7.6053e-02, -4.5585e-01,  1.2900e-01, -5.4872e-01,\n",
      "          -2.8274e-01,  5.7427e-02, -5.8294e-01,  6.7393e-01,  1.5294e-01,\n",
      "           8.0948e-01, -3.9563e-01, -8.2745e-01,  7.4635e-01, -4.0614e-02,\n",
      "          -4.1800e-02,  2.0683e-02, -1.0281e-01, -8.3067e-01, -2.9631e-01,\n",
      "           9.2139e-01, -1.3158e-01,  3.7174e-01, -9.6024e-01,  2.2390e-01,\n",
      "          -2.8238e-01, -1.0406e+00,  3.3558e-01,  9.1403e-02, -2.0357e-02,\n",
      "          -9.7665e-01,  1.5300e-01, -3.4292e-01, -1.6405e-01, -2.3627e-02,\n",
      "           1.4740e-01,  2.7346e-01,  5.2838e-01,  2.7583e-01, -2.6246e-01,\n",
      "           6.3992e-01, -3.6822e-01,  2.4649e-01,  2.0639e-01, -5.0527e-02,\n",
      "          -1.0054e+00, -6.7386e-01,  5.0723e-01, -1.2968e+00,  5.3796e-01,\n",
      "           2.8911e-01, -8.2530e-01,  5.9390e-01, -4.5359e-01, -1.7623e-01,\n",
      "          -4.3988e-01, -1.0501e+00,  2.2759e-01,  3.0390e-03,  7.6577e-01,\n",
      "          -1.0612e+00, -1.1578e-01, -5.0932e-02,  2.8302e-01,  1.1187e-01,\n",
      "          -1.9470e-01, -1.2745e-01,  1.1414e-02, -4.3376e-01, -5.5566e-02,\n",
      "           5.4266e-01,  1.7857e-03,  2.9908e-01,  5.8295e-01,  3.8407e-01,\n",
      "           6.2929e-02,  2.8036e-01,  7.7630e-01,  6.4649e-01, -2.0112e-01,\n",
      "          -7.9518e-01,  3.6789e-01, -7.5658e-01, -4.5441e-01,  3.8835e-02,\n",
      "          -3.5312e-01, -4.2377e-01,  4.6634e-02, -6.4218e-01,  5.0563e-01,\n",
      "          -2.1844e+00,  3.9971e-01, -4.2884e-02,  1.0447e-01,  9.9986e-01,\n",
      "          -5.0893e-01,  3.2538e-01, -7.5686e-01,  4.4693e-01,  2.3096e-01,\n",
      "          -3.3906e-01,  8.2843e-01,  1.4376e-01,  4.6295e-01,  2.3694e-01,\n",
      "           6.6796e-01, -6.3151e-01,  4.0659e-01,  4.1135e-01,  7.3255e-02,\n",
      "          -3.8528e-01, -3.8111e-01,  3.0801e-01, -3.4698e-01,  7.4621e-01,\n",
      "          -7.3729e-01, -3.4893e-01, -9.7589e-01,  5.9701e-01,  2.6277e-01,\n",
      "          -9.1164e-01,  2.8440e-01,  8.2000e-01,  5.7775e-01,  2.3409e-01,\n",
      "          -2.4505e-01,  3.6177e-01,  7.8367e-02,  1.0854e-01, -1.2063e-01,\n",
      "          -6.5858e-01,  7.4630e-01, -1.3371e-01,  9.3207e-03,  3.0860e-01,\n",
      "          -1.5669e-01,  8.8602e-02, -4.9325e-01,  3.3118e-01, -3.4853e-01,\n",
      "           1.2283e-01, -1.0392e+00, -1.2344e-01, -2.0905e-01, -8.1793e-01,\n",
      "          -1.6988e-01, -2.6698e-01,  9.0156e-01, -5.4444e-01,  7.3923e-03,\n",
      "          -7.5379e-01, -4.2945e-03,  3.9944e-01,  7.5645e-02, -7.3624e-01,\n",
      "          -1.2677e+00, -9.2028e-02,  6.8868e-01, -3.3162e-01,  2.6183e-01,\n",
      "           2.8232e-01, -2.9840e-03,  8.5342e-02, -5.9206e-01, -3.6830e-01,\n",
      "          -4.1260e-01,  6.3020e-01,  8.5795e-01, -7.2541e-01,  2.3046e-01,\n",
      "           3.9010e-01, -5.7812e-01,  5.1842e-01,  5.3075e-01, -8.0336e-01,\n",
      "          -2.1635e-01, -2.1163e-01, -5.6073e-02,  6.4656e-01, -4.0760e-01,\n",
      "          -7.4416e-01,  5.2597e-01,  1.7210e-01, -1.6159e-01,  3.2315e-02,\n",
      "           1.1118e+00, -8.5661e-01,  7.9577e-01,  2.0606e-01, -6.7927e-01,\n",
      "           3.2077e-02,  2.3751e-01, -4.7496e-01, -8.0359e-01,  4.3499e-02,\n",
      "          -6.4600e-01,  4.3904e-01,  6.3420e-01,  4.3799e-01, -7.7768e-01,\n",
      "          -3.1361e-01, -1.3178e-01,  1.6372e-01, -7.8290e-01,  4.6827e-01,\n",
      "           8.5454e-01, -8.7752e-01,  1.0266e+00,  1.2472e+00, -1.9292e-01,\n",
      "           4.0510e-01,  8.2641e-02, -1.1040e-01, -5.3817e-01,  5.6838e-02,\n",
      "           8.3988e-01,  1.8735e-01,  6.1837e-01,  5.9772e-01, -5.6735e-01,\n",
      "          -1.8478e-01, -2.3961e-01, -1.5530e-02,  8.0570e-01, -1.0918e+00,\n",
      "          -7.8785e-01, -6.0619e-02, -1.0008e+00]]], device='cuda:0',\n",
      "       grad_fn=<SplitBackward0>)\n"
     ]
    }
   ],
   "source": [
    "with model.trace(\"empty\", scan=False, validate=False):\n",
    "    query = model.transformer._orig_mod.h[3].attn.query_wrapper.output.save()\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Other Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "model = LanguageModel(\"openai-community/gpt2\", device_map=\"cuda:0\", dispatch=True)\n",
    "\n",
    "with model.trace(\"empty\"):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">GraphModule()\n",
       "def forward(self, L_x_ : torch._subclasses.fake_tensor.FakeTensor):\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">l_x_</span> = L_x_\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">size</span> = l_x_.<span style=\"color: #008000; text-decoration-color: #008000\">size</span>()\n",
       "    <span style=\"color: #ff8700; text-decoration-color: #ff8700\">l__self___bias</span> = self.L__self___bias\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">size_1</span> = l_x_.size(-1)\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">view</span> = l_x_.<span style=\"color: #008000; text-decoration-color: #008000\">view</span>(-1, size_1);  l_x_ = size_1 = None\n",
       "    <span style=\"color: #ff8700; text-decoration-color: #ff8700\">l__self___weight</span> = self.L__self___weight\n",
       "    <span style=\"color: #005fff; text-decoration-color: #005fff\">x</span> = torch.addmm(l__self___bias, view, l__self___weight);  l__self___bias = view = l__self___weight = None\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">x_1</span> = x.view((1, 1, 768));  x = None\n",
       "    <span style=\"color: #af00ff; text-decoration-color: #af00ff\">return</span> (x_1,)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "GraphModule()\n",
       "def forward(self, L_x_ : torch._subclasses.fake_tensor.FakeTensor):\n",
       "    \u001b[33ml_x_\u001b[0m = L_x_\n",
       "    \u001b[32msize\u001b[0m = l_x_.\u001b[32msize\u001b[0m()\n",
       "    \u001b[38;5;208ml__self___bias\u001b[0m = self.L__self___bias\n",
       "    \u001b[32msize_1\u001b[0m = l_x_.size(-1)\n",
       "    \u001b[32mview\u001b[0m = l_x_.\u001b[32mview\u001b[0m(-1, size_1);  l_x_ = size_1 = None\n",
       "    \u001b[38;5;208ml__self___weight\u001b[0m = self.L__self___weight\n",
       "    \u001b[38;5;27mx\u001b[0m = torch.addmm(l__self___bias, view, l__self___weight);  l__self___bias = view = l__self___weight = None\n",
       "    \u001b[32mx_1\u001b[0m = x.view((1, 1, 768));  x = None\n",
       "    \u001b[38;5;129mreturn\u001b[0m (x_1,)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "attn_envoy = model._envoy.transformer.h[3].mlp.c_proj\n",
    "\n",
    "print_gm(attn_envoy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
